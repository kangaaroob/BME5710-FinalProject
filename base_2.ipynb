{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "___\n",
    "# **Starter code for BME 5710 project**\n",
    "## Instructor -- Rizwan Ahmad (ahmad.46@osu.edu)\n",
    "## BME5710 -- Spring 2025"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "___\n",
    "### Import libraries and sub-libraries."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "from PIL import Image\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torchvision import transforms\n",
    "import matplotlib.pyplot as plt\n",
    "from torchvision.transforms import functional as TF\n",
    "import torch.nn.functional as F\n",
    "from torchmetrics.image.psnr import PeakSignalNoiseRatio\n",
    "from torchmetrics.image.ssim import StructuralSimilarityIndexMeasure\n",
    "from tqdm.notebook import tqdm\n",
    "import datetime\n",
    "import copy\n",
    "from torch.optim.lr_scheduler import ReduceLROnPlateau"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "___\n",
    "### Calling a custom code to change the default font for figures to `Computer Modern`. (Optional)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from fontsetting import font_cmu\n",
    "# plt = font_cmu(plt)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "___\n",
    "### Check the hardware that is at your disposal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Device available: cuda\n"
     ]
    }
   ],
   "source": [
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print('Device available:', device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "___\n",
    "### Read training data from `data/train/hig-res` and `data/train/low-res`\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TIFFDataset(Dataset):\n",
    "    def __init__(self, high_res_dir, low_res_dir, transform=None, augment=False):\n",
    "        self.high_res_dir = high_res_dir\n",
    "        self.low_res_dir = low_res_dir\n",
    "        self.transform = transform\n",
    "        self.augment = augment\n",
    "        self.original_filenames = sorted([f for f in os.listdir(high_res_dir) if f.endswith('.tif')])\n",
    "        self.num_original_images = len(self.original_filenames)\n",
    "\n",
    "        self.low_res_images = []\n",
    "        self.high_res_images = []\n",
    "\n",
    "        for filename in self.original_filenames:\n",
    "            high_res_path = os.path.join(self.high_res_dir, filename)\n",
    "            low_res_path = os.path.join(self.low_res_dir, filename)\n",
    "\n",
    "            # Load original images\n",
    "            try:\n",
    "                hr_img = Image.open(high_res_path)\n",
    "                lr_img = Image.open(low_res_path)\n",
    "            except Exception as e:\n",
    "                print(f\"Error loading image {filename}: {e}\")\n",
    "                continue\n",
    "\n",
    "            # Resize low-res to 128x128\n",
    "            lr_img = lr_img.resize((128, 128), Image.BICUBIC)\n",
    "\n",
    "            original_lr, original_hr = lr_img, hr_img\n",
    "            processed_lr = []\n",
    "            processed_hr = []\n",
    "\n",
    "            if self.augment:\n",
    "                # --- Apply transformations ---\n",
    "                # 1) Original\n",
    "                processed_lr.append(original_lr)\n",
    "                processed_hr.append(original_hr)\n",
    "                # 2) Rotated by 90 degrees\n",
    "                processed_lr.append(original_lr.rotate(90))\n",
    "                processed_hr.append(original_hr.rotate(90))\n",
    "                # 3) Rotated by 180 degrees\n",
    "                processed_lr.append(original_lr.rotate(180))\n",
    "                processed_hr.append(original_hr.rotate(180))\n",
    "                # 4) Rotated by 270 degrees\n",
    "                processed_lr.append(original_lr.rotate(270))\n",
    "                processed_hr.append(original_hr.rotate(270))\n",
    "\n",
    "                # Apply flips\n",
    "                lr_v_flip = original_lr.transpose(Image.FLIP_LEFT_RIGHT)\n",
    "                hr_v_flip = original_hr.transpose(Image.FLIP_LEFT_RIGHT)\n",
    "                lr_h_flip = original_lr.transpose(Image.FLIP_TOP_BOTTOM)\n",
    "                hr_h_flip = original_hr.transpose(Image.FLIP_TOP_BOTTOM)\n",
    "\n",
    "                # 5) Mirrored on the vertical axis (Flip Left/Right)\n",
    "                processed_lr.append(lr_v_flip)\n",
    "                processed_hr.append(hr_v_flip)\n",
    "                # 6) Mirrored on the horizontal axis (Flip Top/Bottom)\n",
    "                processed_lr.append(lr_h_flip)\n",
    "                processed_hr.append(hr_h_flip)\n",
    "\n",
    "                # Apply flips combined with rotation (unique combinations)\n",
    "                # 7) Rotated by 90 degrees and mirrored on the vertical axis\n",
    "                processed_lr.append(lr_v_flip.rotate(90))\n",
    "                processed_hr.append(hr_v_flip.rotate(90))\n",
    "                # 8) Rotated by 90 degrees and mirrored on the horizontal axis\n",
    "                processed_lr.append(lr_h_flip.rotate(90))\n",
    "                processed_hr.append(hr_h_flip.rotate(90))\n",
    "            else:\n",
    "                # If not augmenting, just use the original\n",
    "                processed_lr.append(original_lr)\n",
    "                processed_hr.append(original_hr)\n",
    "\n",
    "            # Apply the final transform (ToTensor) and store\n",
    "            if self.transform:\n",
    "                for lr, hr in zip(processed_lr, processed_hr):\n",
    "                    self.low_res_images.append(self.transform(lr))\n",
    "                    self.high_res_images.append(self.transform(hr))\n",
    "            else:\n",
    "                 # If no transform, store PIL images (not recommended for training/eval)\n",
    "                self.low_res_images.extend(processed_lr)\n",
    "                self.high_res_images.extend(processed_hr)\n",
    "\n",
    "    # Get the number of samples in the dataset (original or augmented)\n",
    "    def __len__(self):\n",
    "        return len(self.low_res_images)\n",
    "\n",
    "    # Get the sample at the given index\n",
    "    def __getitem__(self, idx):\n",
    "        # Return pre-processed tensors\n",
    "        return self.low_res_images[idx], self.high_res_images[idx]\n",
    "\n",
    "# Define a transform to convert images to PyTorch tensors\n",
    "transform = transforms.Compose([\n",
    "    transforms.ToTensor()\n",
    "])\n",
    "\n",
    "# Create the dataset for training images WITH augmentation\n",
    "train_dataset = TIFFDataset('data/train/high-res', 'data/train/low-res', transform=transform, augment=True)\n",
    "val_dataset = TIFFDataset('data/val/high-res', 'data/val/low-res', transform=transform, augment=False)\n",
    "\n",
    "# Function to create data loader\n",
    "def create_loader(dataset, batch_size, shuffle_data=True): # Added shuffle parameter\n",
    "    torch.manual_seed(0)  # For reproducibility\n",
    "    return DataLoader(dataset, batch_size=batch_size, shuffle=shuffle_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create a custom loss class to easily implement custom loss functions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CombinedLoss(nn.Module):\n",
    "    def __init__(self, alpha=0.5, beta=0.5, device='cpu', stage_weights=None):\n",
    "        super(CombinedLoss, self).__init__()\n",
    "        self.alpha = alpha # Weight for MSE (PSNR) loss component\n",
    "        self.beta = beta # Weight for SSIM loss component\n",
    "        self.mse = nn.MSELoss()\n",
    "        self.ssim = StructuralSimilarityIndexMeasure(data_range=1.0).to(device)\n",
    "        self.device = device\n",
    "        # Weights for intermediate outputs (e.g., [0.2, 0.3, 0.5] for 3 stages)\n",
    "        self.stage_weights = stage_weights if stage_weights else [1.0] # Default: only use final output\n",
    "\n",
    "    def forward(self, y_preds, y_true):\n",
    "        # y_preds can be a single tensor (SRNet) or a list of tensors (MPRNet)\n",
    "        # y_true is always the single ground truth tensor\n",
    "\n",
    "        y_true = y_true.to(self.device)\n",
    "\n",
    "        total_loss = 0.0\n",
    "        num_outputs = 1\n",
    "\n",
    "        if isinstance(y_preds, list): # MPRNet case\n",
    "            num_outputs = len(y_preds)\n",
    "            if len(self.stage_weights) != num_outputs:\n",
    "                # Fallback if weights don't match outputs (e.g., use equal weights)\n",
    "                self.stage_weights = [1.0 / num_outputs] * num_outputs\n",
    "                print(f\"Warning: Mismatched stage_weights ({len(self.stage_weights)}) and MPRNet outputs ({num_outputs}). Using equal weights.\")\n",
    "\n",
    "            for i, y_pred_stage in enumerate(y_preds):\n",
    "                y_pred_stage = y_pred_stage.to(self.device)\n",
    "                mse_loss_stage = self.mse(y_pred_stage, y_true)\n",
    "                # SSIM calculation might fail if intermediate outputs are not in [0, 1] range\n",
    "                # Clamp or ensure range if necessary before SSIM\n",
    "                y_pred_stage_clamped = torch.clamp(y_pred_stage, 0.0, 1.0)\n",
    "                ssim_val_stage = self.ssim(y_pred_stage_clamped, y_true)\n",
    "                ssim_loss_stage = 1.0 - ssim_val_stage\n",
    "                stage_loss = (self.alpha * mse_loss_stage) + (self.beta * ssim_loss_stage)\n",
    "                total_loss += self.stage_weights[i] * stage_loss\n",
    "\n",
    "        else: # SRNet case (or single output model)\n",
    "            y_pred = y_preds.to(self.device)\n",
    "            mse_loss = self.mse(y_pred, y_true)\n",
    "            ssim_val = self.ssim(y_pred, y_true)\n",
    "            ssim_loss = 1.0 - ssim_val\n",
    "            total_loss = (self.alpha * mse_loss) + (self.beta * ssim_loss)\n",
    "\n",
    "        return total_loss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "___\n",
    "### Define a super-resolution network\n",
    "\n",
    "#### Here, I have defined a trivial network, which has only two layers and no activation function. We are essentially doing linear filtering."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SRNet(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(SRNet, self).__init__()\n",
    "\n",
    "        self.relu = nn.ReLU(inplace=True)\n",
    "        upscale_factor = 2\n",
    "        channels_mult = upscale_factor * upscale_factor # = 4\n",
    "\n",
    "        # --- Initial Feature Extraction ---\n",
    "        # Input: 1 x 128 x 128\n",
    "        self.conv1 = nn.Conv2d(in_channels=1, out_channels=80, kernel_size=3, padding=1) # 80 x 128 x 128\n",
    "        self.conv1b = nn.Conv2d(in_channels=80, out_channels=80, kernel_size=3, padding=1) # 80 x 128 x 128\n",
    "\n",
    "        # --- Upscaling Path 1 (128 -> 256) using PixelShuffle ---\n",
    "        # Main Path\n",
    "        self.upscale1_conv = nn.Conv2d(in_channels=80, out_channels=80 * channels_mult, kernel_size=3, padding=1) # 80*4 x 128 x 128\n",
    "        self.pixel_shuffle1 = nn.PixelShuffle(upscale_factor) # -> 80 x 256 x 256\n",
    "        self.conv2 = nn.Conv2d(in_channels=80, out_channels=80, kernel_size=3, padding=1) # 80 x 256 x 256\n",
    "        self.conv2b = nn.Conv2d(in_channels=80, out_channels=80, kernel_size=3, padding=1) # 80 x 256 x 256\n",
    "        # Skip Path 1 (Interpolate features from 128x128 stage)\n",
    "        self.bskip1 = nn.Upsample(scale_factor=upscale_factor, mode='bicubic', align_corners=False) # Interpolates 80 channels to 256x256\n",
    "        # Convolution after combining skip connection 1 (Input channels = 80 from main path + 80 from skip = 160)\n",
    "        self.conv_after_skip1 = nn.Conv2d(in_channels=160, out_channels=80, kernel_size=3, padding=1) # 160x256x256 -> 80x256x256\n",
    "        self.conv_after_skip1b = nn.Conv2d(in_channels=80, out_channels=80, kernel_size=3, padding=1) # 80x256x256\n",
    "\n",
    "        # --- Upscaling Path 2 (256 -> 512) using PixelShuffle & Widened Middle ---\n",
    "        # Main Path\n",
    "        self.upscale2_conv = nn.Conv2d(in_channels=80, out_channels=80 * channels_mult, kernel_size=3, padding=1) # 80*4 x 256 x 256 (Widened)\n",
    "        self.pixel_shuffle2 = nn.PixelShuffle(upscale_factor) # -> 80 x 512 x 512 (Widened)\n",
    "        self.conv3 = nn.Conv2d(in_channels=80, out_channels=80, kernel_size=3, padding=1) # 80 x 512 x 512 (Widened)\n",
    "        self.conv3b = nn.Conv2d(in_channels=80, out_channels=80, kernel_size=3, padding=1) # 80 x 512 x 512 (Widened)\n",
    "        # Skip Path 2 (Interpolate features from 256x256 stage after conv_after_skip1b)\n",
    "        self.bskip2 = nn.Upsample(scale_factor=upscale_factor, mode='bicubic', align_corners=False) # Interpolates 80 channels to 512x512\n",
    "        # Convolution after combining skip connection 2 (Input channels = 80 from main path + 80 from skip = 160) (Widened)\n",
    "        self.conv_after_skip2 = nn.Conv2d(in_channels=160, out_channels=80, kernel_size=3, padding=1) # 160x512x512 -> 80x512x512 (Widened)\n",
    "        self.conv_after_skip2b = nn.Conv2d(in_channels=80, out_channels=80, kernel_size=3, padding=1) # 80x512x512 (Widened)\n",
    "\n",
    "        # --- Downscaling Path (512 -> 256) ---\n",
    "        # Input channel matches the widened middle stage (80)\n",
    "        self.downscale = nn.Conv2d(in_channels=80, out_channels=80, kernel_size=2, stride=2) # 80x512x512 -> 80x256x256\n",
    "        self.conv4 = nn.Conv2d(in_channels=80, out_channels=80, kernel_size=3, padding=1) # 80x256x256\n",
    "        self.conv4b = nn.Conv2d(in_channels=80, out_channels=80, kernel_size=3, padding=1) # 80x256x256\n",
    "\n",
    "        # --- Final Output Layer ---\n",
    "        # Takes the output of the downscaling path (80 channels) and maps to 1 channel\n",
    "        self.conv_out_final = nn.Conv2d(in_channels=80, out_channels=1, kernel_size=3, padding=1) # 80x256x256 -> 1x256x256\n",
    "\n",
    "    def forward(self, x):\n",
    "        # x: Input 1 x 128 x 128\n",
    "\n",
    "        # --- Initial Feature Extraction ---\n",
    "        features128 = self.relu(self.conv1(x))\n",
    "        features128 = self.relu(self.conv1b(features128)) # 80 x 128 x 128\n",
    "\n",
    "        # --- Upscaling Path 1 + Skip Connection 1 ---\n",
    "        # Calculate interpolated skip features first\n",
    "        skip1_interpolated = self.bskip1(features128) # 80 x 256 x 256\n",
    "        # Main path upscale using PixelShuffle\n",
    "        up1_conv_out = self.upscale1_conv(features128) # 80*4 x 128 x 128\n",
    "        up1_out = self.pixel_shuffle1(up1_conv_out)    # 80 x 256 x 256\n",
    "        features256_main = self.relu(self.conv2(up1_out))\n",
    "        features256_main = self.relu(self.conv2b(features256_main)) # 80 x 256 x 256\n",
    "        # Concatenate main path features and interpolated skip features\n",
    "        concat1 = torch.cat((features256_main, skip1_interpolated), dim=1) # 160 x 256 x 256\n",
    "        # Process combined features\n",
    "        features256_processed = self.relu(self.conv_after_skip1(concat1))\n",
    "        features256_processed = self.relu(self.conv_after_skip1b(features256_processed)) # 80 x 256 x 256\n",
    "\n",
    "        # --- Upscaling Path 2 + Skip Connection 2 ---\n",
    "        # Calculate interpolated skip features (using output from conv_after_skip1b)\n",
    "        skip2_interpolated = self.bskip2(features256_processed) # 80 x 512 x 512\n",
    "        # Main path upscale using PixelShuffle (Widened Middle)\n",
    "        up2_conv_out = self.upscale2_conv(features256_processed) # 80*4 x 256 x 256\n",
    "        up2_out = self.pixel_shuffle2(up2_conv_out)    # 80 x 512 x 512\n",
    "        features512_main = self.relu(self.conv3(up2_out))\n",
    "        features512_main = self.relu(self.conv3b(features512_main)) # 80 x 512 x 512\n",
    "        # Concatenate main path features and interpolated skip features\n",
    "        concat2 = torch.cat((features512_main, skip2_interpolated), dim=1) # 160 x 512 x 512\n",
    "        # Process combined features\n",
    "        features512_processed = self.relu(self.conv_after_skip2(concat2))\n",
    "        features512_processed = self.relu(self.conv_after_skip2b(features512_processed)) # 80 x 512 x 512\n",
    "\n",
    "        # --- Downscaling Path ---\n",
    "        down_out = self.downscale(features512_processed) # 80 x 256 x 256\n",
    "        features256_final = self.relu(self.conv4(down_out))\n",
    "        features256_final = self.relu(self.conv4b(features256_final)) # 80 x 256 x 256\n",
    "\n",
    "        # --- Final Output Layer ---\n",
    "        output = self.conv_out_final(features256_final) # 1 x 256 x 256\n",
    "\n",
    "        return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Channel Attention Layer (CALayer) ---\n",
    "class CALayer(nn.Module):\n",
    "    def __init__(self, channel, reduction=16):\n",
    "        super(CALayer, self).__init__()\n",
    "        self.avg_pool = nn.AdaptiveAvgPool2d(1)\n",
    "        self.conv_du = nn.Sequential(\n",
    "                nn.Conv2d(channel, channel // reduction, 1, padding=0, bias=True),\n",
    "                nn.ReLU(inplace=True),\n",
    "                nn.Conv2d(channel // reduction, channel, 1, padding=0, bias=True),\n",
    "                nn.Sigmoid()\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        y = self.avg_pool(x)\n",
    "        y = self.conv_du(y)\n",
    "        return x * y\n",
    "\n",
    "# --- Channel Attention Block (CAB) ---\n",
    "class CAB(nn.Module):\n",
    "    def __init__(self, n_feat, kernel_size=3, reduction=16, bias=True, act=nn.ReLU(True)):\n",
    "        super(CAB, self).__init__()\n",
    "        modules_body = []\n",
    "        modules_body.append(nn.Conv2d(n_feat, n_feat, kernel_size, padding=(kernel_size//2), bias=bias))\n",
    "        modules_body.append(act)\n",
    "        modules_body.append(nn.Conv2d(n_feat, n_feat, kernel_size, padding=(kernel_size//2), bias=bias))\n",
    "\n",
    "        self.CA = CALayer(n_feat, reduction)\n",
    "        self.body = nn.Sequential(*modules_body)\n",
    "        self.act = act\n",
    "\n",
    "    def forward(self, x):\n",
    "        res = self.body(x)\n",
    "        res = self.CA(res)\n",
    "        res += x\n",
    "        return res\n",
    "\n",
    "# --- Encoder ---\n",
    "class Encoder(nn.Module):\n",
    "    def __init__(self, n_feat, kernel_size=3, reduction=16, act=nn.ReLU(True), bias=True, scale=2, num_cab=2):\n",
    "        super(Encoder, self).__init__()\n",
    "        modules_body = []\n",
    "        for _ in range(num_cab):\n",
    "             modules_body.append(CAB(n_feat, kernel_size, reduction, bias=bias, act=act))\n",
    "        self.body = nn.Sequential(*modules_body)\n",
    "        self.down = nn.Sequential(\n",
    "            nn.Upsample(scale_factor=1/scale, mode='bilinear', align_corners=False),\n",
    "            nn.Conv2d(n_feat, n_feat * scale, 1, stride=1, padding=0, bias=bias) # Increase channels after downsampling\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        res = self.body(x)\n",
    "        out = self.down(res)\n",
    "        return res, out # Return features before and after downsampling\n",
    "\n",
    "# --- Decoder ---\n",
    "class Decoder(nn.Module):\n",
    "    def __init__(self, n_feat, kernel_size=3, reduction=16, act=nn.ReLU(True), bias=True, scale=2, num_cab=2):\n",
    "        super(Decoder, self).__init__()\n",
    "        modules_body = []\n",
    "        for _ in range(num_cab):\n",
    "             modules_body.append(CAB(n_feat, kernel_size, reduction, bias=bias, act=act))\n",
    "        self.body = nn.Sequential(*modules_body)\n",
    "        self.up = nn.Sequential(\n",
    "             nn.Upsample(scale_factor=scale, mode='bilinear', align_corners=False),\n",
    "             nn.Conv2d(n_feat, n_feat // scale, 1, stride=1, padding=0, bias=bias) # Decrease channels after upsampling\n",
    "        )\n",
    "        # CAB for refining skip connection\n",
    "        self.skip_attn = CAB(n_feat // scale, kernel_size, reduction, bias=bias, act=act)\n",
    "        self.conv_fuse = nn.Conv2d(n_feat, n_feat // scale, kernel_size, padding=(kernel_size//2), bias=bias) # Fuse upsampled and skip\n",
    "\n",
    "    def forward(self, x, skip):\n",
    "        # Upsample and reduce channels\n",
    "        x_up = self.up(x)\n",
    "        # Refine skip connection features\n",
    "        skip_refined = self.skip_attn(skip)\n",
    "        # Concatenate and fuse\n",
    "        out = self.conv_fuse(torch.cat([x_up, skip_refined], dim=1))\n",
    "        out = self.body(out)\n",
    "        return out\n",
    "\n",
    "# --- Original Resolution Block (ORB) ---\n",
    "class ORB(nn.Module):\n",
    "    def __init__(self, n_feat, kernel_size=3, reduction=16, act=nn.ReLU(True), bias=True, num_cab=8):\n",
    "        super(ORB, self).__init__()\n",
    "        modules_body = []\n",
    "        for _ in range(num_cab):\n",
    "            modules_body.append(CAB(n_feat, kernel_size, reduction, bias=bias, act=act))\n",
    "        self.body = nn.Sequential(*modules_body)\n",
    "\n",
    "    def forward(self, x):\n",
    "        res = self.body(x)\n",
    "        res += x\n",
    "        return res\n",
    "\n",
    "# --- Original Resolution Subnetwork (ORSNet) ---\n",
    "class ORSNet(nn.Module):\n",
    "    def __init__(self, n_feat, scale_ors=1, num_orb=3, num_cab_orb=8, kernel_size=3, reduction=16, act=nn.ReLU(True), bias=True):\n",
    "        super(ORSNet, self).__init__()\n",
    "        modules_body = []\n",
    "        for _ in range(num_orb):\n",
    "            modules_body.append(ORB(n_feat, kernel_size, reduction, bias=bias, act=act, num_cab=num_cab_orb))\n",
    "        self.body = nn.Sequential(*modules_body)\n",
    "\n",
    "        # Upsampling layers to integrate features from Stage 2\n",
    "        self.upsample1 = nn.Sequential(\n",
    "            nn.Upsample(scale_factor=2, mode='bilinear', align_corners=False),\n",
    "            nn.Conv2d(n_feat * 2, n_feat, 1, 1, 0, bias=bias) # From Enc Level 2\n",
    "        )\n",
    "        self.upsample2 = nn.Sequential(\n",
    "            nn.Upsample(scale_factor=4, mode='bilinear', align_corners=False),\n",
    "            nn.Conv2d(n_feat * 4, n_feat, 1, 1, 0, bias=bias) # From Enc Level 3\n",
    "        )\n",
    "        self.conv_refine1 = nn.Conv2d(n_feat, n_feat, kernel_size, padding=(kernel_size//2), bias=bias)\n",
    "        self.conv_refine2 = nn.Conv2d(n_feat, n_feat, kernel_size, padding=(kernel_size//2), bias=bias)\n",
    "        self.conv_final_fuse = nn.Conv2d(n_feat * 3, n_feat, kernel_size, padding=(kernel_size//2), bias=bias)\n",
    "\n",
    "\n",
    "    def forward(self, x, enc2_ft, enc3_ft):\n",
    "        # Upsample and refine Stage 2 features\n",
    "        up1 = self.conv_refine1(self.upsample1(enc2_ft))\n",
    "        up2 = self.conv_refine2(self.upsample2(enc3_ft))\n",
    "        # Concatenate with ORSNet input features\n",
    "        fused = torch.cat([x, up1, up2], dim=1)\n",
    "        fused = self.conv_final_fuse(fused)\n",
    "        # Pass through ORBs\n",
    "        out = self.body(fused)\n",
    "        return out\n",
    "\n",
    "# --- Supervised Attention Module (SAM) ---\n",
    "class SAM(nn.Module):\n",
    "    def __init__(self, n_feat, kernel_size=3, bias=True):\n",
    "        super(SAM,self).__init__()\n",
    "        self.conv1 = nn.Conv2d(n_feat, n_feat, kernel_size, padding=(kernel_size//2), bias=bias)\n",
    "        self.conv2 = nn.Conv2d(n_feat, 1, kernel_size, padding=(kernel_size//2), bias=bias) # Output residual image\n",
    "        self.conv3 = nn.Conv2d(1, n_feat, kernel_size, padding=(kernel_size//2), bias=bias) # Process residual for attention map\n",
    "        self.sigmoid = nn.Sigmoid()\n",
    "\n",
    "    def forward(self, x, x_img):\n",
    "        # x: features from previous stage decoder/ORSNet\n",
    "        # x_img: input image to the current stage (potentially degraded)\n",
    "        x1 = self.conv1(x)\n",
    "        # Generate intermediate restored image (residual prediction)\n",
    "        img = self.conv2(x) + x_img # Add residual to the input image\n",
    "        # Generate attention map from the intermediate restored image\n",
    "        attention_map = self.sigmoid(self.conv3(img))\n",
    "        # Apply attention to features and add skip connection\n",
    "        x_out = x1 * attention_map + x1 # Use x1 for skip connection as well\n",
    "        return x_out, img # Return attention-enhanced features and intermediate image\n",
    "\n",
    "# --- Cross-Stage Feature Fusion (CSFF - Placeholder, implemented within Stage 2) ---\n",
    "# CSFF involves 1x1 convolutions applied to Stage 1 features before adding to Stage 2 features.\n",
    "# This is integrated directly into the Stage 2 Encoder forward pass.\n",
    "\n",
    "# --- MPRNet ---\n",
    "class MPRNet(nn.Module):\n",
    "    def __init__(self, in_c=1, out_c=1, n_feat=80, kernel_size=3, reduction=16, num_cab=2, num_orb=3, num_cab_orb=8, bias=True):\n",
    "        super(MPRNet, self).__init__()\n",
    "        act = nn.ReLU(True)\n",
    "        self.n_feat = n_feat\n",
    "\n",
    "        # --- Shallow Feature Extraction ---\n",
    "        self.shallow_feat1 = nn.Sequential(nn.Conv2d(in_c, n_feat, kernel_size, padding=(kernel_size//2), bias=bias), CAB(n_feat,kernel_size, reduction, bias=bias, act=act))\n",
    "        self.shallow_feat2 = nn.Sequential(nn.Conv2d(in_c, n_feat, kernel_size, padding=(kernel_size//2), bias=bias), CAB(n_feat,kernel_size, reduction, bias=bias, act=act))\n",
    "        self.shallow_feat3 = nn.Sequential(nn.Conv2d(in_c, n_feat, kernel_size, padding=(kernel_size//2), bias=bias), CAB(n_feat,kernel_size, reduction, bias=bias, act=act))\n",
    "\n",
    "        # --- Stage 1: Encoder-Decoder ---\n",
    "        self.enc1_1 = Encoder(n_feat, kernel_size, reduction, act, bias, scale=2, num_cab=num_cab) # 1 -> n_feat*2\n",
    "        self.enc1_2 = Encoder(n_feat*2, kernel_size, reduction, act, bias, scale=2, num_cab=num_cab) # n_feat*2 -> n_feat*4\n",
    "        self.dec1_2 = Decoder(n_feat*4, kernel_size, reduction, act, bias, scale=2, num_cab=num_cab) # n_feat*4 -> n_feat*2\n",
    "        self.dec1_1 = Decoder(n_feat*2, kernel_size, reduction, act, bias, scale=2, num_cab=num_cab) # n_feat*2 -> n_feat\n",
    "\n",
    "        # --- SAM 1 -> 2 ---\n",
    "        self.sam12 = SAM(n_feat, kernel_size=kernel_size, bias=bias)\n",
    "        # Convolution to refine concatenated features (Shallow Stage 2 + SAM output)\n",
    "        self.concat_refine12 = nn.Conv2d(n_feat*2, n_feat, kernel_size, padding=(kernel_size//2), bias=bias)\n",
    "\n",
    "        # --- Stage 2: Encoder-Decoder with CSFF ---\n",
    "        self.enc2_1 = Encoder(n_feat, kernel_size, reduction, act, bias, scale=2, num_cab=num_cab)\n",
    "        self.enc2_2 = Encoder(n_feat*2, kernel_size, reduction, act, bias, scale=2, num_cab=num_cab)\n",
    "        self.dec2_2 = Decoder(n_feat*4, kernel_size, reduction, act, bias, scale=2, num_cab=num_cab)\n",
    "        self.dec2_1 = Decoder(n_feat*2, kernel_size, reduction, act, bias, scale=2, num_cab=num_cab)\n",
    "        # CSFF Convolutions (1x1)\n",
    "        self.csff_enc1 = nn.Conv2d(n_feat*2, n_feat*2, kernel_size=1, bias=bias)\n",
    "        self.csff_enc2 = nn.Conv2d(n_feat*4, n_feat*4, kernel_size=1, bias=bias)\n",
    "        self.csff_dec1 = nn.Conv2d(n_feat*2, n_feat*2, kernel_size=1, bias=bias)\n",
    "        self.csff_dec2 = nn.Conv2d(n_feat, n_feat, kernel_size=1, bias=bias) # Applied before final decoder conv\n",
    "\n",
    "\n",
    "        # --- SAM 2 -> 3 ---\n",
    "        self.sam23 = SAM(n_feat, kernel_size=kernel_size, bias=bias)\n",
    "        # Convolution to refine concatenated features (Shallow Stage 3 + SAM output)\n",
    "        self.concat_refine23 = nn.Conv2d(n_feat*2, n_feat, kernel_size, padding=(kernel_size//2), bias=bias)\n",
    "\n",
    "        # --- Stage 3: ORSNet ---\n",
    "        self.orsnet = ORSNet(n_feat, scale_ors=1, num_orb=num_orb, num_cab_orb=num_cab_orb, kernel_size=kernel_size, reduction=reduction, act=act, bias=bias)\n",
    "\n",
    "        # --- Final Convolution ---\n",
    "        self.conv_out = nn.Conv2d(n_feat, out_c, kernel_size, padding=(kernel_size//2), bias=bias)\n",
    "\n",
    "\n",
    "    def forward(self, x_orig):\n",
    "        # x_orig: Input image (256x256, output from SRNet)\n",
    "        B, C, H, W = x_orig.shape\n",
    "        intermediate_outputs = []\n",
    "\n",
    "        # --- Stage 1 ---\n",
    "        # Process Full Image Shallow Features (although description says patches, let's start simpler)\n",
    "        feat1_shallow = self.shallow_feat1(x_orig)\n",
    "        # Encoder\n",
    "        enc1_ft1, enc1_out1 = self.enc1_1(feat1_shallow) # n_feat -> n_feat*2\n",
    "        enc1_ft2, enc1_out2 = self.enc1_2(enc1_out1)    # n_feat*2 -> n_feat*4\n",
    "        # Decoder\n",
    "        dec1_out2 = self.dec1_2(enc1_out2, enc1_ft2) # n_feat*4 -> n_feat*2\n",
    "        dec1_out1 = self.dec1_1(dec1_out2, enc1_ft1) # n_feat*2 -> n_feat\n",
    "        # SAM 1->2\n",
    "        feat1_sam, img1_sam = self.sam12(dec1_out1, x_orig)\n",
    "        intermediate_outputs.append(img1_sam)\n",
    "\n",
    "        # --- Stage 2 ---\n",
    "        feat2_shallow = self.shallow_feat2(x_orig)\n",
    "        # Refine concatenated features\n",
    "        feat2_in = self.concat_refine12(torch.cat([feat2_shallow, feat1_sam], dim=1))\n",
    "        # Encoder with CSFF\n",
    "        enc2_ft1_raw, enc2_out1_raw = self.enc2_1(feat2_in)\n",
    "        enc2_ft1 = enc2_ft1_raw + self.csff_enc1(enc1_ft1) # Apply CSFF\n",
    "        enc2_out1 = enc2_out1_raw # No downsampling here in CSFF addition\n",
    "        enc2_ft2_raw, enc2_out2_raw = self.enc2_2(enc2_out1)\n",
    "        enc2_ft2 = enc2_ft2_raw + self.csff_enc2(enc1_ft2) # Apply CSFF\n",
    "        enc2_out2 = enc2_out2_raw # No downsampling here\n",
    "        # Decoder with CSFF\n",
    "        dec2_out2_raw = self.dec2_2(enc2_out2, enc2_ft2)\n",
    "        dec2_out2 = dec2_out2_raw + self.csff_dec1(dec1_out2) # Apply CSFF\n",
    "        dec2_out1_raw = self.dec2_1(dec2_out2, enc2_ft1)\n",
    "        dec2_out1 = dec2_out1_raw + self.csff_dec2(dec1_out1) # Apply CSFF before final conv in decoder\n",
    "        # SAM 2->3\n",
    "        feat2_sam, img2_sam = self.sam23(dec2_out1, x_orig)\n",
    "        intermediate_outputs.append(img2_sam)\n",
    "\n",
    "        # --- Stage 3 ---\n",
    "        feat3_shallow = self.shallow_feat3(x_orig)\n",
    "        # Refine concatenated features\n",
    "        feat3_in = self.concat_refine23(torch.cat([feat3_shallow, feat2_sam], dim=1))\n",
    "        # ORSNet processing\n",
    "        ors_out = self.orsnet(feat3_in, enc2_ft1, enc2_ft2) # Pass Stage 2 features needed for ORSNet upsampling\n",
    "\n",
    "        # --- Final Output ---\n",
    "        # Residual connection: Add final output to the original input\n",
    "        final_out = self.conv_out(ors_out) + x_orig\n",
    "        intermediate_outputs.append(final_out) # Add final output as the last intermediate\n",
    "\n",
    "        # Return final output and intermediate restored images for potential loss calculation\n",
    "        return final_out, intermediate_outputs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "___\n",
    "### Create a function to execute training. Note, we will call this function later."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Helper function for one training epoch (Generalized for SRNet or MPRNet)\n",
    "def train_one_epoch(model, train_loader, criterion, optimizer, device, scaler, srnet_model=None):\n",
    "    model.train() # Set the model being trained to training mode\n",
    "    if srnet_model:\n",
    "        srnet_model.eval() # Keep SRNet in eval mode if training MPRNet\n",
    "\n",
    "    total_train_loss = 0.0\n",
    "    epoch_pbar = tqdm(train_loader, desc=\"Epoch Training\", leave=False)\n",
    "    for x_tr_batch, y_tr_batch in epoch_pbar:\n",
    "        x_tr_batch, y_tr_batch = x_tr_batch.to(device), y_tr_batch.to(device) # x is 128, y is 256\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        # --- Determine Input for the Current Model ---\n",
    "        if srnet_model: # Training MPRNet\n",
    "            with torch.no_grad(): # Generate SRNet output without tracking gradients\n",
    "                # Use autocast for SRNet inference if desired\n",
    "                with torch.amp.autocast(device_type=device.type, dtype=torch.float16, enabled=scaler.is_enabled()):\n",
    "                     input_batch = srnet_model(x_tr_batch) # SRNet output (256x256) is input to MPRNet\n",
    "            input_batch = input_batch.detach() # Ensure no gradients flow back to SRNet\n",
    "        else: # Training SRNet\n",
    "            input_batch = x_tr_batch # SRNet input is the low-res image (128x128)\n",
    "\n",
    "        # --- Mixed Precision: Forward pass for the model being trained ---\n",
    "        with torch.amp.autocast(device_type=device.type, dtype=torch.float16, enabled=scaler.is_enabled()):\n",
    "            # Model output can be single (SRNet) or list (MPRNet)\n",
    "            y_hat_tr_batch = model(input_batch)\n",
    "            # Loss function handles single tensor or list of tensors\n",
    "            loss = criterion(y_hat_tr_batch, y_tr_batch) # y_tr_batch is the high-res target (256x256)\n",
    "\n",
    "        # --- Mixed Precision: Scale loss and backward pass ---\n",
    "        scaler.scale(loss).backward()\n",
    "\n",
    "        # --- Mixed Precision: Scaler step and update ---\n",
    "        scaler.step(optimizer)\n",
    "        scaler.update()\n",
    "\n",
    "        total_train_loss += loss.item()\n",
    "        epoch_pbar.set_postfix({'Batch Loss': f'{loss.item():.4f}'})\n",
    "\n",
    "\n",
    "    avg_train_loss = total_train_loss / len(train_loader)\n",
    "    return avg_train_loss\n",
    "\n",
    "# Helper function for one validation epoch (Generalized)\n",
    "def validate_one_epoch(model, val_loader, criterion, psnr_metric, ssim_metric, device, srnet_model=None):\n",
    "    model.eval() # Set the model being evaluated to eval mode\n",
    "    if srnet_model:\n",
    "        srnet_model.eval() # Keep SRNet in eval mode\n",
    "\n",
    "    total_val_loss = 0.0\n",
    "    total_val_psnr = 0.0\n",
    "    total_val_ssim = 0.0\n",
    "    num_samples = 0\n",
    "\n",
    "    with torch.no_grad():\n",
    "        val_pbar = tqdm(val_loader, desc=\"Validation\", leave=False)\n",
    "        for x_val_batch, y_val_batch in val_pbar:\n",
    "            x_val_batch, y_val_batch = x_val_batch.to(device), y_val_batch.to(device) # x is 128, y is 256\n",
    "            batch_size = x_val_batch.size(0)\n",
    "            num_samples += batch_size\n",
    "\n",
    "            # --- Determine Input for the Current Model ---\n",
    "            if srnet_model: # Evaluating MPRNet\n",
    "                # Use autocast for SRNet inference if desired\n",
    "                with torch.amp.autocast(device_type=device.type, dtype=torch.float16, enabled=True):\n",
    "                    input_batch = srnet_model(x_val_batch) # SRNet output (256x256)\n",
    "            else: # Evaluating SRNet\n",
    "                input_batch = x_val_batch # SRNet input is the low-res image (128x128)\n",
    "\n",
    "            # --- Mixed Precision: Validation forward pass ---\n",
    "            with torch.amp.autocast(device_type=device.type, dtype=torch.float16, enabled=True):\n",
    "                y_hat_val_batch = model(input_batch) # Can be single tensor or list\n",
    "\n",
    "                # If model outputs list (MPRNet), use the final output for metrics/loss\n",
    "                final_output = y_hat_val_batch[0] if isinstance(y_hat_val_batch, list) else y_hat_val_batch\n",
    "                # Use the same criterion for validation loss (pass full output if list)\n",
    "                val_loss = criterion(y_hat_val_batch, y_val_batch)\n",
    "\n",
    "            total_val_loss += val_loss.item() * batch_size # Accumulate total loss\n",
    "\n",
    "            # --- Calculate metrics using the final output ---\n",
    "            psnr = psnr_metric(final_output, y_val_batch)\n",
    "            ssim = ssim_metric(final_output, y_val_batch)\n",
    "\n",
    "            total_val_psnr += psnr.item() * batch_size\n",
    "            total_val_ssim += ssim.item() * batch_size\n",
    "            val_pbar.set_postfix({'Batch PSNR': f'{psnr.item():.2f}', 'Batch SSIM': f'{ssim.item():.4f}'})\n",
    "\n",
    "\n",
    "    avg_val_loss = total_val_loss / num_samples # Average loss per image\n",
    "    avg_val_psnr = total_val_psnr / num_samples\n",
    "    avg_val_ssim = total_val_ssim / num_samples\n",
    "    avg_val_score = avg_val_psnr + (40 * avg_val_ssim)\n",
    "\n",
    "    return avg_val_loss, avg_val_score, avg_val_psnr, avg_val_ssim\n",
    "\n",
    "# --- Updated train_model function to handle sequential training ---\n",
    "def train_model(model_name, model, opt, criterion, scheduler, train_loader, val_loader, num_epoch, patience, device, save_dir='saved_models', scaler=None, srnet_model=None):\n",
    "    \"\"\"Trains either SRNet or MPRNet.\"\"\"\n",
    "    print(f\"\\n--- Starting Training for {model_name} ---\")\n",
    "    os.makedirs(save_dir, exist_ok=True)\n",
    "    epoch_nums, avg_train_losses, avg_val_losses, avg_val_scores = [], [], [], []\n",
    "    best_val_score = -float('inf')\n",
    "    epochs_no_improve = 0\n",
    "    best_model_state = None\n",
    "\n",
    "    psnr_metric = PeakSignalNoiseRatio().to(device)\n",
    "    ssim_metric = StructuralSimilarityIndexMeasure(data_range=1.0).to(device)\n",
    "\n",
    "    overall_pbar = tqdm(range(num_epoch), desc=f\"{model_name} Training Progress\")\n",
    "\n",
    "    for epoch in overall_pbar:\n",
    "        # --- Training ---\n",
    "        avg_train_loss = train_one_epoch(model, train_loader, criterion, opt, device, scaler, srnet_model=srnet_model)\n",
    "\n",
    "        # --- Validation ---\n",
    "        avg_val_loss, avg_val_score, avg_val_psnr, avg_val_ssim = validate_one_epoch(\n",
    "            model, val_loader, criterion, psnr_metric, ssim_metric, device, srnet_model=srnet_model\n",
    "        )\n",
    "\n",
    "        # Store metrics for plot\n",
    "        epoch_nums.append(epoch + 1)\n",
    "        avg_train_losses.append(avg_train_loss)\n",
    "        avg_val_losses.append(avg_val_loss)\n",
    "        avg_val_scores.append(avg_val_score)\n",
    "\n",
    "        # --- Update Progress Bar ---\n",
    "        current_lr = opt.param_groups[0]['lr']\n",
    "        overall_pbar.set_description(\n",
    "            f\"{model_name} LR: {current_lr:.1e} | Train Loss: {avg_train_loss:.4f} | \"\n",
    "            f\"Val Score: {avg_val_score:.2f} | Best: {best_val_score:.2f} | \"\n",
    "            f\"No Improve: {epochs_no_improve}\"\n",
    "        )\n",
    "        overall_pbar.set_postfix({\n",
    "            'Val PSNR': f'{avg_val_psnr:.2f}',\n",
    "            'Val SSIM': f'{avg_val_ssim:.4f}'\n",
    "        })\n",
    "\n",
    "\n",
    "        # --- Learning Rate Scheduler Step ---\n",
    "        scheduler.step(avg_val_score)\n",
    "\n",
    "        # --- Checkpointing and Early Stopping ---\n",
    "        if avg_val_score > best_val_score:\n",
    "            best_val_score = avg_val_score\n",
    "            epochs_no_improve = 0\n",
    "            best_model_state = copy.deepcopy(model.state_dict())\n",
    "            # Save the best model immediately\n",
    "            timestamp = datetime.datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
    "            score_str = f\"{best_val_score:.2f}\".replace('.', 'p')\n",
    "            filename = f\"best_{model_name}_score_{score_str}_epoch_{epoch+1}_time_{timestamp}.pth\"\n",
    "            save_path = os.path.join(save_dir, filename)\n",
    "            torch.save(best_model_state, save_path)\n",
    "            # print(f\"  New best model saved to '{save_path}'\")\n",
    "        else:\n",
    "            epochs_no_improve += 1\n",
    "\n",
    "        if epochs_no_improve >= patience:\n",
    "            print(f\"\\nEarly stopping triggered for {model_name} after {patience} epochs with no improvement.\")\n",
    "            break\n",
    "\n",
    "    # --- Load Best Model State ---\n",
    "    if best_model_state:\n",
    "        print(f\"\\nTraining finished for {model_name}. Loading best model state with score: {best_val_score:.2f}\")\n",
    "        model.load_state_dict(best_model_state) # Load best state into model\n",
    "    else:\n",
    "        print(f\"\\nTraining finished for {model_name}. No best model state was found or saved.\")\n",
    "\n",
    "    # --- Final Plot ---\n",
    "    fig_final, ax1_final = plt.subplots(figsize=(10, 6))\n",
    "    ax2_final = ax1_final.twinx()\n",
    "    line1_final, = ax1_final.plot(epoch_nums, avg_train_losses, 'r-', label='Training Loss')\n",
    "    line2_final, = ax1_final.plot(epoch_nums, avg_val_losses, 'r--', label='Validation Loss')\n",
    "    line3_final, = ax2_final.plot(epoch_nums, avg_val_scores, 'b-', label='Validation Score')\n",
    "    ax1_final.set_xlabel('Epochs')\n",
    "    ax1_final.set_ylabel('Loss', color='tab:red')\n",
    "    ax1_final.tick_params(axis='y', labelcolor='tab:red')\n",
    "    ax2_final.set_ylabel('Validation Score (PSNR + 40*SSIM)', color='tab:blue')\n",
    "    ax2_final.tick_params(axis='y', labelcolor='tab:blue')\n",
    "    ax1_final.legend(handles=[line1_final, line2_final], loc='upper left')\n",
    "    ax2_final.legend(handles=[line3_final], loc='upper right')\n",
    "    ax1_final.grid(True)\n",
    "    plt.title('Final Training Progress')\n",
    "    fig_final.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "    return best_val_score, best_model_state"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "___\n",
    "### Now, let us define hyperparameters and train the network. \n",
    "\n",
    "#### Note, in addition to the parameters that controls the network architecture or the training process, you need to select/initialize (i) a data loader, (ii) a model, (iii) an optimizer, and (iv) a loss function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of original training images: 240\n",
      "Number of training images after augmentation: 1920\n",
      "Number of original validation images: 30\n",
      "Number of validation images (no augmentation): 30\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8282f07ce1384fda931ec4147d982e90",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Overall Training Progress:   0%|          | 0/200 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# --- General Hyperparameters ---\n",
    "batch_size = 5 # Adjust based on GPU memory, especially for MPRNet\n",
    "num_epoch = 200\n",
    "patience_early_stopping = 15\n",
    "save_dir = 'saved_models'\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "scaler = torch.amp.GradScaler(enabled=(device.type == 'cuda')) # Enable scaler only for CUDA\n",
    "\n",
    "# --- Stage 1: SRNet Configuration ---\n",
    "srnet_lr = 1e-4\n",
    "srnet_scheduler_patience = 5\n",
    "srnet_scheduler_factor = 0.1\n",
    "srnet_loss_alpha = 0.5\n",
    "srnet_loss_beta = 0.5\n",
    "skip_srnet_training = True # <<<<<<<<<<<<< SET TO True TO LOAD PRE-TRAINED SRNET <<<<<<<<<<<<<\n",
    "srnet_pretrained_path = 'saved_models/best_model_score_75p00_time_20250422_150000.pth' # <<<<<<<<<<<<< PATH TO YOUR SAVED SRNet MODEL <<<<<<<<<<<<<\n",
    "\n",
    "# --- Stage 2: MPRNet Configuration ---\n",
    "mprnet_lr = 1e-5 # Often requires smaller LR than the first stage\n",
    "mprnet_scheduler_patience = 7\n",
    "mprnet_scheduler_factor = 0.5\n",
    "mprnet_loss_alpha = 0.5\n",
    "mprnet_loss_beta = 0.5\n",
    "# Weights for MPRNet intermediate losses (Stage1 SAM, Stage2 SAM, Final Output)\n",
    "mprnet_stage_weights = [0.2, 0.3, 0.5] # Example weights, adjust as needed\n",
    "mprnet_n_feat = 64 # MPRNet can be memory intensive, adjust features if needed\n",
    "mprnet_num_cab = 2 # Number of CABs per Encoder/Decoder level\n",
    "mprnet_num_orb = 2 # Number of ORBs in Stage 3\n",
    "mprnet_num_cab_orb = 4 # Number of CABs per ORB\n",
    "\n",
    "\n",
    "# --- Setup DataLoaders ---\n",
    "# Define transform (ensure it's defined before datasets)\n",
    "transform = transforms.Compose([transforms.ToTensor()])\n",
    "# Create datasets\n",
    "train_dataset = TIFFDataset('data/train/high-res', 'data/train/low-res', transform=transform, augment=True)\n",
    "val_dataset = TIFFDataset('data/val/high-res', 'data/val/low-res', transform=transform, augment=False)\n",
    "# Create DataLoaders\n",
    "train_loader = create_loader(train_dataset, batch_size, shuffle_data=True)\n",
    "val_loader = create_loader(val_dataset, batch_size, shuffle_data=False)\n",
    "print(f\"Training data: {len(train_dataset)} samples\")\n",
    "print(f\"Validation data: {len(val_dataset)} samples\")\n",
    "\n",
    "\n",
    "# --- Stage 1: SRNet Training (or Loading) ---\n",
    "print(\"\\n--- Preparing Stage 1: SRNet ---\")\n",
    "srnet_model = SRNet().to(device)\n",
    "best_srnet_score = -1\n",
    "best_srnet_state = None\n",
    "\n",
    "if skip_srnet_training:\n",
    "    if os.path.exists(srnet_pretrained_path):\n",
    "        print(f\"Loading pre-trained SRNet from: {srnet_pretrained_path}\")\n",
    "        srnet_model.load_state_dict(torch.load(srnet_pretrained_path, map_location=device))\n",
    "        # Optional: Evaluate the loaded model\n",
    "        print(\"Evaluating loaded SRNet model...\")\n",
    "        temp_criterion = CombinedLoss(alpha=srnet_loss_alpha, beta=srnet_loss_beta, device=device).to(device)\n",
    "        psnr_metric_eval = PeakSignalNoiseRatio().to(device)\n",
    "        ssim_metric_eval = StructuralSimilarityIndexMeasure(data_range=1.0).to(device)\n",
    "        _, best_srnet_score, _, _ = validate_one_epoch(srnet_model, val_loader, temp_criterion, psnr_metric_eval, ssim_metric_eval, device)\n",
    "        print(f\"Loaded SRNet validation score: {best_srnet_score:.2f}\")\n",
    "        best_srnet_state = srnet_model.state_dict() # Keep the state\n",
    "    else:\n",
    "        print(f\"Error: Pre-trained SRNet model not found at {srnet_pretrained_path}. Exiting.\")\n",
    "        exit()\n",
    "else:\n",
    "    print(\"Configuring SRNet for training...\")\n",
    "    srnet_opt = optim.Adam(srnet_model.parameters(), lr=srnet_lr)\n",
    "    srnet_criterion = CombinedLoss(alpha=srnet_loss_alpha, beta=srnet_loss_beta, device=device).to(device)\n",
    "    srnet_scheduler = ReduceLROnPlateau(srnet_opt, mode='max', factor=srnet_scheduler_factor, patience=srnet_scheduler_patience)\n",
    "\n",
    "    best_srnet_score, best_srnet_state = train_model(\n",
    "        model_name=\"SRNet\",\n",
    "        model=srnet_model,\n",
    "        opt=srnet_opt,\n",
    "        criterion=srnet_criterion,\n",
    "        scheduler=srnet_scheduler,\n",
    "        train_loader=train_loader,\n",
    "        val_loader=val_loader,\n",
    "        num_epoch=num_epoch,\n",
    "        patience=patience_early_stopping,\n",
    "        device=device,\n",
    "        save_dir=save_dir,\n",
    "        scaler=scaler\n",
    "    )\n",
    "    if best_srnet_state:\n",
    "        srnet_model.load_state_dict(best_srnet_state) # Ensure model has best state\n",
    "    else:\n",
    "         print(\"SRNet training finished without finding a best state. MPRNet training may be affected.\")\n",
    "         # Decide how to proceed - exit or use the last state? For now, we proceed.\n",
    "\n",
    "\n",
    "# --- Stage 2: MPRNet Training ---\n",
    "print(\"\\n--- Preparing Stage 2: MPRNet ---\")\n",
    "# Ensure SRNet is in evaluation mode and gradients are off\n",
    "srnet_model.eval()\n",
    "for param in srnet_model.parameters():\n",
    "    param.requires_grad = False\n",
    "\n",
    "# Initialize MPRNet\n",
    "mprnet_model = MPRNet(in_c=1, out_c=1, n_feat=mprnet_n_feat, kernel_size=3, reduction=16,\n",
    "                      num_cab=mprnet_num_cab, num_orb=mprnet_num_orb, num_cab_orb=mprnet_num_cab_orb, bias=True).to(device)\n",
    "\n",
    "mprnet_opt = optim.Adam(mprnet_model.parameters(), lr=mprnet_lr)\n",
    "# Use stage weights in the criterion for MPRNet\n",
    "mprnet_criterion = CombinedLoss(alpha=mprnet_loss_alpha, beta=mprnet_loss_beta, device=device, stage_weights=mprnet_stage_weights).to(device)\n",
    "mprnet_scheduler = ReduceLROnPlateau(mprnet_opt, mode='max', factor=mprnet_scheduler_factor, patience=mprnet_scheduler_patience)\n",
    "\n",
    "best_mprnet_score, best_mprnet_state = train_model(\n",
    "    model_name=\"MPRNet\",\n",
    "    model=mprnet_model,\n",
    "    opt=mprnet_opt,\n",
    "    criterion=mprnet_criterion,\n",
    "    scheduler=mprnet_scheduler,\n",
    "    train_loader=train_loader,\n",
    "    val_loader=val_loader,\n",
    "    num_epoch=num_epoch, # Can use the same or different number of epochs\n",
    "    patience=patience_early_stopping, # Can use the same or different patience\n",
    "    device=device,\n",
    "    save_dir=save_dir,\n",
    "    scaler=scaler,\n",
    "    srnet_model=srnet_model # Pass the frozen SRNet model\n",
    ")\n",
    "\n",
    "if best_mprnet_state:\n",
    "    mprnet_model.load_state_dict(best_mprnet_state) # Ensure model has best state\n",
    "else:\n",
    "    print(\"MPRNet training finished without finding a best state.\")\n",
    "\n",
    "print(f\"\\nSRNet Best Validation Score: {best_srnet_score:.2f}\")\n",
    "print(f\"MPRNet Best Validation Score: {best_mprnet_score:.2f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "___\n",
    "### Apply it one of the validation image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load one (low-res, high-res) image pair from validation dataset\n",
    "val_low_res, val_high_res = val_dataset[1]  # Input (128x128), Ground truth (256x256)\n",
    "val_low_res, val_high_res = val_low_res.to(device), val_high_res.to(device)\n",
    "\n",
    "# Keep the interpolated version ONLY for visualization comparison\n",
    "val_low_res_interpolated = torch.nn.functional.interpolate(val_low_res.unsqueeze(0), scale_factor=2, mode='bicubic', align_corners=False).squeeze(0)\n",
    "\n",
    "# Apply the trained models sequentially\n",
    "with torch.no_grad():\n",
    "    with torch.amp.autocast(device_type=device.type, dtype=torch.float16, enabled=True):\n",
    "        val_sr_res = srnet_model(val_low_res.unsqueeze(0))\n",
    "        val_mpr_res_full = mprnet_model(val_sr_res)\n",
    "        # Use final output from MPRNet if it returns a list\n",
    "        val_mpr_res = val_mpr_res_full[0] if isinstance(val_mpr_res_full, list) else val_mpr_res_full\n",
    "\n",
    "val_sr_res = val_sr_res.detach().squeeze(0)\n",
    "val_mpr_res = val_mpr_res.detach().squeeze(0)\n",
    "\n",
    "\n",
    "# Convert tensors to numpy for visualization\n",
    "val_low_res_np = val_low_res_interpolated.squeeze().cpu().numpy()\n",
    "val_high_res_np = val_high_res.squeeze().cpu().numpy()\n",
    "val_sr_res_np = val_sr_res.squeeze().cpu().numpy()\n",
    "val_mpr_res_np = val_mpr_res.squeeze().cpu().numpy()\n",
    "\n",
    "\n",
    "# Plot example images and error maps\n",
    "fig, ax = plt.subplots(2, 4, figsize=(13, 7)) # Increased columns for MPRNet\n",
    "\n",
    "# Plot images\n",
    "ax[0, 0].imshow(val_high_res_np, cmap='gray', vmin=0, vmax=1)\n",
    "ax[0, 0].set_title('Ground Truth (High-Res)')\n",
    "ax[0, 0].axis('off')\n",
    "\n",
    "ax[0, 1].imshow(val_low_res_np, cmap='gray', vmin=0, vmax=1)\n",
    "ax[0, 1].set_title('Interpolated Low-Res')\n",
    "ax[0, 1].axis('off')\n",
    "\n",
    "ax[0, 2].imshow(val_sr_res_np, cmap='gray', vmin=0, vmax=1)\n",
    "ax[0, 2].set_title('SRNet Output')\n",
    "ax[0, 2].axis('off')\n",
    "\n",
    "ax[0, 3].imshow(val_mpr_res_np, cmap='gray', vmin=0, vmax=1)\n",
    "ax[0, 3].set_title('MPRNet Output (Final)')\n",
    "ax[0, 3].axis('off')\n",
    "\n",
    "\n",
    "# Error maps (multiplied by 5 for visibility)\n",
    "error_scale = 5\n",
    "ax[1, 0].imshow(error_scale * np.abs(val_high_res_np - val_high_res_np), cmap='gray', vmin=0, vmax=1)\n",
    "ax[1, 0].set_title('Error: GT vs GT')\n",
    "ax[1, 0].axis('off')\n",
    "ax[1, 0].text(0.02, 0.98, f'x{error_scale}', transform=ax[1, 0].transAxes, fontsize=12, va='top', ha='left', color='white')\n",
    "\n",
    "ax[1, 1].imshow(error_scale * np.abs(val_high_res_np - val_low_res_np), cmap='gray', vmin=0, vmax=1)\n",
    "ax[1, 1].set_title('Error: GT vs Interp.')\n",
    "ax[1, 1].axis('off')\n",
    "ax[1, 1].text(0.02, 0.98, f'x{error_scale}', transform=ax[1, 1].transAxes, fontsize=12, va='top', ha='left', color='white')\n",
    "\n",
    "ax[1, 2].imshow(error_scale * np.abs(val_high_res_np - val_sr_res_np), cmap='gray', vmin=0, vmax=1)\n",
    "ax[1, 2].set_title('Error: GT vs SRNet')\n",
    "ax[1, 2].axis('off')\n",
    "ax[1, 2].text(0.02, 0.98, f'x{error_scale}', transform=ax[1, 2].transAxes, fontsize=12, va='top', ha='left', color='white')\n",
    "\n",
    "ax[1, 3].imshow(error_scale * np.abs(val_high_res_np - val_mpr_res_np), cmap='gray', vmin=0, vmax=1)\n",
    "ax[1, 3].set_title('Error: GT vs MPRNet')\n",
    "ax[1, 3].axis('off')\n",
    "ax[1, 3].text(0.02, 0.98, f'x{error_scale}', transform=ax[1, 3].transAxes, fontsize=12, va='top', ha='left', color='white')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Compute PSNR and SSIM over the entire validation dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_final_models(srnet_model, mprnet_model, data_loader, device):\n",
    "    \"\"\"Computes metrics for SRNet and MPRNet outputs.\"\"\"\n",
    "    srnet_model.eval()\n",
    "    mprnet_model.eval()\n",
    "    psnr_metric = PeakSignalNoiseRatio().to(device)\n",
    "    ssim_metric = StructuralSimilarityIndexMeasure(data_range=1.0).to(device)\n",
    "\n",
    "    total_psnr_sr = 0.0\n",
    "    total_ssim_sr = 0.0\n",
    "    total_psnr_mpr = 0.0\n",
    "    total_ssim_mpr = 0.0\n",
    "    total_psnr_interp = 0.0\n",
    "    total_ssim_interp = 0.0\n",
    "    num_samples = 0\n",
    "\n",
    "    with torch.no_grad():\n",
    "        eval_pbar = tqdm(data_loader, desc=\"Final Evaluation\")\n",
    "        for x_batch, y_batch in eval_pbar:\n",
    "            x_batch, y_batch = x_batch.to(device), y_batch.to(device) # x is 128x128\n",
    "            batch_size_actual = x_batch.size(0)\n",
    "            num_samples += batch_size_actual\n",
    "\n",
    "            # Interpolated Baseline\n",
    "            x_batch_interpolated = torch.nn.functional.interpolate(x_batch, scale_factor=2, mode='bicubic', align_corners=False)\n",
    "\n",
    "            # --- Mixed Precision Evaluation ---\n",
    "            with torch.amp.autocast(device_type=device.type, dtype=torch.float16, enabled=True):\n",
    "                # SRNet Output\n",
    "                sr_output = srnet_model(x_batch) # 256x256\n",
    "\n",
    "                # MPRNet Output\n",
    "                mpr_output_full = mprnet_model(sr_output) # MPRNet takes SRNet output\n",
    "                # Use final output from MPRNet if it returns a list\n",
    "                mpr_output = mpr_output_full[0] if isinstance(mpr_output_full, list) else mpr_output_full\n",
    "\n",
    "\n",
    "            # --- Calculate Metrics ---\n",
    "            # Interpolated\n",
    "            psnr_interp = psnr_metric(x_batch_interpolated, y_batch)\n",
    "            ssim_interp = ssim_metric(x_batch_interpolated, y_batch)\n",
    "            total_psnr_interp += psnr_interp.item() * batch_size_actual\n",
    "            total_ssim_interp += ssim_interp.item() * batch_size_actual\n",
    "\n",
    "            # SRNet\n",
    "            psnr_sr = psnr_metric(sr_output, y_batch)\n",
    "            ssim_sr = ssim_metric(sr_output, y_batch)\n",
    "            total_psnr_sr += psnr_sr.item() * batch_size_actual\n",
    "            total_ssim_sr += ssim_sr.item() * batch_size_actual\n",
    "\n",
    "            # MPRNet\n",
    "            psnr_mpr = psnr_metric(mpr_output, y_batch)\n",
    "            ssim_mpr = ssim_metric(mpr_output, y_batch)\n",
    "            total_psnr_mpr += psnr_mpr.item() * batch_size_actual\n",
    "            total_ssim_mpr += ssim_mpr.item() * batch_size_actual\n",
    "\n",
    "            eval_pbar.set_postfix({'SR Score': f'{psnr_sr.item() + 40*ssim_sr.item():.2f}',\n",
    "                                   'MPR Score': f'{psnr_mpr.item() + 40*ssim_mpr.item():.2f}'})\n",
    "\n",
    "\n",
    "    # Calculate averages\n",
    "    avg_psnr_interp = total_psnr_interp / num_samples\n",
    "    avg_ssim_interp = total_ssim_interp / num_samples\n",
    "    avg_psnr_sr = total_psnr_sr / num_samples\n",
    "    avg_ssim_sr = total_ssim_sr / num_samples\n",
    "    avg_psnr_mpr = total_psnr_mpr / num_samples\n",
    "    avg_ssim_mpr = total_ssim_mpr / num_samples\n",
    "\n",
    "    # Calculate scores\n",
    "    avg_score_interp = avg_psnr_interp + (40 * avg_ssim_interp)\n",
    "    avg_score_sr = avg_psnr_sr + (40 * avg_ssim_sr)\n",
    "    avg_score_mpr = avg_psnr_mpr + (40 * avg_ssim_mpr)\n",
    "\n",
    "    # Print results\n",
    "    print(\"\\n--- Final Evaluation Results ---\")\n",
    "    print(f'Metric                | Interpolated |   SRNet   |  MPRNet  ')\n",
    "    print(f'----------------------|--------------|-----------|----------')\n",
    "    print(f'Average PSNR (dB)     | {avg_psnr_interp:12.2f} | {avg_psnr_sr:9.2f} | {avg_psnr_mpr:8.2f}')\n",
    "    print(f'Average SSIM          | {avg_ssim_interp:12.4f} | {avg_ssim_sr:9.4f} | {avg_ssim_mpr:8.4f}')\n",
    "    print(f'Average Score         | {avg_score_interp:12.2f} | {avg_score_sr:9.2f} | {avg_score_mpr:8.2f}')\n",
    "\n",
    "    return avg_score_sr, avg_score_mpr, avg_score_interp\n",
    "\n",
    "# --- Call the Final Evaluation Function ---\n",
    "# Ensure both models have their best states loaded before calling this\n",
    "# (This should be handled by the end of the training section)\n",
    "final_sr_score, final_mpr_score, final_interp_score = evaluate_final_models(\n",
    "    srnet_model, mprnet_model, val_loader, device\n",
    ")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
