{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "8d03dfd8",
   "metadata": {},
   "source": [
    "Import"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "fe29d543",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "from PIL import Image\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torchvision import transforms\n",
    "import matplotlib.pyplot as plt\n",
    "from torchvision.transforms import functional as TF\n",
    "import torch.nn.functional as F\n",
    "from torchmetrics.image.psnr import PeakSignalNoiseRatio\n",
    "from torchmetrics.image.ssim import StructuralSimilarityIndexMeasure"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "82217412",
   "metadata": {},
   "source": [
    "Resource check"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "121f95f1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Device available: cuda\n"
     ]
    }
   ],
   "source": [
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print('Device available:', device)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0b86f9ac",
   "metadata": {},
   "source": [
    "Dataload"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "db09e5d9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of images in the dataset: 480\n"
     ]
    }
   ],
   "source": [
    "# Loading TIFF images for Super-resolution\n",
    "class TIFFDataset(Dataset):\n",
    "    def __init__(self, high_res_dir, low_res_dir, transform=None):\n",
    "        self.high_res_dir = high_res_dir\n",
    "        self.low_res_dir = low_res_dir\n",
    "        self.transform = transform\n",
    "        self.filenames = sorted([f for f in os.listdir(high_res_dir) if f.endswith('.tif')])\n",
    "    \n",
    "    # Get the number of samples in the dataset\n",
    "    def __len__(self):\n",
    "        return len(self.filenames)\n",
    "\n",
    "    # Get the sample at the given index\n",
    "    def __getitem__(self, idx):\n",
    "        high_res_path = os.path.join(self.high_res_dir, self.filenames[idx])\n",
    "        low_res_path = os.path.join(self.low_res_dir, self.filenames[idx])\n",
    "\n",
    "        # Load images\n",
    "        high_res = Image.open(high_res_path)\n",
    "        low_res = Image.open(low_res_path)\n",
    "\n",
    "        # Resize low-res to 128x128 (ensuring correct input size)\n",
    "        low_res = low_res.resize((128, 128), Image.BICUBIC)\n",
    "\n",
    "        # Apply transformations\n",
    "        if self.transform:\n",
    "            high_res = self.transform(high_res)\n",
    "            low_res = self.transform(low_res)\n",
    "\n",
    "        return low_res, high_res  # Returning input-output pairs\n",
    "\n",
    "# Define a transform to convert images to PyTorch tensors\n",
    "transform = transforms.Compose([\n",
    "    transforms.ToTensor()\n",
    "])\n",
    "\n",
    "flip = transforms.Compose([\n",
    "    transforms.RandomHorizontalFlip(p=1.0),\n",
    "    transforms.ToTensor()\n",
    "])\n",
    "\n",
    "# Define a transform to convert images to PyTorch tensors with normalization\n",
    "normalize = transforms.Compose([\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=[0.5], std=[0.5])\n",
    "])\n",
    "\n",
    "normalize_flip = transforms.Compose([\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=[0.5], std=[0.5]),\n",
    "    transforms.RandomHorizontalFlip(p=1.0)\n",
    "])\n",
    "\n",
    "# Create the dataset for training images\n",
    "train_dataset = TIFFDataset('data/train/high-res', 'data/train/low-res', transform=transform)\n",
    "# Create the dataset for training images with flipping\n",
    "train_dataset_flip = TIFFDataset('data/train/high-res', 'data/train/low-res', transform=flip)\n",
    "\n",
    "# Combine the datasets\n",
    "train_dataset = torch.utils.data.ConcatDataset([train_dataset, train_dataset_flip])\n",
    "\n",
    "# Function to create data loader\n",
    "def create_loader(dataset, batch_size):\n",
    "    torch.manual_seed(0)  # For reproducibility\n",
    "    return DataLoader(dataset, batch_size=batch_size, shuffle=True)\n",
    "\n",
    "# Print dataset size\n",
    "dataset_size = len(train_dataset)\n",
    "print('Number of images in the dataset:', dataset_size)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c251e0ce",
   "metadata": {},
   "source": [
    "Define Network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "92ac0c76",
   "metadata": {},
   "outputs": [],
   "source": [
    "class testNet(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(testNet, self).__init__()\n",
    "        self.gelu = nn.GELU()\n",
    "\n",
    "        # 128 x 128 input\n",
    "        self.conv1 = nn.Conv2d(1, 64, kernel_size=3, stride=1, padding=1)\n",
    "        self.conv2 = nn.Conv2d(64, 64, kernel_size=3, stride=1, padding=1)\n",
    "\n",
    "        # upscale to 256 x 256\n",
    "        self.upconv1 = nn.ConvTranspose2d(64, 64, kernel_size=4, stride=2, padding=1)\n",
    "        self.conv3 = nn.Conv2d(64, 64, kernel_size=3, stride=1, padding=1)\n",
    "\n",
    "        # bicubic interpolation to 256 x 256 skip connection\n",
    "        self.bskip1 = nn.Upsample(scale_factor=2, mode='bicubic', align_corners=False)\n",
    "\n",
    "        # convolution to 256 x 256 combine with skip connection\n",
    "        self.conv4 = nn.Conv2d(128, 128, kernel_size=3, stride=1, padding=1)\n",
    "\n",
    "        # upscale to 512 x 512\n",
    "        self.upconv2 = nn.ConvTranspose2d(128, 128, kernel_size=4, stride=2, padding=1)\n",
    "        self.conv5 = nn.Conv2d(128, 128, kernel_size=3, stride=1, padding=1)\n",
    "\n",
    "        # bicubic interpolation to 512 x 512 skip connection\n",
    "        self.bskip2 = nn.Upsample(scale_factor=2, mode='bicubic', align_corners=False)\n",
    "\n",
    "        # convolution to 512 x 512 combine with skip connection\n",
    "        self.conv6 = nn.Conv2d(256, 256, kernel_size=3, stride=1, padding=1)\n",
    "\n",
    "        # downscale to 256 x 256\n",
    "        self.downconv1 = nn.Conv2d(256, 256, kernel_size=3, stride=2, padding=1)\n",
    "        self.conv7 = nn.Conv2d(256, 128, kernel_size=3, stride=1, padding=1)\n",
    "        self.conv8 = nn.Conv2d(128, 64, kernel_size=3, stride=1, padding=1)\n",
    "        self.conv9 = nn.Conv2d(64, 1, kernel_size=3, stride=1, padding=1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.conv1(x)\n",
    "        x = self.conv2(x)\n",
    "        x = self.gelu(x)\n",
    "\n",
    "        x_prev = self.bskip1(x)\n",
    "        x = self.upconv1(x)\n",
    "        x = self.conv3(x)\n",
    "        x = torch.cat((x, x_prev), dim=1)\n",
    "\n",
    "        x = self.conv4(x)\n",
    "        x_prev = self.bskip2(x)\n",
    "        x = self.upconv2(x)\n",
    "        x = self.conv5(x)\n",
    "        x = torch.cat((x, x_prev), dim=1)\n",
    "\n",
    "        x = self.conv6(x)\n",
    "        x = self.downconv1(x)\n",
    "        x = self.conv7(x)\n",
    "        x = self.gelu(self.conv8(x))\n",
    "        x = self.conv9(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c6168f76",
   "metadata": {},
   "source": [
    "Training Fn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "acba960a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model(model, opt, criterion, train_loader, num_epoch):\n",
    "    avg_train_losses = []\n",
    "\n",
    "    for epoch in range(num_epoch):  # Loop over the dataset multiple times\n",
    "        model.train()\n",
    "        total_train_loss = 0\n",
    "\n",
    "        for i, (x_tr_batch, y_tr_batch) in enumerate(train_loader):  # Loop over mini-batches\n",
    "            x_tr_batch, y_tr_batch = x_tr_batch.to(device), y_tr_batch.to(device)\n",
    "\n",
    "            # Upsample low-resolution input to 256x256\n",
    "            # x_tr_batch = torch.nn.functional.interpolate(x_tr_batch, scale_factor=2, mode='bicubic', align_corners=False)\n",
    "\n",
    "            opt.zero_grad()  # Delete previous gradients\n",
    "            y_hat_tr_batch = model(x_tr_batch)  # Forward pass\n",
    "            loss = criterion(y_hat_tr_batch, y_tr_batch)  # Compute loss\n",
    "            loss.backward()  # Backward pass\n",
    "            opt.step()  # Update weights\n",
    "            total_train_loss += loss.item()  # Accumulate loss\n",
    "\n",
    "            if (i + 1) % 10 == 0:\n",
    "                print(f'Epoch {epoch+1}, Iteration {i+1}, Loss: {loss.item():.6f}')\n",
    "\n",
    "        avg_train_loss = total_train_loss / len(train_loader)  # Compute average loss\n",
    "        avg_train_losses.append(avg_train_loss)  # Store average loss\n",
    "\n",
    "    # Plot training loss\n",
    "    fig, ax = plt.subplots(figsize=(8, 5))\n",
    "    ax.plot(range(1, num_epoch + 1), avg_train_losses, label='training loss')\n",
    "    ax.set_xlabel('epochs')\n",
    "    ax.set_ylabel('Loss')\n",
    "    ax.set_yscale('log')  # Log scale for better visualization\n",
    "    ax.set_title('training loss')\n",
    "    ax.grid(True)\n",
    "    ax.legend()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f4019070",
   "metadata": {},
   "source": [
    "Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fda96137",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1, Iteration 10, Loss: 0.036182\n",
      "Epoch 1, Iteration 20, Loss: 0.049470\n",
      "Epoch 1, Iteration 30, Loss: 0.010362\n",
      "Epoch 1, Iteration 40, Loss: 0.005561\n",
      "Epoch 2, Iteration 10, Loss: 0.002669\n",
      "Epoch 2, Iteration 20, Loss: 0.002304\n",
      "Epoch 2, Iteration 30, Loss: 0.001793\n",
      "Epoch 2, Iteration 40, Loss: 0.001701\n",
      "Epoch 3, Iteration 10, Loss: 0.001344\n",
      "Epoch 3, Iteration 20, Loss: 0.001206\n",
      "Epoch 3, Iteration 30, Loss: 0.001226\n",
      "Epoch 3, Iteration 40, Loss: 0.001971\n",
      "Epoch 4, Iteration 10, Loss: 0.001026\n",
      "Epoch 4, Iteration 20, Loss: 0.001011\n",
      "Epoch 4, Iteration 30, Loss: 0.000943\n",
      "Epoch 4, Iteration 40, Loss: 0.000959\n",
      "Epoch 5, Iteration 10, Loss: 0.000860\n",
      "Epoch 5, Iteration 20, Loss: 0.000863\n",
      "Epoch 5, Iteration 30, Loss: 0.000801\n",
      "Epoch 5, Iteration 40, Loss: 0.000805\n",
      "Epoch 6, Iteration 10, Loss: 0.000981\n",
      "Epoch 6, Iteration 20, Loss: 0.001459\n",
      "Epoch 6, Iteration 30, Loss: 0.000974\n",
      "Epoch 6, Iteration 40, Loss: 0.000772\n",
      "Epoch 7, Iteration 10, Loss: 0.000757\n",
      "Epoch 7, Iteration 20, Loss: 0.000711\n",
      "Epoch 7, Iteration 30, Loss: 0.000785\n",
      "Epoch 7, Iteration 40, Loss: 0.000647\n",
      "Epoch 8, Iteration 10, Loss: 0.000712\n",
      "Epoch 8, Iteration 20, Loss: 0.000660\n",
      "Epoch 8, Iteration 30, Loss: 0.000673\n",
      "Epoch 8, Iteration 40, Loss: 0.000632\n",
      "Epoch 9, Iteration 10, Loss: 0.000622\n",
      "Epoch 9, Iteration 20, Loss: 0.000608\n",
      "Epoch 9, Iteration 30, Loss: 0.000625\n",
      "Epoch 9, Iteration 40, Loss: 0.000654\n",
      "Epoch 10, Iteration 10, Loss: 0.000561\n",
      "Epoch 10, Iteration 20, Loss: 0.000643\n",
      "Epoch 10, Iteration 30, Loss: 0.000729\n",
      "Epoch 10, Iteration 40, Loss: 0.000689\n",
      "Epoch 11, Iteration 10, Loss: 0.000547\n",
      "Epoch 11, Iteration 20, Loss: 0.000568\n",
      "Epoch 11, Iteration 30, Loss: 0.000551\n",
      "Epoch 11, Iteration 40, Loss: 0.000546\n",
      "Epoch 12, Iteration 10, Loss: 0.000573\n",
      "Epoch 12, Iteration 20, Loss: 0.000560\n",
      "Epoch 12, Iteration 30, Loss: 0.000562\n",
      "Epoch 12, Iteration 40, Loss: 0.000516\n",
      "Epoch 13, Iteration 10, Loss: 0.000533\n",
      "Epoch 13, Iteration 20, Loss: 0.000471\n",
      "Epoch 13, Iteration 30, Loss: 0.000560\n",
      "Epoch 13, Iteration 40, Loss: 0.000498\n",
      "Epoch 14, Iteration 10, Loss: 0.000551\n",
      "Epoch 14, Iteration 20, Loss: 0.000596\n",
      "Epoch 14, Iteration 30, Loss: 0.000637\n",
      "Epoch 14, Iteration 40, Loss: 0.000454\n",
      "Epoch 15, Iteration 10, Loss: 0.000483\n",
      "Epoch 15, Iteration 20, Loss: 0.000548\n",
      "Epoch 15, Iteration 30, Loss: 0.000512\n",
      "Epoch 15, Iteration 40, Loss: 0.000527\n",
      "Epoch 16, Iteration 10, Loss: 31.395880\n",
      "Epoch 16, Iteration 20, Loss: 1620825.875000\n",
      "Epoch 16, Iteration 30, Loss: 122280689664.000000\n",
      "Epoch 16, Iteration 40, Loss: 49413550080.000000\n",
      "Epoch 17, Iteration 10, Loss: 6706052096.000000\n",
      "Epoch 17, Iteration 20, Loss: 2035640320.000000\n",
      "Epoch 17, Iteration 30, Loss: 879867840.000000\n",
      "Epoch 17, Iteration 40, Loss: 182299984.000000\n",
      "Epoch 18, Iteration 10, Loss: 72646848.000000\n",
      "Epoch 18, Iteration 20, Loss: 29256938.000000\n",
      "Epoch 18, Iteration 30, Loss: 19413824.000000\n",
      "Epoch 18, Iteration 40, Loss: 14977261.000000\n",
      "Epoch 19, Iteration 10, Loss: 10121659.000000\n",
      "Epoch 19, Iteration 20, Loss: 10978743.000000\n",
      "Epoch 19, Iteration 30, Loss: 9522578.000000\n",
      "Epoch 19, Iteration 40, Loss: 7821736.000000\n",
      "Epoch 20, Iteration 10, Loss: 7867568.000000\n",
      "Epoch 20, Iteration 20, Loss: 8059696.000000\n",
      "Epoch 20, Iteration 30, Loss: 7858671.500000\n",
      "Epoch 20, Iteration 40, Loss: 5758291.500000\n",
      "Epoch 21, Iteration 10, Loss: 7510109.000000\n",
      "Epoch 21, Iteration 20, Loss: 6112278.000000\n",
      "Epoch 21, Iteration 30, Loss: 5637135.500000\n",
      "Epoch 21, Iteration 40, Loss: 5141410.500000\n",
      "Epoch 22, Iteration 10, Loss: 5374188.000000\n",
      "Epoch 22, Iteration 20, Loss: 5053020.500000\n",
      "Epoch 22, Iteration 30, Loss: 4641047.000000\n",
      "Epoch 22, Iteration 40, Loss: 5062229.000000\n",
      "Epoch 23, Iteration 10, Loss: 5214506.500000\n",
      "Epoch 23, Iteration 20, Loss: 4098188.750000\n",
      "Epoch 23, Iteration 30, Loss: 4366360.500000\n",
      "Epoch 23, Iteration 40, Loss: 3967396.000000\n",
      "Epoch 24, Iteration 10, Loss: 3522645.750000\n",
      "Epoch 24, Iteration 20, Loss: 3904941.750000\n",
      "Epoch 24, Iteration 30, Loss: 4215909.500000\n",
      "Epoch 24, Iteration 40, Loss: 3986832.500000\n",
      "Epoch 25, Iteration 10, Loss: 3578164.000000\n",
      "Epoch 25, Iteration 20, Loss: 2985616.750000\n",
      "Epoch 25, Iteration 30, Loss: 2943713.750000\n",
      "Epoch 25, Iteration 40, Loss: 3523483.250000\n",
      "Epoch 26, Iteration 10, Loss: 2945137.750000\n",
      "Epoch 26, Iteration 20, Loss: 2876896.250000\n",
      "Epoch 26, Iteration 30, Loss: 4355276.000000\n",
      "Epoch 26, Iteration 40, Loss: 2820320.750000\n",
      "Epoch 27, Iteration 10, Loss: 3399596.000000\n",
      "Epoch 27, Iteration 20, Loss: 2636339.750000\n",
      "Epoch 27, Iteration 30, Loss: 2916667.250000\n",
      "Epoch 27, Iteration 40, Loss: 3238301.000000\n",
      "Epoch 28, Iteration 10, Loss: 2716082.500000\n",
      "Epoch 28, Iteration 20, Loss: 2367456.000000\n",
      "Epoch 28, Iteration 30, Loss: 2836250.500000\n",
      "Epoch 28, Iteration 40, Loss: 2801287.750000\n",
      "Epoch 29, Iteration 10, Loss: 2200765.750000\n",
      "Epoch 29, Iteration 20, Loss: 2594015.500000\n",
      "Epoch 29, Iteration 30, Loss: 2347656.750000\n",
      "Epoch 29, Iteration 40, Loss: 2295875.750000\n",
      "Epoch 30, Iteration 10, Loss: 2003604.875000\n",
      "Epoch 30, Iteration 20, Loss: 1967904.000000\n",
      "Epoch 30, Iteration 30, Loss: 2051421.625000\n",
      "Epoch 30, Iteration 40, Loss: 2516162.000000\n",
      "Epoch 31, Iteration 10, Loss: 2023750.000000\n",
      "Epoch 31, Iteration 20, Loss: 1812125.375000\n",
      "Epoch 31, Iteration 30, Loss: 1735899.875000\n",
      "Epoch 31, Iteration 40, Loss: 1829741.625000\n",
      "Epoch 32, Iteration 10, Loss: 1967120.625000\n",
      "Epoch 32, Iteration 20, Loss: 2279367.000000\n",
      "Epoch 32, Iteration 30, Loss: 2180706.750000\n",
      "Epoch 32, Iteration 40, Loss: 1824653.000000\n",
      "Epoch 33, Iteration 10, Loss: 1984547.875000\n",
      "Epoch 33, Iteration 20, Loss: 1786663.000000\n",
      "Epoch 33, Iteration 30, Loss: 1831376.000000\n",
      "Epoch 33, Iteration 40, Loss: 1775524.375000\n",
      "Epoch 34, Iteration 10, Loss: 2106715.000000\n",
      "Epoch 34, Iteration 20, Loss: 1529327.750000\n",
      "Epoch 34, Iteration 30, Loss: 1908133.000000\n",
      "Epoch 34, Iteration 40, Loss: 1601936.125000\n",
      "Epoch 35, Iteration 10, Loss: 1553762.375000\n",
      "Epoch 35, Iteration 20, Loss: 1562095.250000\n",
      "Epoch 35, Iteration 30, Loss: 1482102.625000\n",
      "Epoch 35, Iteration 40, Loss: 1563556.375000\n",
      "Epoch 36, Iteration 10, Loss: 1415889.250000\n",
      "Epoch 36, Iteration 20, Loss: 1712521.875000\n",
      "Epoch 36, Iteration 30, Loss: 1408818.250000\n",
      "Epoch 36, Iteration 40, Loss: 1289242.875000\n",
      "Epoch 37, Iteration 10, Loss: 1349437.875000\n",
      "Epoch 37, Iteration 20, Loss: 1337487.000000\n",
      "Epoch 37, Iteration 30, Loss: 1545762.875000\n",
      "Epoch 37, Iteration 40, Loss: 1501937.000000\n",
      "Epoch 38, Iteration 10, Loss: 1347928.000000\n",
      "Epoch 38, Iteration 20, Loss: 1414330.625000\n",
      "Epoch 38, Iteration 30, Loss: 1456636.375000\n",
      "Epoch 38, Iteration 40, Loss: 1405190.625000\n",
      "Epoch 39, Iteration 10, Loss: 1425648.750000\n",
      "Epoch 39, Iteration 20, Loss: 1304508.000000\n",
      "Epoch 39, Iteration 30, Loss: 1268274.875000\n",
      "Epoch 39, Iteration 40, Loss: 1287031.500000\n",
      "Epoch 40, Iteration 10, Loss: 1185101.625000\n",
      "Epoch 40, Iteration 20, Loss: 1082215.125000\n",
      "Epoch 40, Iteration 30, Loss: 1346483.875000\n",
      "Epoch 40, Iteration 40, Loss: 1177406.625000\n",
      "Epoch 41, Iteration 10, Loss: 1088504.250000\n",
      "Epoch 41, Iteration 20, Loss: 1267278.250000\n",
      "Epoch 41, Iteration 30, Loss: 1164799.125000\n",
      "Epoch 41, Iteration 40, Loss: 1125164.750000\n",
      "Epoch 42, Iteration 10, Loss: 1234177.375000\n",
      "Epoch 42, Iteration 20, Loss: 1128715.000000\n",
      "Epoch 42, Iteration 30, Loss: 964448.187500\n",
      "Epoch 42, Iteration 40, Loss: 958972.437500\n",
      "Epoch 43, Iteration 10, Loss: 1223349.375000\n",
      "Epoch 43, Iteration 20, Loss: 820496.500000\n",
      "Epoch 43, Iteration 30, Loss: 967134.437500\n",
      "Epoch 43, Iteration 40, Loss: 1131275.875000\n",
      "Epoch 44, Iteration 10, Loss: 1004470.437500\n",
      "Epoch 44, Iteration 20, Loss: 1088969.625000\n",
      "Epoch 44, Iteration 30, Loss: 925089.187500\n",
      "Epoch 44, Iteration 40, Loss: 882437.937500\n",
      "Epoch 45, Iteration 10, Loss: 928084.500000\n",
      "Epoch 45, Iteration 20, Loss: 884248.500000\n",
      "Epoch 45, Iteration 30, Loss: 915550.812500\n",
      "Epoch 45, Iteration 40, Loss: 1084997.625000\n",
      "Epoch 46, Iteration 10, Loss: 896035.187500\n",
      "Epoch 46, Iteration 20, Loss: 877067.812500\n",
      "Epoch 46, Iteration 30, Loss: 935886.125000\n",
      "Epoch 46, Iteration 40, Loss: 992439.625000\n",
      "Epoch 47, Iteration 10, Loss: 781901.937500\n",
      "Epoch 47, Iteration 20, Loss: 803628.125000\n",
      "Epoch 47, Iteration 30, Loss: 784146.750000\n",
      "Epoch 47, Iteration 40, Loss: 878963.312500\n",
      "Epoch 48, Iteration 10, Loss: 858512.812500\n",
      "Epoch 48, Iteration 20, Loss: 790664.187500\n",
      "Epoch 48, Iteration 30, Loss: 874964.187500\n",
      "Epoch 48, Iteration 40, Loss: 877414.937500\n",
      "Epoch 49, Iteration 10, Loss: 741631.187500\n",
      "Epoch 49, Iteration 20, Loss: 862455.187500\n",
      "Epoch 49, Iteration 30, Loss: 925148.437500\n",
      "Epoch 49, Iteration 40, Loss: 762755.562500\n",
      "Epoch 50, Iteration 10, Loss: 814957.187500\n",
      "Epoch 50, Iteration 20, Loss: 822913.000000\n",
      "Epoch 50, Iteration 30, Loss: 688072.187500\n",
      "Epoch 50, Iteration 40, Loss: 689734.312500\n",
      "Epoch 51, Iteration 10, Loss: 693556.687500\n",
      "Epoch 51, Iteration 20, Loss: 796264.062500\n",
      "Epoch 51, Iteration 30, Loss: 847696.187500\n",
      "Epoch 51, Iteration 40, Loss: 690557.125000\n",
      "Epoch 52, Iteration 10, Loss: 666158.875000\n",
      "Epoch 52, Iteration 20, Loss: 610227.437500\n",
      "Epoch 52, Iteration 30, Loss: 697908.125000\n",
      "Epoch 52, Iteration 40, Loss: 631651.000000\n",
      "Epoch 53, Iteration 10, Loss: 679489.687500\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mKeyboardInterrupt\u001b[39m                         Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[6]\u001b[39m\u001b[32m, line 12\u001b[39m\n\u001b[32m      9\u001b[39m criterion = nn.MSELoss()  \u001b[38;5;66;03m# Pick a loss function\u001b[39;00m\n\u001b[32m     11\u001b[39m \u001b[38;5;66;03m# Train the model\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m12\u001b[39m \u001b[43mtrain_model\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mopt\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcriterion\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrain_loader\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnum_epoch\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[5]\u001b[39m\u001b[32m, line 19\u001b[39m, in \u001b[36mtrain_model\u001b[39m\u001b[34m(model, opt, criterion, train_loader, num_epoch)\u001b[39m\n\u001b[32m     17\u001b[39m loss.backward()  \u001b[38;5;66;03m# Backward pass\u001b[39;00m\n\u001b[32m     18\u001b[39m opt.step()  \u001b[38;5;66;03m# Update weights\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m19\u001b[39m total_train_loss += \u001b[43mloss\u001b[49m\u001b[43m.\u001b[49m\u001b[43mitem\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# Accumulate loss\u001b[39;00m\n\u001b[32m     21\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m (i + \u001b[32m1\u001b[39m) % \u001b[32m10\u001b[39m == \u001b[32m0\u001b[39m:\n\u001b[32m     22\u001b[39m     \u001b[38;5;28mprint\u001b[39m(\u001b[33mf\u001b[39m\u001b[33m'\u001b[39m\u001b[33mEpoch \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mepoch+\u001b[32m1\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m, Iteration \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mi+\u001b[32m1\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m, Loss: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mloss.item()\u001b[38;5;132;01m:\u001b[39;00m\u001b[33m.6f\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m'\u001b[39m)\n",
      "\u001b[31mKeyboardInterrupt\u001b[39m: "
     ]
    }
   ],
   "source": [
    "batch_size = 10  # Number of complete images in each batch\n",
    "lr = 1e-3  # Learning rate\n",
    "num_epoch = 10  # Epochs\n",
    "\n",
    "# Model, criterion, and optimizer\n",
    "train_loader = create_loader(train_dataset, batch_size)\n",
    "model = testNet().to(device)  # Pick a model and move to GPU/CPU\n",
    "opt = optim.Adam(model.parameters(), lr=lr)  # Pick an optimizer\n",
    "criterion = nn.MSELoss()  # Pick a loss function\n",
    "\n",
    "# Train the model\n",
    "train_model(model, opt, criterion, train_loader, num_epoch)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a6ed0851",
   "metadata": {},
   "source": [
    "Metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "43ddf324",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'TIFFDataset' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mNameError\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[1]\u001b[39m\u001b[32m, line 2\u001b[39m\n\u001b[32m      1\u001b[39m \u001b[38;5;66;03m# Dataset for validation\u001b[39;00m\n\u001b[32m----> \u001b[39m\u001b[32m2\u001b[39m val_dataset = \u001b[43mTIFFDataset\u001b[49m(\u001b[33m'\u001b[39m\u001b[33mdata/val/high-res\u001b[39m\u001b[33m'\u001b[39m, \u001b[33m'\u001b[39m\u001b[33mdata/val/low-res\u001b[39m\u001b[33m'\u001b[39m, transform=transform)\n\u001b[32m      4\u001b[39m \u001b[38;5;66;03m# Initialize metrics\u001b[39;00m\n\u001b[32m      5\u001b[39m psnr_metric = PeakSignalNoiseRatio().to(device)\n",
      "\u001b[31mNameError\u001b[39m: name 'TIFFDataset' is not defined"
     ]
    }
   ],
   "source": [
    "# Dataset for validation\n",
    "val_dataset = TIFFDataset('data/val/high-res', 'data/val/low-res', transform=transform)\n",
    "\n",
    "# Initialize metrics\n",
    "psnr_metric = PeakSignalNoiseRatio().to(device)\n",
    "ssim_metric = StructuralSimilarityIndexMeasure(data_range=1.0).to(device)\n",
    "\n",
    "# Accumulators\n",
    "total_psnr_interpolated = 0\n",
    "total_psnr_super_resolved = 0\n",
    "total_ssim_interpolated = 0\n",
    "total_ssim_super_resolved = 0\n",
    "num_samples = len(val_dataset)\n",
    "\n",
    "# Loop over validation set\n",
    "for i in range(num_samples):\n",
    "    val_low_res, val_high_res = val_dataset[i]\n",
    "    val_low_res, val_high_res = val_low_res.to(device), val_high_res.to(device)\n",
    "\n",
    "    # Compute data range dynamically\n",
    "    data_range = val_high_res.max() - val_high_res.min()\n",
    "    ssim_metric.data_range = data_range\n",
    "\n",
    "    # Upsample low-res image\n",
    "\n",
    "    val_low_res_up = torch.nn.functional.interpolate(val_low_res.unsqueeze(0), scale_factor=2, mode='bicubic', align_corners=False)\n",
    "\n",
    "    val_high_res = val_high_res.unsqueeze(0)  # Add batch dim\n",
    "\n",
    "    # val_super_res = model(val_low_res_up).detach()\n",
    "\n",
    "    val_super_res = model(val_low_res.unsqueeze(1)).detach()\n",
    "\n",
    "    # PSNR\n",
    "    # psnr_interp = psnr_metric(val_low_res_up, val_high_res).item()\n",
    "    psnr_interp = psnr_metric(val_low_res_up, val_high_res).item()\n",
    "    psnr_sr = psnr_metric(val_super_res, val_high_res).item()\n",
    "\n",
    "    # SSIM (data_range is now set in metric initialization)\n",
    "    # ssim_interp = ssim_metric(val_low_res_up, val_high_res).item()\n",
    "    ssim_interp = ssim_metric(val_low_res_up, val_high_res).item()\n",
    "    ssim_sr = ssim_metric(val_super_res, val_high_res).item()\n",
    "\n",
    "    # Accumulate\n",
    "    total_psnr_interpolated += psnr_interp\n",
    "    total_psnr_super_resolved += psnr_sr\n",
    "    total_ssim_interpolated += ssim_interp\n",
    "    total_ssim_super_resolved += ssim_sr\n",
    "\n",
    "# Averages\n",
    "avg_psnr_interp = total_psnr_interpolated / num_samples\n",
    "avg_psnr_sr = total_psnr_super_resolved / num_samples\n",
    "avg_ssim_interp = total_ssim_interpolated / num_samples\n",
    "avg_ssim_sr = total_ssim_super_resolved / num_samples\n",
    "\n",
    "# Print results\n",
    "print(f'Average PSNR (interpolated): {avg_psnr_interp:.2f} dB')\n",
    "print(f'Average PSNR (super-resolved): {avg_psnr_sr:.2f} dB')\n",
    "print(f'Average SSIM (interpolated): {avg_ssim_interp:.4f}')\n",
    "print(f'Average SSIM (super-resolved): {avg_ssim_sr:.4f}')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
