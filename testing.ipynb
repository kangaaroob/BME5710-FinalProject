{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "8d03dfd8",
   "metadata": {},
   "source": [
    "Import"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "fe29d543",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "from PIL import Image\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torchvision import transforms\n",
    "import matplotlib.pyplot as plt\n",
    "from torchvision.transforms import functional as TF\n",
    "import torch.nn.functional as F\n",
    "from torchmetrics.image.psnr import PeakSignalNoiseRatio\n",
    "from torchmetrics.image.ssim import StructuralSimilarityIndexMeasure"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "82217412",
   "metadata": {},
   "source": [
    "Resource check"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "121f95f1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Device available: cpu\n"
     ]
    }
   ],
   "source": [
    "# ON MACOS\n",
    "# device = torch.device('mps' if torch.backends.mps.is_available() else 'cpu')\n",
    "# print('Device available:', device)\n",
    "\n",
    "# ON WINDOWS\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print('Device available:', device)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0b86f9ac",
   "metadata": {},
   "source": [
    "Dataload"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "db09e5d9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of images in the dataset: 240\n"
     ]
    }
   ],
   "source": [
    "# Loading TIFF images for Super-resolution\n",
    "class TIFFDataset(Dataset):\n",
    "    def __init__(self, high_res_dir, low_res_dir, transform=None):\n",
    "        self.high_res_dir = high_res_dir\n",
    "        self.low_res_dir = low_res_dir\n",
    "        self.transform = transform\n",
    "        self.filenames = sorted([f for f in os.listdir(high_res_dir) if f.endswith('.tif')])\n",
    "    \n",
    "    # Get the number of samples in the dataset\n",
    "    def __len__(self):\n",
    "        return len(self.filenames)\n",
    "\n",
    "    # Get the sample at the given index\n",
    "    def __getitem__(self, idx):\n",
    "        high_res_path = os.path.join(self.high_res_dir, self.filenames[idx])\n",
    "        low_res_path = os.path.join(self.low_res_dir, self.filenames[idx])\n",
    "\n",
    "        # Load images\n",
    "        high_res = Image.open(high_res_path)\n",
    "        low_res = Image.open(low_res_path)\n",
    "\n",
    "        # Resize low-res to 128x128 (ensuring correct input size)\n",
    "        low_res = low_res.resize((128, 128), Image.BICUBIC)\n",
    "\n",
    "        # Apply transformations\n",
    "        if self.transform:\n",
    "            high_res = self.transform(high_res)\n",
    "            low_res = self.transform(low_res)\n",
    "\n",
    "        return low_res, high_res  # Returning input-output pairs\n",
    "\n",
    "# Define a transform to convert images to PyTorch tensors\n",
    "transform = transforms.Compose([\n",
    "    transforms.RandomHorizontalFlip(p=0.5),\n",
    "    transforms.ToTensor()\n",
    "])\n",
    "\n",
    "flip = transforms.Compose([\n",
    "    transforms.RandomHorizontalFlip(p=1.0),\n",
    "    transforms.ToTensor()\n",
    "])\n",
    "\n",
    "# Define a transform to convert images to PyTorch tensors with normalization\n",
    "normalize = transforms.Compose([\n",
    "    transforms.RandomHorizontalFlip(p=0.5),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=[0.5], std=[0.5])\n",
    "])\n",
    "\n",
    "# normalize_flip = transforms.Compose([\n",
    "#     transforms.ToTensor(),\n",
    "#     transforms.Normalize(mean=[0.5], std=[0.5]),\n",
    "#     transforms.RandomHorizontalFlip(p=1.0)\n",
    "# ])\n",
    "\n",
    "# Create the dataset for training images\n",
    "train_dataset = TIFFDataset('data/train/high-res', 'data/train/low-res', transform=transform)\n",
    "# Create the dataset for training images with flipping\n",
    "# train_dataset_flip = TIFFDataset('data/train/high-res', 'data/train/low-res', transform=flip)\n",
    "\n",
    "# Combine the datasets\n",
    "# train_dataset = torch.utils.data.ConcatDataset([train_dataset, train_dataset_flip])\n",
    "\n",
    "# Function to create data loader\n",
    "def create_loader(dataset, batch_size):\n",
    "    torch.manual_seed(0)  # For reproducibility\n",
    "    return DataLoader(dataset, batch_size=batch_size, shuffle=True)\n",
    "\n",
    "# Print dataset size\n",
    "dataset_size = len(train_dataset)\n",
    "print('Number of images in the dataset:', dataset_size)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c251e0ce",
   "metadata": {},
   "source": [
    "Define Network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "92ac0c76",
   "metadata": {},
   "outputs": [],
   "source": [
    "class testNet(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(testNet, self).__init__()\n",
    "        self.gelu = nn.GELU()\n",
    "\n",
    "        # long reaching bicubic interpolation skip connection\n",
    "        self.conv0 = nn.Conv2d(1, 64, kernel_size=3, stride=1, padding=1) # 128^2 -> 128^2\n",
    "        self.bskip1 = nn.Upsample(scale_factor=2, mode='bicubic', align_corners=False) # 128^2 -> 256^2\n",
    "\n",
    "        self.conv1 = nn.Conv2d(64, 64, kernel_size=5, stride=1, padding=1) # 128^2 -> 126^2\n",
    "        self.conv2 = nn.Conv2d(64, 64, kernel_size=5, stride=1, padding=1) # 126^2 -> 124^2\n",
    "        \n",
    "        # max pooling\n",
    "        self.pool1 = nn.MaxPool2d(kernel_size=2, stride=2) # 124^2 -> 62^2\n",
    "\n",
    "        self.conv3 = nn.Conv2d(64, 128, kernel_size=5, stride=1, padding=1) # 62^2 -> 60^2\n",
    "        self.conv4 = nn.Conv2d(128, 128, kernel_size=5, stride=1, padding=1) # 60^2 -> 58^2\n",
    "\n",
    "        # second bicubic interpolation skip connection\n",
    "        self.bskip2 = nn.Upsample(scale_factor=2, mode='bicubic', align_corners=False) # 58^2 -> 116^2\n",
    "\n",
    "        # upconvolution double size\n",
    "        self.upconv1 = nn.ConvTranspose2d(128, 64, kernel_size=4, stride=2, padding=1) # 58^2 -> 116^2\n",
    "\n",
    "        # post concat\n",
    "        self.conv5 = nn.Conv2d(192, 64, kernel_size=5, stride=1, padding=1) # 116^2 -> 114^2\n",
    "\n",
    "        # convolution to 114^2 to 64^2\n",
    "        self.conv6 = nn.Conv2d(64, 64, kernel_size=53, stride=1, padding=1) # 114^2 -> 64^2\n",
    "\n",
    "        # upconvolution double size twice\n",
    "        self.upconv2 = nn.ConvTranspose2d(64, 64, kernel_size=4, stride=2, padding=1) # 64^2 -> 128^2\n",
    "        self.upconv3 = nn.ConvTranspose2d(64, 64, kernel_size=4, stride=2, padding=1) # 128^2 -> 256^2\n",
    "        \n",
    "        # post concat\n",
    "        self.conv7 = nn.Conv2d(128, 64, kernel_size=3, stride=1, padding=1) # 256^2 -> 256^2\n",
    "        self.conv8 = nn.Conv2d(64, 1, kernel_size=3, stride=1, padding=1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.conv0(x)\n",
    "        x_prev1 = self.bskip1(x)\n",
    "        x = self.conv1(x)\n",
    "        x = self.conv2(x)\n",
    "        \n",
    "        x = self.pool1(x)\n",
    "        x = self.conv3(x)\n",
    "        x = self.conv4(x)\n",
    "\n",
    "        x_prev2 = self.bskip2(x)\n",
    "        x = self.upconv1(x)\n",
    "        x = torch.cat((x, x_prev2), 1)\n",
    "\n",
    "        x = self.conv5(x)\n",
    "        x = self.conv6(x)\n",
    "        x = self.upconv2(x)\n",
    "        x = self.upconv3(x)\n",
    "        x = torch.cat((x, x_prev1), 1)\n",
    "        x = self.conv7(x)\n",
    "        x = self.conv8(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c6168f76",
   "metadata": {},
   "source": [
    "Training Fn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "acba960a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model(model, opt, criterion, train_loader, num_epoch):\n",
    "    avg_train_losses = []\n",
    "\n",
    "    for epoch in range(num_epoch):  # Loop over the dataset multiple times\n",
    "        model.train()\n",
    "        total_train_loss = 0\n",
    "\n",
    "        for i, (x_tr_batch, y_tr_batch) in enumerate(train_loader):  # Loop over mini-batches\n",
    "            x_tr_batch, y_tr_batch = x_tr_batch.to(device), y_tr_batch.to(device)\n",
    "\n",
    "            # Upsample low-resolution input to 256x256\n",
    "            # x_tr_batch = torch.nn.functional.interpolate(x_tr_batch, scale_factor=2, mode='bicubic', align_corners=False)\n",
    "\n",
    "            opt.zero_grad()  # Delete previous gradients\n",
    "            y_hat_tr_batch = model(x_tr_batch)  # Forward pass\n",
    "            loss = criterion(y_hat_tr_batch, y_tr_batch)  # Compute loss\n",
    "            loss.backward()  # Backward pass\n",
    "            opt.step()  # Update weights\n",
    "            total_train_loss += loss.item()  # Accumulate loss\n",
    "\n",
    "            if (i + 1) % 10 == 0:\n",
    "                print(f'Epoch {epoch+1}, Iteration {i+1}, Loss: {loss.item():.6f}')\n",
    "\n",
    "        avg_train_loss = total_train_loss / len(train_loader)  # Compute average loss\n",
    "        avg_train_losses.append(avg_train_loss)  # Store average loss\n",
    "\n",
    "    # Plot training loss\n",
    "    fig, ax = plt.subplots(figsize=(8, 5))\n",
    "    ax.plot(range(1, num_epoch + 1), avg_train_losses, label='training loss')\n",
    "    ax.set_xlabel('epochs')\n",
    "    ax.set_ylabel('Loss')\n",
    "    ax.set_yscale('log')  # Log scale for better visualization\n",
    "    ax.set_title('training loss')\n",
    "    ax.grid(True)\n",
    "    ax.legend()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f4019070",
   "metadata": {},
   "source": [
    "Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "fda96137",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1, Iteration 10, Loss: 4505.092773\n",
      "Epoch 1, Iteration 20, Loss: 7922.063965\n",
      "Epoch 1, Iteration 30, Loss: 122528.039062\n",
      "Epoch 1, Iteration 40, Loss: 2425.820068\n",
      "Epoch 2, Iteration 10, Loss: 2776624.000000\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mKeyboardInterrupt\u001b[39m                         Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[62]\u001b[39m\u001b[32m, line 12\u001b[39m\n\u001b[32m      9\u001b[39m criterion = nn.MSELoss()  \u001b[38;5;66;03m# Pick a loss function\u001b[39;00m\n\u001b[32m     11\u001b[39m \u001b[38;5;66;03m# Train the model\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m12\u001b[39m \u001b[43mtrain_model\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mopt\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcriterion\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrain_loader\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnum_epoch\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[61]\u001b[39m\u001b[32m, line 17\u001b[39m, in \u001b[36mtrain_model\u001b[39m\u001b[34m(model, opt, criterion, train_loader, num_epoch)\u001b[39m\n\u001b[32m     15\u001b[39m y_hat_tr_batch = model(x_tr_batch)  \u001b[38;5;66;03m# Forward pass\u001b[39;00m\n\u001b[32m     16\u001b[39m loss = criterion(y_hat_tr_batch, y_tr_batch)  \u001b[38;5;66;03m# Compute loss\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m17\u001b[39m \u001b[43mloss\u001b[49m\u001b[43m.\u001b[49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# Backward pass\u001b[39;00m\n\u001b[32m     18\u001b[39m opt.step()  \u001b[38;5;66;03m# Update weights\u001b[39;00m\n\u001b[32m     19\u001b[39m total_train_loss += loss.item()  \u001b[38;5;66;03m# Accumulate loss\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/GitHub/BME5710-FinalProject/.venv/lib/python3.11/site-packages/torch/_tensor.py:522\u001b[39m, in \u001b[36mTensor.backward\u001b[39m\u001b[34m(self, gradient, retain_graph, create_graph, inputs)\u001b[39m\n\u001b[32m    512\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m has_torch_function_unary(\u001b[38;5;28mself\u001b[39m):\n\u001b[32m    513\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m handle_torch_function(\n\u001b[32m    514\u001b[39m         Tensor.backward,\n\u001b[32m    515\u001b[39m         (\u001b[38;5;28mself\u001b[39m,),\n\u001b[32m   (...)\u001b[39m\u001b[32m    520\u001b[39m         inputs=inputs,\n\u001b[32m    521\u001b[39m     )\n\u001b[32m--> \u001b[39m\u001b[32m522\u001b[39m \u001b[43mtorch\u001b[49m\u001b[43m.\u001b[49m\u001b[43mautograd\u001b[49m\u001b[43m.\u001b[49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    523\u001b[39m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgradient\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m=\u001b[49m\u001b[43minputs\u001b[49m\n\u001b[32m    524\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/GitHub/BME5710-FinalProject/.venv/lib/python3.11/site-packages/torch/autograd/__init__.py:266\u001b[39m, in \u001b[36mbackward\u001b[39m\u001b[34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[39m\n\u001b[32m    261\u001b[39m     retain_graph = create_graph\n\u001b[32m    263\u001b[39m \u001b[38;5;66;03m# The reason we repeat the same comment below is that\u001b[39;00m\n\u001b[32m    264\u001b[39m \u001b[38;5;66;03m# some Python versions print out the first line of a multi-line function\u001b[39;00m\n\u001b[32m    265\u001b[39m \u001b[38;5;66;03m# calls in the traceback and some print out the last line\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m266\u001b[39m \u001b[43mVariable\u001b[49m\u001b[43m.\u001b[49m\u001b[43m_execution_engine\u001b[49m\u001b[43m.\u001b[49m\u001b[43mrun_backward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# Calls into the C++ engine to run the backward pass\u001b[39;49;00m\n\u001b[32m    267\u001b[39m \u001b[43m    \u001b[49m\u001b[43mtensors\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    268\u001b[39m \u001b[43m    \u001b[49m\u001b[43mgrad_tensors_\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    269\u001b[39m \u001b[43m    \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    270\u001b[39m \u001b[43m    \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    271\u001b[39m \u001b[43m    \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    272\u001b[39m \u001b[43m    \u001b[49m\u001b[43mallow_unreachable\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[32m    273\u001b[39m \u001b[43m    \u001b[49m\u001b[43maccumulate_grad\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[32m    274\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[31mKeyboardInterrupt\u001b[39m: "
     ]
    }
   ],
   "source": [
    "batch_size = 5  # Number of complete images in each batch\n",
    "lr = 1e-3  # Learning rate\n",
    "num_epoch = 50  # Epochs\n",
    "\n",
    "# Model, criterion, and optimizer\n",
    "train_loader = create_loader(train_dataset, batch_size)\n",
    "model = testNet().to(device)  # Pick a model and move to GPU/CPU\n",
    "opt = optim.Adam(model.parameters(), lr=lr)  # Pick an optimizer\n",
    "criterion = nn.MSELoss()  # Pick a loss function\n",
    "\n",
    "# Train the model\n",
    "train_model(model, opt, criterion, train_loader, num_epoch)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a6ed0851",
   "metadata": {},
   "source": [
    "Metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "43ddf324",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average PSNR (interpolated): 30.81 dB\n",
      "Average PSNR (super-resolved): 32.37 dB\n",
      "Average SSIM (interpolated): 0.9056\n",
      "Average SSIM (super-resolved): 0.9177\n"
     ]
    }
   ],
   "source": [
    "# Dataset for validation\n",
    "val_dataset = TIFFDataset('data/val/high-res', 'data/val/low-res', transform=transform)\n",
    "\n",
    "# Initialize metrics\n",
    "psnr_metric = PeakSignalNoiseRatio().to(device)\n",
    "ssim_metric = StructuralSimilarityIndexMeasure(data_range=1.0).to(device)\n",
    "\n",
    "# Accumulators\n",
    "total_psnr_interpolated = 0\n",
    "total_psnr_super_resolved = 0\n",
    "total_ssim_interpolated = 0\n",
    "total_ssim_super_resolved = 0\n",
    "num_samples = len(val_dataset)\n",
    "\n",
    "# Loop over validation set\n",
    "for i in range(num_samples):\n",
    "    val_low_res, val_high_res = val_dataset[i]\n",
    "    val_low_res, val_high_res = val_low_res.to(device), val_high_res.to(device)\n",
    "\n",
    "    # Compute data range dynamically\n",
    "    data_range = val_high_res.max() - val_high_res.min()\n",
    "    ssim_metric.data_range = data_range\n",
    "\n",
    "    # Upsample low-res image\n",
    "\n",
    "    val_low_res_up = torch.nn.functional.interpolate(val_low_res.unsqueeze(0), scale_factor=2, mode='bicubic', align_corners=False)\n",
    "\n",
    "    val_high_res = val_high_res.unsqueeze(0)  # Add batch dim\n",
    "\n",
    "    # val_super_res = model(val_low_res_up).detach()\n",
    "\n",
    "    val_super_res = model(val_low_res.unsqueeze(1)).detach()\n",
    "\n",
    "    # PSNR\n",
    "    # psnr_interp = psnr_metric(val_low_res_up, val_high_res).item()\n",
    "    psnr_interp = psnr_metric(val_low_res_up, val_high_res).item()\n",
    "    psnr_sr = psnr_metric(val_super_res, val_high_res).item()\n",
    "\n",
    "    # SSIM (data_range is now set in metric initialization)\n",
    "    # ssim_interp = ssim_metric(val_low_res_up, val_high_res).item()\n",
    "    ssim_interp = ssim_metric(val_low_res_up, val_high_res).item()\n",
    "    ssim_sr = ssim_metric(val_super_res, val_high_res).item()\n",
    "\n",
    "    # Accumulate\n",
    "    total_psnr_interpolated += psnr_interp\n",
    "    total_psnr_super_resolved += psnr_sr\n",
    "    total_ssim_interpolated += ssim_interp\n",
    "    total_ssim_super_resolved += ssim_sr\n",
    "\n",
    "# Averages\n",
    "avg_psnr_interp = total_psnr_interpolated / num_samples\n",
    "avg_psnr_sr = total_psnr_super_resolved / num_samples\n",
    "avg_ssim_interp = total_ssim_interpolated / num_samples\n",
    "avg_ssim_sr = total_ssim_super_resolved / num_samples\n",
    "\n",
    "# Print results\n",
    "print(f'Average PSNR (interpolated): {avg_psnr_interp:.2f} dB')\n",
    "print(f'Average PSNR (super-resolved): {avg_psnr_sr:.2f} dB')\n",
    "print(f'Average SSIM (interpolated): {avg_ssim_interp:.4f}')\n",
    "print(f'Average SSIM (super-resolved): {avg_ssim_sr:.4f}')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
