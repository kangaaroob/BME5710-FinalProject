{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "___\n",
    "# **Starter code for BME 5710 project**\n",
    "## Instructor -- Rizwan Ahmad (ahmad.46@osu.edu)\n",
    "## BME5710 -- Spring 2025"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "___\n",
    "### Import libraries and sub-libraries."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "from PIL import Image\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torchvision import transforms\n",
    "import matplotlib.pyplot as plt\n",
    "from torchvision.transforms import functional as TF\n",
    "import torch.nn.functional as F\n",
    "from torchmetrics.image.psnr import PeakSignalNoiseRatio\n",
    "from torchmetrics.image.ssim import StructuralSimilarityIndexMeasure\n",
    "from tqdm.notebook import tqdm\n",
    "import datetime\n",
    "import copy\n",
    "from torch.optim.lr_scheduler import ReduceLROnPlateau"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "___\n",
    "### Calling a custom code to change the default font for figures to `Computer Modern`. (Optional)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from fontsetting import font_cmu\n",
    "# plt = font_cmu(plt)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "___\n",
    "### Check the hardware that is at your disposal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Device available: cuda\n"
     ]
    }
   ],
   "source": [
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print('Device available:', device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "___\n",
    "### Read training data from `data/train/hig-res` and `data/train/low-res`\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TIFFDataset(Dataset):\n",
    "    def __init__(self, high_res_dir, low_res_dir, transform=None, augment=False, dataset_name=\"Dataset\"):\n",
    "        self.high_res_dir = high_res_dir\n",
    "        self.low_res_dir = low_res_dir\n",
    "        self.transform = transform\n",
    "        self.augment = augment\n",
    "        self.dataset_name = dataset_name\n",
    "        self.original_filenames = sorted([f for f in os.listdir(high_res_dir) if f.endswith('.tif')])\n",
    "        self.num_original_images = len(self.original_filenames)\n",
    "\n",
    "        self.low_res_images = []\n",
    "        self.high_res_images = []\n",
    "\n",
    "        for filename in self.original_filenames:\n",
    "            high_res_path = os.path.join(self.high_res_dir, filename)\n",
    "            low_res_path = os.path.join(self.low_res_dir, filename)\n",
    "\n",
    "            # Load original images\n",
    "            try:\n",
    "                hr_img = Image.open(high_res_path)\n",
    "                lr_img = Image.open(low_res_path)\n",
    "            except Exception as e:\n",
    "                print(f\"Error loading image {filename}: {e}\")\n",
    "                continue\n",
    "\n",
    "            # Resize low-res to 128x128\n",
    "            lr_img = lr_img.resize((128, 128), Image.BICUBIC)\n",
    "\n",
    "            original_lr, original_hr = lr_img, hr_img\n",
    "            processed_lr = []\n",
    "            processed_hr = []\n",
    "\n",
    "            if self.augment:\n",
    "                # --- Apply transformations ---\n",
    "                # 1) Original\n",
    "                processed_lr.append(original_lr)\n",
    "                processed_hr.append(original_hr)\n",
    "                # 2) Rotated by 90 degrees\n",
    "                processed_lr.append(original_lr.rotate(90))\n",
    "                processed_hr.append(original_hr.rotate(90))\n",
    "                # 3) Rotated by 180 degrees\n",
    "                processed_lr.append(original_lr.rotate(180))\n",
    "                processed_hr.append(original_hr.rotate(180))\n",
    "                # 4) Rotated by 270 degrees\n",
    "                processed_lr.append(original_lr.rotate(270))\n",
    "                processed_hr.append(original_hr.rotate(270))\n",
    "\n",
    "                # Apply flips\n",
    "                lr_v_flip = original_lr.transpose(Image.FLIP_LEFT_RIGHT)\n",
    "                hr_v_flip = original_hr.transpose(Image.FLIP_LEFT_RIGHT)\n",
    "                lr_h_flip = original_lr.transpose(Image.FLIP_TOP_BOTTOM)\n",
    "                hr_h_flip = original_hr.transpose(Image.FLIP_TOP_BOTTOM)\n",
    "\n",
    "                # 5) Mirrored on the vertical axis (Flip Left/Right)\n",
    "                processed_lr.append(lr_v_flip)\n",
    "                processed_hr.append(hr_v_flip)\n",
    "                # 6) Mirrored on the horizontal axis (Flip Top/Bottom)\n",
    "                processed_lr.append(lr_h_flip)\n",
    "                processed_hr.append(hr_h_flip)\n",
    "\n",
    "                # Apply flips combined with rotation (unique combinations)\n",
    "                # 7) Rotated by 90 degrees and mirrored on the vertical axis\n",
    "                processed_lr.append(lr_v_flip.rotate(90))\n",
    "                processed_hr.append(hr_v_flip.rotate(90))\n",
    "                # 8) Rotated by 90 degrees and mirrored on the horizontal axis\n",
    "                processed_lr.append(lr_h_flip.rotate(90))\n",
    "                processed_hr.append(hr_h_flip.rotate(90))\n",
    "            else:\n",
    "                # If not augmenting, just use the original\n",
    "                processed_lr.append(original_lr)\n",
    "                processed_hr.append(original_hr)\n",
    "\n",
    "            # Apply the final transform (ToTensor) and store\n",
    "            if self.transform:\n",
    "                for lr, hr in zip(processed_lr, processed_hr):\n",
    "                    self.low_res_images.append(torch.clamp(self.transform(lr), min=0.0, max=1.0))\n",
    "                    self.high_res_images.append(torch.clamp(self.transform(hr), min=0.0, max=1.0))\n",
    "            else:\n",
    "                 # If no transform, store PIL images (not recommended for training/eval)\n",
    "                self.low_res_images.extend(processed_lr)\n",
    "                self.high_res_images.extend(processed_hr)\n",
    "\n",
    "    # Get the number of samples in the dataset (original or augmented)\n",
    "    def __len__(self):\n",
    "        return len(self.low_res_images)\n",
    "\n",
    "    # Get the sample at the given index\n",
    "    def __getitem__(self, idx):\n",
    "        # Return pre-processed tensors\n",
    "        return self.low_res_images[idx], self.high_res_images[idx]\n",
    "    \n",
    "    def verify_clamped_values(self):\n",
    "        \"\"\"\n",
    "        Verifies the min/max values of the loaded and clamped tensors.\n",
    "        \"\"\"\n",
    "        print(f\"\\n--- Verifying Clamped Values for {self.dataset_name} ---\")\n",
    "        if not self.low_res_images or not self.high_res_images:\n",
    "            print(\"No images loaded to verify.\")\n",
    "            return\n",
    "\n",
    "        min_lr, max_lr = float('inf'), float('-inf')\n",
    "        min_hr, max_hr = float('inf'), float('-inf')\n",
    "\n",
    "        # Check low-resolution images\n",
    "        for tensor in self.low_res_images:\n",
    "            current_min_lr = torch.min(tensor).item()\n",
    "            current_max_lr = torch.max(tensor).item()\n",
    "            if current_min_lr < min_lr: min_lr = current_min_lr\n",
    "            if current_max_lr > max_lr: max_lr = current_max_lr\n",
    "\n",
    "        # Check high-resolution images\n",
    "        for tensor in self.high_res_images:\n",
    "            current_min_hr = torch.min(tensor).item()\n",
    "            current_max_hr = torch.max(tensor).item()\n",
    "            if current_min_hr < min_hr: min_hr = current_min_hr\n",
    "            if current_max_hr > max_hr: max_hr = current_max_hr\n",
    "\n",
    "        print(f\"Low-Res Images (Clamped): Min={min_lr:.8f}, Max={max_lr:.8f}\")\n",
    "        print(f\"High-Res Images (Clamped): Min={min_hr:.8f}, Max={max_hr:.8f}\")\n",
    "        print(\"----------------------------------------------\")\n",
    "\n",
    "# Function to create data loader\n",
    "def create_loader(dataset, batch_size, shuffle_data=True): # Added shuffle parameter\n",
    "    torch.manual_seed(0)  # For reproducibility\n",
    "    return DataLoader(dataset, batch_size=batch_size, shuffle=shuffle_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create a custom loss class to easily implement custom loss functions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CombinedLoss(nn.Module):\n",
    "    def __init__(self, alpha=0.5, beta=0.5, device='cpu'):\n",
    "        super(CombinedLoss, self).__init__()\n",
    "        self.alpha = alpha # Weight for MSE (PSNR) loss\n",
    "        self.beta = beta # Weight for SSIM loss\n",
    "        self.mse = nn.MSELoss()\n",
    "        self.ssim = StructuralSimilarityIndexMeasure(data_range=1.0).to(device)\n",
    "        self.device = device\n",
    "\n",
    "    def forward(self, y_pred, y_true):\n",
    "        # Ensure inputs are on the same device as the ssim metric\n",
    "        y_pred = y_pred.to(self.device)\n",
    "        y_true = y_true.to(self.device)\n",
    "\n",
    "        # Calculate MSE Loss\n",
    "        mse_loss = self.mse(y_pred, y_true)\n",
    "\n",
    "        # Calculate SSIM Loss (1 - SSIM, as SSIM higher is better)\n",
    "        y_pred_clamped = torch.clamp(y_pred, 0.0, 1.0)\n",
    "        ssim_val = self.ssim(y_pred_clamped, y_true)\n",
    "        ssim_val_clamped = torch.clamp(ssim_val, min=-1.0, max=1.0)\n",
    "        ssim_loss = 1.0 - ssim_val_clamped\n",
    "\n",
    "        # Combine losses\n",
    "        combined_loss = (self.alpha * mse_loss) + (self.beta * ssim_loss)\n",
    "\n",
    "        return combined_loss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "___\n",
    "### Define a super-resolution network\n",
    "\n",
    "#### Here, I have defined a trivial network, which has only two layers and no activation function. We are essentially doing linear filtering."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SuperResolutionNet(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(SuperResolutionNet, self).__init__()\n",
    "\n",
    "        self.relu = nn.ReLU(inplace=True)\n",
    "        upscale_factor = 2\n",
    "        channels_mult = upscale_factor * upscale_factor # = 4\n",
    "\n",
    "        # --- Initial Feature Extraction ---\n",
    "        # Input: 1 x 128 x 128\n",
    "        self.conv1 = nn.Conv2d(in_channels=1, out_channels=80, kernel_size=3, padding=1) # 80 x 128 x 128\n",
    "        self.conv1b = nn.Conv2d(in_channels=80, out_channels=80, kernel_size=3, padding=1) # 80 x 128 x 128\n",
    "\n",
    "        # --- Upscaling Path 1 (128 -> 256) using PixelShuffle ---\n",
    "        # Main Path\n",
    "        self.upscale1_conv = nn.Conv2d(in_channels=80, out_channels=80 * channels_mult, kernel_size=3, padding=1) # 80*4 x 128 x 128\n",
    "        self.pixel_shuffle1 = nn.PixelShuffle(upscale_factor) # -> 80 x 256 x 256\n",
    "        self.conv2 = nn.Conv2d(in_channels=80, out_channels=80, kernel_size=3, padding=1) # 80 x 256 x 256\n",
    "        self.conv2b = nn.Conv2d(in_channels=80, out_channels=80, kernel_size=3, padding=1) # 80 x 256 x 256\n",
    "        # Skip Path 1 (Interpolate features from 128x128 stage)\n",
    "        self.bskip1 = nn.Upsample(scale_factor=upscale_factor, mode='bicubic', align_corners=False) # Interpolates 80 channels to 256x256\n",
    "        # Convolution after combining skip connection 1 (Input channels = 80 from main path + 80 from skip = 160)\n",
    "        self.conv_after_skip1 = nn.Conv2d(in_channels=160, out_channels=80, kernel_size=3, padding=1) # 160x256x256 -> 80x256x256\n",
    "        self.conv_after_skip1b = nn.Conv2d(in_channels=80, out_channels=80, kernel_size=3, padding=1) # 80x256x256\n",
    "\n",
    "        # --- Upscaling Path 2 (256 -> 512) using PixelShuffle & Widened Middle ---\n",
    "        # Main Path\n",
    "        self.upscale2_conv = nn.Conv2d(in_channels=80, out_channels=80 * channels_mult, kernel_size=3, padding=1) # 80*4 x 256 x 256 (Widened)\n",
    "        self.pixel_shuffle2 = nn.PixelShuffle(upscale_factor) # -> 80 x 512 x 512 (Widened)\n",
    "        self.conv3 = nn.Conv2d(in_channels=80, out_channels=80, kernel_size=3, padding=1) # 80 x 512 x 512 (Widened)\n",
    "        self.conv3b = nn.Conv2d(in_channels=80, out_channels=80, kernel_size=3, padding=1) # 80 x 512 x 512 (Widened)\n",
    "        # Skip Path 2 (Interpolate features from 256x256 stage after conv_after_skip1b)\n",
    "        self.bskip2 = nn.Upsample(scale_factor=upscale_factor, mode='bicubic', align_corners=False) # Interpolates 80 channels to 512x512\n",
    "        # Convolution after combining skip connection 2 (Input channels = 80 from main path + 80 from skip = 160) (Widened)\n",
    "        self.conv_after_skip2 = nn.Conv2d(in_channels=160, out_channels=80, kernel_size=3, padding=1) # 160x512x512 -> 80x512x512 (Widened)\n",
    "        self.conv_after_skip2b = nn.Conv2d(in_channels=80, out_channels=80, kernel_size=3, padding=1) # 80x512x512 (Widened)\n",
    "\n",
    "        # --- Downscaling Path (512 -> 256) ---\n",
    "        # Input channel matches the widened middle stage (80)\n",
    "        self.downscale = nn.Conv2d(in_channels=80, out_channels=80, kernel_size=2, stride=2) # 80x512x512 -> 80x256x256\n",
    "        self.conv4 = nn.Conv2d(in_channels=80, out_channels=80, kernel_size=3, padding=1) # 80x256x256\n",
    "        self.conv4b = nn.Conv2d(in_channels=80, out_channels=80, kernel_size=3, padding=1) # 80x256x256\n",
    "\n",
    "        # --- Final Output Layer ---\n",
    "        # Takes the output of the downscaling path (80 channels) and maps to 1 channel\n",
    "        self.conv_out_final = nn.Conv2d(in_channels=80, out_channels=1, kernel_size=3, padding=1) # 80x256x256 -> 1x256x256\n",
    "\n",
    "    def forward(self, x):\n",
    "        # x: Input 1 x 128 x 128\n",
    "\n",
    "        # --- Initial Feature Extraction ---\n",
    "        features128 = self.relu(self.conv1(x))\n",
    "        features128 = self.relu(self.conv1b(features128)) # 80 x 128 x 128\n",
    "\n",
    "        # --- Upscaling Path 1 + Skip Connection 1 ---\n",
    "        # Calculate interpolated skip features first\n",
    "        skip1_interpolated = self.bskip1(features128) # 80 x 256 x 256\n",
    "        # Main path upscale using PixelShuffle\n",
    "        up1_conv_out = self.upscale1_conv(features128) # 80*4 x 128 x 128\n",
    "        up1_out = self.pixel_shuffle1(up1_conv_out)    # 80 x 256 x 256\n",
    "        features256_main = self.relu(self.conv2(up1_out))\n",
    "        features256_main = self.relu(self.conv2b(features256_main)) # 80 x 256 x 256\n",
    "        # Concatenate main path features and interpolated skip features\n",
    "        concat1 = torch.cat((features256_main, skip1_interpolated), dim=1) # 160 x 256 x 256\n",
    "        # Process combined features\n",
    "        features256_processed = self.relu(self.conv_after_skip1(concat1))\n",
    "        features256_processed = self.relu(self.conv_after_skip1b(features256_processed)) # 80 x 256 x 256\n",
    "\n",
    "        # --- Upscaling Path 2 + Skip Connection 2 ---\n",
    "        # Calculate interpolated skip features (using output from conv_after_skip1b)\n",
    "        skip2_interpolated = self.bskip2(features256_processed) # 80 x 512 x 512\n",
    "        # Main path upscale using PixelShuffle (Widened Middle)\n",
    "        up2_conv_out = self.upscale2_conv(features256_processed) # 80*4 x 256 x 256\n",
    "        up2_out = self.pixel_shuffle2(up2_conv_out)    # 80 x 512 x 512\n",
    "        features512_main = self.relu(self.conv3(up2_out))\n",
    "        features512_main = self.relu(self.conv3b(features512_main)) # 80 x 512 x 512\n",
    "        # Concatenate main path features and interpolated skip features\n",
    "        concat2 = torch.cat((features512_main, skip2_interpolated), dim=1) # 160 x 512 x 512\n",
    "        # Process combined features\n",
    "        features512_processed = self.relu(self.conv_after_skip2(concat2))\n",
    "        features512_processed = self.relu(self.conv_after_skip2b(features512_processed)) # 80 x 512 x 512\n",
    "\n",
    "        # --- Downscaling Path ---\n",
    "        down_out = self.downscale(features512_processed) # 80 x 256 x 256\n",
    "        features256_final = self.relu(self.conv4(down_out))\n",
    "        features256_final = self.relu(self.conv4b(features256_final)) # 80 x 256 x 256\n",
    "\n",
    "        # --- Final Output Layer ---\n",
    "        output = self.conv_out_final(features256_final) # 1 x 256 x 256\n",
    "\n",
    "        return output"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "___\n",
    "### Create a function to execute training. Note, we will call this function later."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Helper function for one training epoch\n",
    "def train_one_epoch(model, train_loader, criterion, optimizer, device, scaler):\n",
    "    model.train() # Set model to training mode\n",
    "    total_train_loss = 0.0\n",
    "    for x_tr_batch, y_tr_batch in train_loader:\n",
    "        x_tr_batch, y_tr_batch = x_tr_batch.to(device), y_tr_batch.to(device)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        # --- Mixed Precision: Forward pass with autocast ---\n",
    "        with torch.amp.autocast(device_type=device.type, dtype=torch.float16, enabled=scaler.is_enabled()):\n",
    "            y_hat_tr_batch = model(x_tr_batch)\n",
    "            loss = criterion(y_hat_tr_batch, y_tr_batch)\n",
    "\n",
    "        # --- Mixed Precision: Scale loss and backward pass ---\n",
    "        scaler.scale(loss).backward()\n",
    "\n",
    "        # --- Mixed Precision: Scaler step and update ---\n",
    "        scaler.step(optimizer)\n",
    "        scaler.update()\n",
    "\n",
    "        total_train_loss += loss.item()\n",
    "\n",
    "    avg_train_loss = total_train_loss / len(train_loader)\n",
    "    return avg_train_loss\n",
    "\n",
    "# Helper function for one validation epoch\n",
    "def validate_one_epoch(model, val_loader, criterion, psnr_metric, ssim_metric, device):\n",
    "    model.eval() # Set model to evaluation mode\n",
    "    total_val_loss = 0.0\n",
    "    total_val_psnr = 0.0\n",
    "    total_val_ssim = 0.0\n",
    "    num_samples = 0\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for x_val_batch, y_val_batch in val_loader:\n",
    "            x_val_batch, y_val_batch = x_val_batch.to(device), y_val_batch.to(device)\n",
    "            batch_size = x_val_batch.size(0)\n",
    "            num_samples += batch_size\n",
    "\n",
    "            # Upsample low-resolution input for baseline comparison (outside autocast if not needed)\n",
    "            x_val_batch_interpolated = torch.nn.functional.interpolate(x_val_batch, scale_factor=2, mode='bicubic', align_corners=False)\n",
    "\n",
    "            # --- Mixed Precision: Validation forward pass with autocast ---\n",
    "            # Enable autocast for potential efficiency, disable if it causes issues with metrics/loss\n",
    "            with torch.amp.autocast(device_type=device.type, dtype=torch.float16, enabled=True):\n",
    "                y_hat_val_batch = model(x_val_batch)\n",
    "                # Ensure loss calculation happens with autocast output type or cast explicitly if needed\n",
    "                val_loss = criterion(y_hat_val_batch, y_val_batch)\n",
    "\n",
    "            total_val_loss += val_loss.item() * batch_size # Accumulate total loss\n",
    "\n",
    "            # Calculate metrics (ensure they handle potential float16 inputs from autocast)\n",
    "            # TorchMetrics usually handles this, but double-check if using custom metrics.\n",
    "            # Cast inputs to float32 if metrics require it:\n",
    "            # psnr = psnr_metric(y_hat_val_batch.float(), y_val_batch.float())\n",
    "            # ssim = ssim_metric(y_hat_val_batch.float(), y_val_batch.float())\n",
    "            psnr = psnr_metric(y_hat_val_batch, y_val_batch)\n",
    "            ssim = ssim_metric(y_hat_val_batch, y_val_batch)\n",
    "\n",
    "            total_val_psnr += psnr.item() * batch_size\n",
    "            total_val_ssim += ssim.item() * batch_size\n",
    "\n",
    "    avg_val_loss = total_val_loss / num_samples # Average loss per image\n",
    "    avg_val_psnr = total_val_psnr / num_samples\n",
    "    avg_val_ssim = total_val_ssim / num_samples\n",
    "    avg_val_score = avg_val_psnr + (40 * avg_val_ssim)\n",
    "\n",
    "    return avg_val_loss, avg_val_score, avg_val_psnr, avg_val_ssim\n",
    "\n",
    "def train_model(model, opt, criterion, scheduler, train_loader, val_loader, num_epoch, patience, device, save_dir='saved_models', scaler=None):\n",
    "    os.makedirs(save_dir, exist_ok=True)\n",
    "    epoch_nums, avg_train_losses, avg_val_losses, avg_val_scores = [], [], [], []\n",
    "    best_val_score = -float('inf')\n",
    "    epochs_no_improve = 0\n",
    "    best_model_state = None\n",
    "\n",
    "    # Initialize metrics outside the loop - ensure they handle accumulation correctly or reset them if needed\n",
    "    # For torchmetrics, calling them per batch and averaging manually is often safer than relying on internal state across epochs.\n",
    "    # We will re-initialize inside validate_one_epoch for clarity here.\n",
    "    psnr_metric = PeakSignalNoiseRatio().to(device)\n",
    "    ssim_metric = StructuralSimilarityIndexMeasure(data_range=1.0).to(device) # data_range=1.0 assumes normalization\n",
    "\n",
    "    overall_pbar = tqdm(range(num_epoch), desc=\"Overall Training Progress\")\n",
    "\n",
    "    for epoch in overall_pbar:\n",
    "        # --- Training ---\n",
    "        avg_train_loss = train_one_epoch(model, train_loader, criterion, opt, device, scaler)\n",
    "\n",
    "        # --- Validation ---\n",
    "        avg_val_loss, avg_val_score, avg_val_psnr, avg_val_ssim = validate_one_epoch(\n",
    "            model, val_loader, criterion, psnr_metric, ssim_metric, device\n",
    "        )\n",
    "\n",
    "        # Store metrics for plot\n",
    "        epoch_nums.append(epoch + 1)\n",
    "        avg_train_losses.append(avg_train_loss)\n",
    "        avg_val_losses.append(avg_val_loss)\n",
    "        avg_val_scores.append(avg_val_score)\n",
    "\n",
    "        # --- Update Progress Bar ---\n",
    "        current_lr = opt.param_groups[0]['lr']\n",
    "        overall_pbar.set_description(\n",
    "            f\"LR: {current_lr:.1e} | Train Loss: {avg_train_loss:.4f} | Current Val Score: {avg_val_score:.2f} | Best Val Score: {best_val_score:.2f} | Epochs Since Best: {epochs_no_improve:.0f}\"\n",
    "        )\n",
    "\n",
    "        # --- Learning Rate Scheduler Step ---\n",
    "        scheduler.step(avg_val_score)\n",
    "\n",
    "        # --- Checkpointing and Early Stopping ---\n",
    "        if avg_val_score > best_val_score:\n",
    "            best_val_score = avg_val_score\n",
    "            epochs_no_improve = 0\n",
    "            best_model_state = copy.deepcopy(model.state_dict())\n",
    "            # Optionally add a print statement here if desired\n",
    "        else:\n",
    "            epochs_no_improve += 1\n",
    "\n",
    "        if epochs_no_improve >= patience:\n",
    "            print(f\"\\nEarly stopping triggered after {patience} epochs with no improvement.\")\n",
    "            break\n",
    "\n",
    "    # --- Save Best Model ---\n",
    "    if best_model_state:\n",
    "        timestamp = datetime.datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
    "        score_str = f\"{best_val_score:.2f}\".replace('.', 'p')\n",
    "        filename = f\"best_model_score_{score_str}_time_{timestamp}.pth\"\n",
    "        save_path = os.path.join(save_dir, filename)\n",
    "        torch.save(best_model_state, save_path)\n",
    "        print(f\"\\nTraining finished. Best model saved to '{save_path}' with score: {best_val_score:.2f}\")\n",
    "        model.load_state_dict(best_model_state) # Load best state into model\n",
    "    else:\n",
    "        print(\"\\nTraining finished. No model state was saved.\")\n",
    "\n",
    "    # --- Final Plot ---\n",
    "    fig_final, ax1_final = plt.subplots(figsize=(10, 6))\n",
    "    ax2_final = ax1_final.twinx()\n",
    "    line1_final, = ax1_final.plot(epoch_nums, avg_train_losses, 'r-', label='Training Loss')\n",
    "    line2_final, = ax1_final.plot(epoch_nums, avg_val_losses, 'r--', label='Validation Loss')\n",
    "    line3_final, = ax2_final.plot(epoch_nums, avg_val_scores, 'b-', label='Validation Score')\n",
    "    ax1_final.set_xlabel('Epochs')\n",
    "    ax1_final.set_ylabel('Loss', color='tab:red')\n",
    "    ax1_final.tick_params(axis='y', labelcolor='tab:red')\n",
    "    ax2_final.set_ylabel('Validation Score (PSNR + 40*SSIM)', color='tab:blue')\n",
    "    ax2_final.tick_params(axis='y', labelcolor='tab:blue')\n",
    "    ax1_final.legend(handles=[line1_final, line2_final], loc='upper left')\n",
    "    ax2_final.legend(handles=[line3_final], loc='upper right')\n",
    "    ax1_final.grid(True)\n",
    "    plt.title('Final Training Progress')\n",
    "    fig_final.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "    return best_val_score"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "___\n",
    "### Now, let us define hyperparameters and train the network. \n",
    "\n",
    "#### Note, in addition to the parameters that controls the network architecture or the training process, you need to select/initialize (i) a data loader, (ii) a model, (iii) an optimizer, and (iv) a loss function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Verifying Clamped Values for Training Set ---\n",
      "Low-Res Images (Clamped): Min=0.00000000, Max=0.91556430\n",
      "High-Res Images (Clamped): Min=0.00000000, Max=1.00000000\n",
      "----------------------------------------------\n",
      "\n",
      "--- Verifying Clamped Values for Validation Set ---\n",
      "Low-Res Images (Clamped): Min=0.00000000, Max=0.90858984\n",
      "High-Res Images (Clamped): Min=0.00000000, Max=1.00000000\n",
      "----------------------------------------------\n",
      "Number of original training images: 507\n",
      "Number of training images after augmentation: 4056\n",
      "Number of original validation images: 30\n",
      "Number of validation images (no augmentation): 30\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7197bd09e0134e4f8dab3f037cf651c0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Overall Training Progress:   0%|          | 0/200 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mKeyboardInterrupt\u001b[39m                         Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[8]\u001b[39m\u001b[32m, line 50\u001b[39m\n\u001b[32m     42\u001b[39m scheduler = ReduceLROnPlateau(\n\u001b[32m     43\u001b[39m     opt,\n\u001b[32m     44\u001b[39m     mode=\u001b[33m'\u001b[39m\u001b[33mmax\u001b[39m\u001b[33m'\u001b[39m,\n\u001b[32m     45\u001b[39m     factor=scheduler_factor,\n\u001b[32m     46\u001b[39m     patience=scheduler_patience\n\u001b[32m     47\u001b[39m )\n\u001b[32m     49\u001b[39m \u001b[38;5;66;03m# --- Train the model ---\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m50\u001b[39m best_score = \u001b[43mtrain_model\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m     51\u001b[39m \u001b[43m    \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m=\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     52\u001b[39m \u001b[43m    \u001b[49m\u001b[43mopt\u001b[49m\u001b[43m=\u001b[49m\u001b[43mopt\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     53\u001b[39m \u001b[43m    \u001b[49m\u001b[43mcriterion\u001b[49m\u001b[43m=\u001b[49m\u001b[43mcriterion\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     54\u001b[39m \u001b[43m    \u001b[49m\u001b[43mscheduler\u001b[49m\u001b[43m=\u001b[49m\u001b[43mscheduler\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     55\u001b[39m \u001b[43m    \u001b[49m\u001b[43mtrain_loader\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtrain_loader\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     56\u001b[39m \u001b[43m    \u001b[49m\u001b[43mval_loader\u001b[49m\u001b[43m=\u001b[49m\u001b[43mval_loader\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     57\u001b[39m \u001b[43m    \u001b[49m\u001b[43mnum_epoch\u001b[49m\u001b[43m=\u001b[49m\u001b[43mnum_epoch\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     58\u001b[39m \u001b[43m    \u001b[49m\u001b[43mpatience\u001b[49m\u001b[43m=\u001b[49m\u001b[43mpatience_early_stopping\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     59\u001b[39m \u001b[43m    \u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m=\u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     60\u001b[39m \u001b[43m    \u001b[49m\u001b[43msave_dir\u001b[49m\u001b[43m=\u001b[49m\u001b[43msave_dir\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     61\u001b[39m \u001b[43m    \u001b[49m\u001b[43mscaler\u001b[49m\u001b[43m=\u001b[49m\u001b[43mscaler\u001b[49m\n\u001b[32m     62\u001b[39m \u001b[43m)\u001b[49m\n\u001b[32m     64\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33mBest validation score achieved during training: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mbest_score\u001b[38;5;132;01m:\u001b[39;00m\u001b[33m.2f\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m)\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[7]\u001b[39m\u001b[32m, line 88\u001b[39m, in \u001b[36mtrain_model\u001b[39m\u001b[34m(model, opt, criterion, scheduler, train_loader, val_loader, num_epoch, patience, device, save_dir, scaler)\u001b[39m\n\u001b[32m     84\u001b[39m overall_pbar = tqdm(\u001b[38;5;28mrange\u001b[39m(num_epoch), desc=\u001b[33m\"\u001b[39m\u001b[33mOverall Training Progress\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m     86\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m epoch \u001b[38;5;129;01min\u001b[39;00m overall_pbar:\n\u001b[32m     87\u001b[39m     \u001b[38;5;66;03m# --- Training ---\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m88\u001b[39m     avg_train_loss = \u001b[43mtrain_one_epoch\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrain_loader\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcriterion\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mopt\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mscaler\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     90\u001b[39m     \u001b[38;5;66;03m# --- Validation ---\u001b[39;00m\n\u001b[32m     91\u001b[39m     avg_val_loss, avg_val_score, avg_val_psnr, avg_val_ssim = validate_one_epoch(\n\u001b[32m     92\u001b[39m         model, val_loader, criterion, psnr_metric, ssim_metric, device\n\u001b[32m     93\u001b[39m     )\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[7]\u001b[39m\u001b[32m, line 19\u001b[39m, in \u001b[36mtrain_one_epoch\u001b[39m\u001b[34m(model, train_loader, criterion, optimizer, device, scaler)\u001b[39m\n\u001b[32m     16\u001b[39m scaler.scale(loss).backward()\n\u001b[32m     18\u001b[39m \u001b[38;5;66;03m# --- Mixed Precision: Scaler step and update ---\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m19\u001b[39m \u001b[43mscaler\u001b[49m\u001b[43m.\u001b[49m\u001b[43mstep\u001b[49m\u001b[43m(\u001b[49m\u001b[43moptimizer\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     20\u001b[39m scaler.update()\n\u001b[32m     22\u001b[39m total_train_loss += loss.item()\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\Rahb\\Documents\\GitHub\\BME5710-FinalProject\\.venv\\Lib\\site-packages\\torch\\amp\\grad_scaler.py:457\u001b[39m, in \u001b[36mGradScaler.step\u001b[39m\u001b[34m(self, optimizer, *args, **kwargs)\u001b[39m\n\u001b[32m    451\u001b[39m     \u001b[38;5;28mself\u001b[39m.unscale_(optimizer)\n\u001b[32m    453\u001b[39m \u001b[38;5;28;01massert\u001b[39;00m (\n\u001b[32m    454\u001b[39m     \u001b[38;5;28mlen\u001b[39m(optimizer_state[\u001b[33m\"\u001b[39m\u001b[33mfound_inf_per_device\u001b[39m\u001b[33m\"\u001b[39m]) > \u001b[32m0\u001b[39m\n\u001b[32m    455\u001b[39m ), \u001b[33m\"\u001b[39m\u001b[33mNo inf checks were recorded for this optimizer.\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m--> \u001b[39m\u001b[32m457\u001b[39m retval = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_maybe_opt_step\u001b[49m\u001b[43m(\u001b[49m\u001b[43moptimizer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moptimizer_state\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    459\u001b[39m optimizer_state[\u001b[33m\"\u001b[39m\u001b[33mstage\u001b[39m\u001b[33m\"\u001b[39m] = OptState.STEPPED\n\u001b[32m    461\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m retval\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\Rahb\\Documents\\GitHub\\BME5710-FinalProject\\.venv\\Lib\\site-packages\\torch\\amp\\grad_scaler.py:351\u001b[39m, in \u001b[36mGradScaler._maybe_opt_step\u001b[39m\u001b[34m(self, optimizer, optimizer_state, *args, **kwargs)\u001b[39m\n\u001b[32m    343\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34m_maybe_opt_step\u001b[39m(\n\u001b[32m    344\u001b[39m     \u001b[38;5;28mself\u001b[39m,\n\u001b[32m    345\u001b[39m     optimizer: torch.optim.Optimizer,\n\u001b[32m   (...)\u001b[39m\u001b[32m    348\u001b[39m     **kwargs: Any,\n\u001b[32m    349\u001b[39m ) -> Optional[\u001b[38;5;28mfloat\u001b[39m]:\n\u001b[32m    350\u001b[39m     retval: Optional[\u001b[38;5;28mfloat\u001b[39m] = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m351\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28msum\u001b[39m(v.item() \u001b[38;5;28;01mfor\u001b[39;00m v \u001b[38;5;129;01min\u001b[39;00m optimizer_state[\u001b[33m\"\u001b[39m\u001b[33mfound_inf_per_device\u001b[39m\u001b[33m\"\u001b[39m].values()):\n\u001b[32m    352\u001b[39m         retval = optimizer.step(*args, **kwargs)\n\u001b[32m    353\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m retval\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\Rahb\\Documents\\GitHub\\BME5710-FinalProject\\.venv\\Lib\\site-packages\\torch\\amp\\grad_scaler.py:351\u001b[39m, in \u001b[36m<genexpr>\u001b[39m\u001b[34m(.0)\u001b[39m\n\u001b[32m    343\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34m_maybe_opt_step\u001b[39m(\n\u001b[32m    344\u001b[39m     \u001b[38;5;28mself\u001b[39m,\n\u001b[32m    345\u001b[39m     optimizer: torch.optim.Optimizer,\n\u001b[32m   (...)\u001b[39m\u001b[32m    348\u001b[39m     **kwargs: Any,\n\u001b[32m    349\u001b[39m ) -> Optional[\u001b[38;5;28mfloat\u001b[39m]:\n\u001b[32m    350\u001b[39m     retval: Optional[\u001b[38;5;28mfloat\u001b[39m] = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m351\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28msum\u001b[39m(\u001b[43mv\u001b[49m\u001b[43m.\u001b[49m\u001b[43mitem\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;28;01mfor\u001b[39;00m v \u001b[38;5;129;01min\u001b[39;00m optimizer_state[\u001b[33m\"\u001b[39m\u001b[33mfound_inf_per_device\u001b[39m\u001b[33m\"\u001b[39m].values()):\n\u001b[32m    352\u001b[39m         retval = optimizer.step(*args, **kwargs)\n\u001b[32m    353\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m retval\n",
      "\u001b[31mKeyboardInterrupt\u001b[39m: "
     ]
    }
   ],
   "source": [
    "# --- Hyperparameters ---\n",
    "batch_size = 8\n",
    "lr = 1e-4\n",
    "num_epoch = 200\n",
    "patience_early_stopping = 25\n",
    "scheduler_patience = 10\n",
    "scheduler_factor = 0.05\n",
    "loss_alpha = 0.5\n",
    "loss_beta = 0.5\n",
    "save_dir = 'saved_models'\n",
    "\n",
    "# --- Setup ---\n",
    "# Define a transform to convert images to PyTorch tensors\n",
    "transform = transforms.Compose([transforms.ToTensor()])\n",
    "# Create the dataset for images\n",
    "train_dataset = TIFFDataset('data/train/high-res', 'data/train/low-res', transform=transform, augment=True, dataset_name=\"Training Set\")\n",
    "val_dataset = TIFFDataset('data/val/high-res', 'data/val/low-res', transform=transform, augment=False, dataset_name=\"Validation Set\")\n",
    "# Verify that all data are normalized\n",
    "train_dataset.verify_clamped_values()\n",
    "val_dataset.verify_clamped_values()\n",
    "# Create DataLoaders\n",
    "train_loader = create_loader(train_dataset, batch_size, shuffle_data=True)\n",
    "val_loader = create_loader(val_dataset, batch_size, shuffle_data=False)\n",
    "# Print dataset sizes\n",
    "if 'train_dataset' in locals() and 'val_dataset' in locals():\n",
    "    print(f'Number of original training images: {getattr(train_dataset, \"num_original_images\", \"N/A\")}')\n",
    "    print(f'Number of training images after augmentation: {len(train_dataset)}')\n",
    "    print(f'Number of original validation images: {getattr(val_dataset, \"num_original_images\", \"N/A\")}')\n",
    "    print(f'Number of validation images (no augmentation): {len(val_dataset)}')\n",
    "\n",
    "# Model\n",
    "model = SuperResolutionNet().to(device)\n",
    "scaler = torch.amp.GradScaler('cuda')\n",
    "\n",
    "# Optimizer\n",
    "opt = optim.Adam(model.parameters(), lr=lr)\n",
    "\n",
    "# Loss Function (Criterion)\n",
    "criterion = CombinedLoss(alpha=loss_alpha, beta=loss_beta, device=device).to(device)\n",
    "\n",
    "# Learning Rate Scheduler\n",
    "scheduler = ReduceLROnPlateau(\n",
    "    opt,\n",
    "    mode='max',\n",
    "    factor=scheduler_factor,\n",
    "    patience=scheduler_patience\n",
    ")\n",
    "\n",
    "# --- Train the model ---\n",
    "best_score = train_model(\n",
    "    model=model,\n",
    "    opt=opt,\n",
    "    criterion=criterion,\n",
    "    scheduler=scheduler,\n",
    "    train_loader=train_loader,\n",
    "    val_loader=val_loader,\n",
    "    num_epoch=num_epoch,\n",
    "    patience=patience_early_stopping,\n",
    "    device=device,\n",
    "    save_dir=save_dir,\n",
    "    scaler=scaler\n",
    ")\n",
    "\n",
    "print(f\"\\nBest validation score achieved during training: {best_score:.2f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "___\n",
    "### Apply it one of the validation image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load one (low-res, high-res) image pair from validation dataset and move it to the dedvice\n",
    "val_low_res, val_high_res = val_dataset[1]  # Input (128x128), Ground truth (256x256)\n",
    "val_low_res, val_high_res = val_low_res.to(device), val_high_res.to(device)\n",
    "\n",
    "# Keep the interpolated version ONLY for visualization comparison\n",
    "val_low_res_interpolated = torch.nn.functional.interpolate(val_low_res.unsqueeze(0), scale_factor=2, mode='bicubic', align_corners=False).squeeze(0)\n",
    "\n",
    "# Apply the trained model to the original low-res image\n",
    "# Add batch dimension for model, then remove it and detach\n",
    "val_super_res = model(val_low_res.unsqueeze(0)).detach().squeeze(0)\n",
    "\n",
    "# Convert tensors to numpy for visualization\n",
    "val_low_res_np = val_low_res_interpolated.squeeze().cpu().numpy()  # Use the interpolated version for vis/error maps\n",
    "val_high_res_np = val_high_res.squeeze().cpu().numpy()\n",
    "val_super_res_np = val_super_res.squeeze().cpu().numpy()\n",
    "\n",
    "# Plot an example image and error maps\n",
    "fig, ax = plt.subplots(2, 3, figsize=(10, 7))\n",
    "\n",
    "# Plot images\n",
    "ax[0, 0].imshow(val_high_res_np, cmap='gray', vmin=0, vmax=1)\n",
    "ax[0, 0].set_title('ground truth (high-res)')\n",
    "ax[0, 0].axis('off')\n",
    "\n",
    "ax[0, 1].imshow(val_low_res_np, cmap='gray', vmin=0, vmax=1)\n",
    "ax[0, 1].set_title('interpolated low-res image')\n",
    "ax[0, 1].axis('off')\n",
    "\n",
    "ax[0, 2].imshow(val_super_res_np, cmap='gray', vmin=0, vmax=1)\n",
    "ax[0, 2].set_title('super-resolved image')\n",
    "ax[0, 2].axis('off')\n",
    "\n",
    "# Error maps\n",
    "ax[1, 0].imshow(5 * np.abs(val_high_res_np - val_high_res_np), cmap='gray', vmin=0, vmax=1)\n",
    "ax[1, 0].axis('off')\n",
    "ax[1, 0].text(0.02, 0.98, r'$\\times 5$', transform=ax[1, 0].transAxes, fontsize=14, va='top', ha='left', color='white')\n",
    "\n",
    "ax[1, 1].imshow(5 * np.abs(val_high_res_np - val_low_res_np), cmap='gray', vmin=0, vmax=1)\n",
    "ax[1, 1].axis('off')\n",
    "ax[1, 1].text(0.02, 0.98, r'$\\times 5$', transform=ax[1, 1].transAxes, fontsize=14, va='top', ha='left', color='white')\n",
    "\n",
    "ax[1, 2].imshow(5 * np.abs(val_high_res_np - val_super_res_np), cmap='gray', vmin=0, vmax=1)\n",
    "ax[1, 2].axis('off')\n",
    "ax[1, 2].text(0.02, 0.98, r'$\\times 5$', transform=ax[1, 2].transAxes, fontsize=14, va='top', ha='left', color='white')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Compute PSNR and SSIM over the entire validation dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_model(model, data_loader, device):\n",
    "    \"\"\"Computes PSNR, SSIM, and Score over a dataset using a DataLoader.\"\"\"\n",
    "    model.eval() # Set model to evaluation mode\n",
    "    psnr_metric = PeakSignalNoiseRatio().to(device)\n",
    "    ssim_metric = StructuralSimilarityIndexMeasure(data_range=1.0).to(device)\n",
    "\n",
    "    total_psnr_sr = 0.0\n",
    "    total_ssim_sr = 0.0\n",
    "    total_psnr_interp = 0.0\n",
    "    total_ssim_interp = 0.0\n",
    "    num_samples = 0\n",
    "\n",
    "    with torch.no_grad():\n",
    "        eval_pbar = tqdm(data_loader, desc=\"Evaluating Model\")\n",
    "        for x_batch, y_batch in eval_pbar:\n",
    "            x_batch, y_batch = x_batch.to(device), y_batch.to(device) # x_batch is 128x128\n",
    "            batch_size_actual = x_batch.size(0)\n",
    "            num_samples += batch_size_actual\n",
    "\n",
    "            # Keep this interpolation for baseline calculation (outside autocast)\n",
    "            x_batch_interpolated = torch.nn.functional.interpolate(x_batch, scale_factor=2, mode='bicubic', align_corners=False)\n",
    "\n",
    "            # --- Mixed Precision: Evaluation forward pass with autocast ---\n",
    "            # Enable autocast for potential efficiency\n",
    "            with torch.amp.autocast(device_type=device.type, dtype=torch.float16, enabled=True):\n",
    "                y_hat_batch = model(x_batch) # Model outputs 256x256\n",
    "\n",
    "            # Calculate metrics for super-resolved\n",
    "            # Cast inputs to float32 if metrics require it:\n",
    "            # psnr_sr = psnr_metric(y_hat_batch.float(), y_batch.float())\n",
    "            # ssim_sr = ssim_metric(y_hat_batch.float(), y_batch.float())\n",
    "            psnr_sr = psnr_metric(y_hat_batch, y_batch)\n",
    "            ssim_sr = ssim_metric(y_hat_batch, y_batch)\n",
    "            total_psnr_sr += psnr_sr.item() * batch_size_actual\n",
    "            total_ssim_sr += ssim_sr.item() * batch_size_actual\n",
    "\n",
    "            # Calculate metrics for interpolated (baseline) - ensure y_batch is float32 if needed\n",
    "            psnr_interp = psnr_metric(x_batch_interpolated, y_batch)\n",
    "            ssim_interp = ssim_metric(x_batch_interpolated, y_batch)\n",
    "            total_psnr_interp += psnr_interp.item() * batch_size_actual\n",
    "            total_ssim_interp += ssim_interp.item() * batch_size_actual\n",
    "\n",
    "    # Calculate averages\n",
    "    avg_psnr_sr = total_psnr_sr / num_samples\n",
    "    avg_ssim_sr = total_ssim_sr / num_samples\n",
    "    avg_psnr_interp = total_psnr_interp / num_samples\n",
    "    avg_ssim_interp = total_ssim_interp / num_samples\n",
    "\n",
    "    # Calculate scores\n",
    "    avg_score_sr = avg_psnr_sr + (40 * avg_ssim_sr)\n",
    "    avg_score_interp = avg_psnr_interp + (40 * avg_ssim_interp)\n",
    "\n",
    "    # Print results\n",
    "    print(\"\\n--- Final Evaluation Results ---\")\n",
    "    print(f'Average PSNR (interpolated): {avg_psnr_interp:.2f} dB')\n",
    "    print(f'Average PSNR (super-resolved): {avg_psnr_sr:.2f} dB')\n",
    "    print(f'Average SSIM (interpolated): {avg_ssim_interp:.4f}')\n",
    "    print(f'Average SSIM (super-resolved): {avg_ssim_sr:.4f}')\n",
    "    print(f'Average Score (interpolated): {avg_score_interp:.2f}')\n",
    "    print(f'Average Score (super-resolved): {avg_score_sr:.2f}')\n",
    "\n",
    "    return avg_score_sr, avg_score_interp\n",
    "\n",
    "# --- Call the Evaluation Function ---\n",
    "final_sr_score, final_interp_score = evaluate_model(model, val_loader, device)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
