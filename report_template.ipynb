{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0779cb53-a916-41f9-ab95-c24039d89a16",
   "metadata": {},
   "source": [
    "___\n",
    "# **BME 5710 project report**\n",
    "## Instructor -- Rizwan Ahmad (ahmad.46@osu.edu)\n",
    "## BME5710 -- Spring 2025 "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6ca22128-d705-4915-9165-1138474e2eaf",
   "metadata": {},
   "source": [
    "___\n",
    "### Provide descriptive answers at `???` locations and insert figures or tables at `?content?`"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "874d1a16-3368-4f9f-94d1-771dba37e494",
   "metadata": {},
   "source": [
    "___\n",
    "### Write your name below. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9f14effb-5721-472d-b299-c311b1edb084",
   "metadata": {},
   "source": [
    "Answer: Kate Herr"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a6a46acc",
   "metadata": {},
   "source": [
    "___\n",
    "### Write the names of your teammates"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b756447a",
   "metadata": {},
   "source": [
    "Answer: Robert Smith, Luka Medvedovic, Kate Herr, Dhaarini Prasad-Sudha"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "af677ae3-4e2b-49df-a06a-771af1154d91",
   "metadata": {},
   "source": [
    "___\n",
    "### (1.1 -- 5%) Provide a layout of your CNN\n",
    "\n",
    "#### The layout should provide all the necessary details about the CNN architecture including number of channels, size of convolution kernels, activation functions, etc. For inspiration, see examples [here](https://www.geeksforgeeks.org/u-net-architecture-explained/), [here](https://www.researchgate.net/figure/The-architecture-of-Unet_fig2_334287825), and [here](https://www.researchgate.net/figure/Modified-U-net-network-architecture_fig2_356216368)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "738302d6-ccf3-49af-85c7-eac2de4ed5e2",
   "metadata": {},
   "source": [
    "Answer:\n",
    "\n",
    "The resultant CNN we used for this process is an encoder-decoder-style CNN with two upscaling stages (using PixelShuffle), two skip connections, and a final downscaling stage to refine high-resolution outputs. The total upscaling performed is from 128 x 128 to 512 x 512, with the final output downsampled to 256x256. ReLU activation functions were used after every convolution step except the final output. Upsampling was done via PixelShuffle. Additionally, skip connections are used. Below is a detailed breakdown of the details for each of the main steps in the CNN architecture we used:\n",
    "\n",
    "**Input:** 1 × 128 × 128 grayscale image\n",
    "\n",
    "**Feature Extraction:**\n",
    "| Layer | Type         | Channels | Kernel Size | Output Size      | Activation |\n",
    "|-------|--------------|----------|-------------|------------------|------------|\n",
    "| conv1 | Conv2D       | 1 → 80   | 3×3         | 80 × 128 × 128   | ReLU       |\n",
    "| conv1b| Conv2D       | 80 → 80  | 3×3         | 80 × 128 × 128   | ReLU       |\n",
    "\n",
    "**Upscaling Path 1:** (128 → 256)\n",
    "| Layer               | Type           | Channels       | Kernel/Details  | Output Size      | Activation |\n",
    "|--------------------|----------------|----------------|-----------------|------------------|------------|\n",
    "| upscale1_conv      | Conv2D         | 80 → 320       | 3×3             | 320 × 128 × 128  | -          |\n",
    "| pixel_shuffle1     | PixelShuffle   | 320 → 80       | Scale=2         | 80 × 256 × 256   | -          |\n",
    "| conv2 + conv2b     | Conv2D         | 80 → 80 (×2)   | 3×3             | 80 × 256 × 256   | ReLU ×2    |\n",
    "| bskip1             | Bicubic Upsamp.| 80 → 80        | Scale=2         | 80 × 256 × 256   | -          |\n",
    "| concat + conv_after_skip1 → 1b | Conv2D (×2) | 160 → 80 → 80 | 3×3         | 80 × 256 × 256   | ReLU ×2    |\n",
    "\n",
    "**Upscaling Path 2:** (256 → 512)\n",
    "| Layer               | Type           | Channels       | Kernel/Details  | Output Size      | Activation |\n",
    "|--------------------|----------------|----------------|-----------------|------------------|------------|\n",
    "| upscale2_conv      | Conv2D         | 80 → 320       | 3×3             | 320 × 256 × 256  | -          |\n",
    "| pixel_shuffle2     | PixelShuffle   | 320 → 80       | Scale=2         | 80 × 512 × 512   | -          |\n",
    "| conv3 + conv3b     | Conv2D         | 80 → 80 (×2)   | 3×3             | 80 × 512 × 512   | ReLU ×2    |\n",
    "| bskip2             | Bicubic Upsamp.| 80 → 80        | Scale=2         | 80 × 512 × 512   | -          |\n",
    "| concat + conv_after_skip2 → 2b | Conv2D (×2) | 160 → 80 → 80 | 3×3         | 80 × 512 × 512   | ReLU ×2    |\n",
    "\n",
    "**Downscaling Path:** (512 → 256)\n",
    "| Layer   | Type     | Channels | Kernel Size | Output Size      | Activation |\n",
    "|---------|----------|----------|-------------|------------------|------------|\n",
    "| downscale | Conv2D | 80 → 80  | 2×2, Stride=2 | 80 × 256 × 256 | -          |\n",
    "| conv4 + conv4b | Conv2D | 80 → 80 (×2) | 3×3     | 80 × 256 × 256 | ReLU ×2    |\n",
    "\n",
    "\n",
    "**Output Layer:**\n",
    "| Layer          | Type     | Channels | Kernel Size | Output Size    | Activation |\n",
    "|----------------|----------|----------|-------------|----------------|------------|\n",
    "| conv_out_final | Conv2D   | 80 → 1   | 3×3         | 1 × 256 × 256  | None       |"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "53876c54",
   "metadata": {},
   "source": [
    "___\n",
    "### (1.2 -- 1%) List *all* non-trivial features of your CNN and the training process. This may include use of dropout, learning rate scheduling, transfer learning, data augmentation, etc. Don't include items that are visible in the layout provided above."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d1f65401",
   "metadata": {},
   "source": [
    "Answer: \n",
    "\n",
    "**Skip Connections with Interpolation:** Low-level features from previous layers are interpolated using bicubic upsampling and added into deeper layers to keep spatial details.\n",
    "\n",
    "**PixelShuffle for Upsampling:** Final feature maps are upscaled using `PixelShuffle`, which reduces checkerboard artifacts and produces sharper outputs compared to transposed convolutions.\n",
    "\n",
    "**In-Place ReLU Activations:** ReLU activations are applied in-place to improve memory efficiency.\n",
    "\n",
    "**Custom Loss Function (MSE + SSIM):** The loss function is a weighted combination of **Mean Squared Error (MSE)** and **Structural Similarity Index Measure (SSIM)** to balance numerical fidelity with perceptual quality.\n",
    "\n",
    "**Validation Metric = PSNR + 40 × SSIM:** Model selection is based on a **custom validation score**, combining PSNR and SSIM into a single number for improved perceptual relevance.\n",
    "\n",
    "**Data Augmentation:** To increase the amount of training data available for training the network, data augmentation was performed on the existing training images. Data was augmented either through rotation, mirroring, or a combination of the two:\n",
    "- Rotation: 90, 180, or 270 degrees\n",
    "- Mirroring: flipping along horizontal or vertical axis\n",
    "- Combination: rotation by 90 and mirroring along either the horizontal or vertical axis\n",
    "\n",
    "**Learning Rate Scheduler:** A `ReduceLROnPlateau` scheduler is used, reducing the learning rate by a factor of 0.05 if the validation score doesn't improve for 10 epochs.\n",
    "\n",
    "**Early Stopping:** Training stops early if the validation score does not improve for **25 consecutive epochs** (patience-based early stopping).\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "819917d8-0b7d-4cf9-9636-60764389eb90",
   "metadata": {},
   "source": [
    "___\n",
    "### (2.1 -- 0.5%) Provide at least ten hyperparameters that you *could* optimize in your CNN design and training."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "82087b67-e000-489a-95ff-0015550911a5",
   "metadata": {},
   "source": [
    "Answer: \n",
    "1. **Learning Rate:**  Control optimization step size; affects convergence speed/stability.\n",
    "\n",
    "2. **Batch Size:** How many samples are processed before updating the model weights.\n",
    "\n",
    "3. **Number of Epochs:** Too few may underfit, too many may overfit.\n",
    "\n",
    "4. **Loss Function Weights (in this CNN, alpha and beta):** Balance MSE and SSIM in the custom loss function.\n",
    "\n",
    "5. **Optimizer Choice:** I.e. Adam, SGD, RMSprop\n",
    "\n",
    "6. **Learning Rate Scheduler Parameters:**  Scheduler patience (number of epochs to wait before reducing LR) and scheduler factor (multiplier to reduce LR)\n",
    "\n",
    "7. **Upsampling Strategy:** I.e. PixelShuffle, transposed convolution, nearest-neighbor + conv,  etc.\n",
    "\n",
    "8. **Dropout Rate:** Could be added and tuned to help reduce overfittingt.\n",
    "\n",
    "9. **Kernel Sizes:**  Affects the model’s capacity; tradeoff between performance and computational cost.\n",
    "\n",
    "10. **Data Augmentation Strategies:** Choosing different/more rotation angles, flipping, etc."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "604e191e-148d-4da1-89e7-f44e40cff7f0",
   "metadata": {},
   "source": [
    "___\n",
    "### (2.2 -- 0.5%) Now, list the hyperparameters that you *have* optimized."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "572f0d19-fca5-45cc-a9d9-d9cb87dde1cf",
   "metadata": {},
   "source": [
    "Answer: \n",
    "\n",
    "1. **Learning Rate:**  Set to `1e-4` for the Adam optimizer\n",
    "\n",
    "2. **Batch Size:**  Set to `8` based on available memory and convergence considerations\n",
    "\n",
    "3. **Number of Epochs:**  Set to `200`; enough epochs for convergence without overfitting\n",
    "\n",
    "4. **Loss Function Weights (alpha, beta):**  Set to `alpha = 0.5` and `beta = 0.5`, balancing contributions of (MSE) and (SSIM).\n",
    "\n",
    "5. **Optimizer Choice:** Set as `Adam`, optimized for stable training with adaptive learning rates.\n",
    "\n",
    "6. **Learning Rate Scheduler Parameters:**  \n",
    "   - `scheduler_patience = 10`: How long to wait before adjusting the learning rate.\n",
    "   - `scheduler_factor = 0.05`: Multiplies learning rate by this factor when improvement plateaus.\n",
    "\n",
    "7. **Data Augmentation:**  Random transformations `flipping`,`rotation angle` to increase generalization.\n",
    "\n",
    "8. **Early Stopping Patience:** Set to `25`; training will stop if no improvement in validation score for 25 epochs\n",
    "\n",
    "9. **Mixed Precision Training:**  Optimized for  to speed up training and reduce memory usage \n",
    "\n",
    "10. **Combined Loss Function:** Combination of MSE and SSIM\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dbbfa6b3-c2e7-48af-bf3a-88d912c015ab",
   "metadata": {},
   "source": [
    "___\n",
    "### (2.3 -- 0.5%) Describe your stretegy/approach for optimizing hyperparameters."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0d5850bf-b2b9-4541-8c10-cc5cd9a4651a",
   "metadata": {},
   "source": [
    "Answer: ???"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3a9d7bc2-ed30-432f-bf37-556ca4bdd575",
   "metadata": {},
   "source": [
    "___\n",
    "### (2.4 -- 0.5%) What loss function did you use?  Express the loss function mathematically."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6c29d8db-783d-45f6-aef1-d63b16c29570",
   "metadata": {},
   "source": [
    "Answer: \n",
    "\n",
    "The combined loss is a weighted sum of Mean Squared Error (MSE) loss and Structural Similarity Index Measure (SSIM):\n",
    "\n",
    "    CombinedLoss = alpha * MSE + beta * (1 - SSIM)\n",
    "\n",
    "Where:\n",
    "- `alpha` is the weight for the MSE loss.\n",
    "- `beta` is the weight for the SSIM loss.\n",
    "- `MSE` is the Mean Squared Error between the predicted and ground truth images.\n",
    "- `SSIM` is the Structural Similarity Index between the predicted and ground truth images (clamped to [−1, 1]).\n",
    "\n",
    "Lower MSE indicates better image quality, hence why MSE is minimized. Higher `SSIM` indicates better quality, which is why 1-SSIM is minimized. The predicted image is clamped between 0 and 1 before SSIM is computed.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8f12df09-b84d-47d7-a0a8-52356975c8d3",
   "metadata": {},
   "source": [
    "___\n",
    "### (2.5 -- 1%) Calculate the number of learnable parameters in your final CNN. How does that number compare with the number of training samples? Is your network overfitting or underfitting and how did you arrive at that conclusion? Explain that in the context of loss vs. epoch and/or performance metric vs. epoch curve for training and validation fold."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "318136f5-a027-4128-a481-9e90d7ea85b6",
   "metadata": {},
   "source": [
    "Answer: ???"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8ddb81cd",
   "metadata": {},
   "source": [
    "___\n",
    "### (3.1 -- 5%) Insert (or draw using Markdown) a table that provides average values of PSNR, SSIM, and PSNR + 40xSSIM for (i) interpolated low-resolution images, (ii) images super-resolved with the TrivialNet model included in `starter_code.ipynb`, and (iii) images super-resolved with your CNN. Include the metrics from training, validation, and test folds."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "62727b26",
   "metadata": {},
   "source": [
    "PSNR, SSIM, and Combined Score metrics from the **training**, **validation**, and **test** folds.\n",
    "\n",
    "| Method                    | Fold       | PSNR (↑) | SSIM (↑) | PSNR + 40×SSIM (↑) |\n",
    "|--------------------------|------------|----------|----------|---------------------|\n",
    "| Interpolated LR          | Training   | XX.XX    | 0.XXXX   | XX.XX               |\n",
    "|                          | Validation | XX.XX    | 0.XXXX   | XX.XX               |\n",
    "|                          | Test       | XX.XX    | 0.XXXX   | XX.XX               |\n",
    "| TrivialNet               | Training   | XX.XX    | 0.XXXX   | XX.XX               |\n",
    "|                          | Validation | XX.XX    | 0.XXXX   | XX.XX               |\n",
    "|                          | Test       | XX.XX    | 0.XXXX   | XX.XX               |\n",
    "| Your CNN                 | Training   | XX.XX    | 0.XXXX   | XX.XX               |\n",
    "|                          | Validation | XX.XX    | 0.XXXX   | XX.XX               |\n",
    "|                          | Test       | XX.XX    | 0.XXXX   | XX.XX               |\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c308882b",
   "metadata": {},
   "source": [
    "___\n",
    "### (3.2 -- 1%) When it comes to comparing images, what does SSIM capture that PSNR does not?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e814291f",
   "metadata": {},
   "source": [
    "Answer: ???"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2f83ccbb",
   "metadata": {},
   "source": [
    "___\n",
    "### (4.1 -- 4%) Display a figure where the first row (from left to right) shows an example of high-resolution image, interpolated low-resolution image, image super-resolved with TrivialNet, and the image super-resolved with your CNN, and the second row shows corresponding error maps after 5-fold amplification. For this figure, you may select any image from the test dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3cb7daed",
   "metadata": {},
   "source": [
    "`?content?`"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "93c89ab4-45de-4bd2-ac00-57eed8ed6e95",
   "metadata": {},
   "source": [
    "___\n",
    "### (4.2 -- 1%) From (4.1), subjectively identify which image features are well-preserved by your CNN and which are lost. Additionally, describe how you could further improve the performance of your model if given more time and resources."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "98c3b816",
   "metadata": {},
   "source": [
    "Answer: ???"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
