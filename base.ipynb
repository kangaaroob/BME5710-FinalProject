{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "___\n",
    "# **Starter code for BME 5710 project**\n",
    "## Instructor -- Rizwan Ahmad (ahmad.46@osu.edu)\n",
    "## BME5710 -- Spring 2025"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "___\n",
    "### Import libraries and sub-libraries."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "from PIL import Image\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torchvision import transforms\n",
    "import matplotlib.pyplot as plt\n",
    "from torchvision.transforms import functional as TF\n",
    "import torch.nn.functional as F\n",
    "from torchmetrics.image.psnr import PeakSignalNoiseRatio\n",
    "from torchmetrics.image.ssim import StructuralSimilarityIndexMeasure\n",
    "from tqdm.notebook import tqdm\n",
    "import datetime\n",
    "import copy\n",
    "from torch.optim.lr_scheduler import ReduceLROnPlateau"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "___\n",
    "### Calling a custom code to change the default font for figures to `Computer Modern`. (Optional)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from fontsetting import font_cmu\n",
    "# plt = font_cmu(plt)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "___\n",
    "### Check the hardware that is at your disposal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print('Device available:', device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "___\n",
    "### Read training data from `data/train/hig-res` and `data/train/low-res`\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TIFFDataset(Dataset):\n",
    "    def __init__(self, high_res_dir, low_res_dir, transform=None, augment=False):\n",
    "        self.high_res_dir = high_res_dir\n",
    "        self.low_res_dir = low_res_dir\n",
    "        self.transform = transform\n",
    "        self.augment = augment\n",
    "        self.original_filenames = sorted([f for f in os.listdir(high_res_dir) if f.endswith('.tif')])\n",
    "        self.num_original_images = len(self.original_filenames)\n",
    "\n",
    "        self.low_res_images = []\n",
    "        self.high_res_images = []\n",
    "\n",
    "        for filename in self.original_filenames:\n",
    "            high_res_path = os.path.join(self.high_res_dir, filename)\n",
    "            low_res_path = os.path.join(self.low_res_dir, filename)\n",
    "\n",
    "            # Load original images\n",
    "            try:\n",
    "                hr_img = Image.open(high_res_path)\n",
    "                lr_img = Image.open(low_res_path)\n",
    "            except Exception as e:\n",
    "                print(f\"Error loading image {filename}: {e}\")\n",
    "                continue\n",
    "\n",
    "            # Resize low-res to 128x128\n",
    "            lr_img = lr_img.resize((128, 128), Image.BICUBIC)\n",
    "\n",
    "            original_lr, original_hr = lr_img, hr_img\n",
    "            processed_lr = []\n",
    "            processed_hr = []\n",
    "\n",
    "            if self.augment:\n",
    "                # --- Apply transformations ---\n",
    "                # 1) Original\n",
    "                processed_lr.append(original_lr)\n",
    "                processed_hr.append(original_hr)\n",
    "                # 2) Rotated by 90 degrees\n",
    "                processed_lr.append(original_lr.rotate(90))\n",
    "                processed_hr.append(original_hr.rotate(90))\n",
    "                # 3) Rotated by 180 degrees\n",
    "                processed_lr.append(original_lr.rotate(180))\n",
    "                processed_hr.append(original_hr.rotate(180))\n",
    "                # 4) Rotated by 270 degrees\n",
    "                processed_lr.append(original_lr.rotate(270))\n",
    "                processed_hr.append(original_hr.rotate(270))\n",
    "\n",
    "                # Apply flips\n",
    "                lr_v_flip = original_lr.transpose(Image.FLIP_LEFT_RIGHT)\n",
    "                hr_v_flip = original_hr.transpose(Image.FLIP_LEFT_RIGHT)\n",
    "                lr_h_flip = original_lr.transpose(Image.FLIP_TOP_BOTTOM)\n",
    "                hr_h_flip = original_hr.transpose(Image.FLIP_TOP_BOTTOM)\n",
    "\n",
    "                # 5) Mirrored on the vertical axis (Flip Left/Right)\n",
    "                processed_lr.append(lr_v_flip)\n",
    "                processed_hr.append(hr_v_flip)\n",
    "                # 6) Mirrored on the horizontal axis (Flip Top/Bottom)\n",
    "                processed_lr.append(lr_h_flip)\n",
    "                processed_hr.append(hr_h_flip)\n",
    "\n",
    "                # Apply flips combined with rotation (unique combinations)\n",
    "                # 7) Rotated by 90 degrees and mirrored on the vertical axis\n",
    "                processed_lr.append(lr_v_flip.rotate(90))\n",
    "                processed_hr.append(hr_v_flip.rotate(90))\n",
    "                # 8) Rotated by 90 degrees and mirrored on the horizontal axis\n",
    "                processed_lr.append(lr_h_flip.rotate(90))\n",
    "                processed_hr.append(hr_h_flip.rotate(90))\n",
    "            else:\n",
    "                # If not augmenting, just use the original\n",
    "                processed_lr.append(original_lr)\n",
    "                processed_hr.append(original_hr)\n",
    "\n",
    "            # Apply the final transform (ToTensor) and store\n",
    "            if self.transform:\n",
    "                for lr, hr in zip(processed_lr, processed_hr):\n",
    "                    self.low_res_images.append(self.transform(lr))\n",
    "                    self.high_res_images.append(self.transform(hr))\n",
    "            else:\n",
    "                 # If no transform, store PIL images (not recommended for training/eval)\n",
    "                self.low_res_images.extend(processed_lr)\n",
    "                self.high_res_images.extend(processed_hr)\n",
    "\n",
    "    # Get the number of samples in the dataset (original or augmented)\n",
    "    def __len__(self):\n",
    "        return len(self.low_res_images)\n",
    "\n",
    "    # Get the sample at the given index\n",
    "    def __getitem__(self, idx):\n",
    "        # Return pre-processed tensors\n",
    "        return self.low_res_images[idx], self.high_res_images[idx]\n",
    "\n",
    "# Define a transform to convert images to PyTorch tensors\n",
    "transform = transforms.Compose([\n",
    "    transforms.ToTensor()\n",
    "])\n",
    "\n",
    "# Create the dataset for training images WITH augmentation\n",
    "train_dataset = TIFFDataset('data/train/high-res', 'data/train/low-res', transform=transform, augment=True)\n",
    "val_dataset = TIFFDataset('data/val/high-res', 'data/val/low-res', transform=transform, augment=False)\n",
    "\n",
    "# Function to create data loader\n",
    "def create_loader(dataset, batch_size, shuffle_data=True): # Added shuffle parameter\n",
    "    torch.manual_seed(0)  # For reproducibility\n",
    "    return DataLoader(dataset, batch_size=batch_size, shuffle=shuffle_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create a custom loss class to easily implement custom loss functions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CombinedLoss(nn.Module):\n",
    "    def __init__(self, alpha=0.5, beta=0.5, device='cpu'):\n",
    "        super(CombinedLoss, self).__init__()\n",
    "        self.alpha = alpha # Weight for MSE (PSNR) loss\n",
    "        self.beta = beta # Weight for SSIM loss\n",
    "        self.mse = nn.MSELoss()\n",
    "        self.ssim = StructuralSimilarityIndexMeasure(data_range=1.0).to(device)\n",
    "        self.device = device\n",
    "\n",
    "    def forward(self, y_pred, y_true):\n",
    "        # Ensure inputs are on the same device as the ssim metric\n",
    "        y_pred = y_pred.to(self.device)\n",
    "        y_true = y_true.to(self.device)\n",
    "\n",
    "        # Calculate MSE Loss\n",
    "        mse_loss = self.mse(y_pred, y_true)\n",
    "\n",
    "        # Calculate SSIM Loss (1 - SSIM, as SSIM higher is better)\n",
    "        ssim_val = self.ssim(y_pred, y_true)\n",
    "        ssim_loss = 1.0 - ssim_val\n",
    "\n",
    "        # Combine losses\n",
    "        combined_loss = (self.alpha * mse_loss) + (self.beta * ssim_loss)\n",
    "\n",
    "        return combined_loss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "___\n",
    "### Define a super-resolution network\n",
    "\n",
    "#### Here, I have defined a trivial network, which has only two layers and no activation function. We are essentially doing linear filtering."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SuperResolutionNet(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(SuperResolutionNet, self).__init__()\n",
    "\n",
    "        self.relu = nn.ReLU(inplace=True)\n",
    "\n",
    "        # --- Initial Feature Extraction ---\n",
    "        # Input: 1 x 128 x 128\n",
    "        self.conv1 = nn.Conv2d(in_channels=1, out_channels=128, kernel_size=3, padding=1) # 128 x 128 x 128 (Increased width)\n",
    "        self.conv1b = nn.Conv2d(in_channels=128, out_channels=128, kernel_size=3, padding=1) # 128 x 128 x 128\n",
    "\n",
    "        # --- Upscaling Path 1 (128 -> 256) ---\n",
    "        # Main Path\n",
    "        self.upscale1 = nn.ConvTranspose2d(in_channels=128, out_channels=128, kernel_size=2, stride=2) # 128x128x128 -> 128x256x256\n",
    "        self.conv2 = nn.Conv2d(in_channels=128, out_channels=128, kernel_size=3, padding=1) # 128x256x256\n",
    "        self.conv2b = nn.Conv2d(in_channels=128, out_channels=128, kernel_size=3, padding=1) # 128x256x256\n",
    "        # Skip Path 1 (Interpolate features from 128x128 stage)\n",
    "        self.bskip1 = nn.Upsample(scale_factor=2, mode='bicubic', align_corners=False) # Interpolates 128 channels to 256x256\n",
    "        # Convolution after combining skip connection 1 (Input channels = 128 from main path + 128 from skip = 256)\n",
    "        self.conv_after_skip1 = nn.Conv2d(in_channels=256, out_channels=128, kernel_size=3, padding=1) # 256x256x256 -> 128x256x256\n",
    "        self.conv_after_skip1b = nn.Conv2d(in_channels=128, out_channels=128, kernel_size=3, padding=1) # 128x256x256\n",
    "\n",
    "        # --- Upscaling Path 2 (256 -> 512) ---\n",
    "        # Main Path\n",
    "        self.upscale2 = nn.ConvTranspose2d(in_channels=128, out_channels=64, kernel_size=2, stride=2) # 128x256x256 -> 64x512x512 (Increased width)\n",
    "        self.conv3 = nn.Conv2d(in_channels=64, out_channels=64, kernel_size=3, padding=1) # 64x512x512\n",
    "        self.conv3b = nn.Conv2d(in_channels=64, out_channels=64, kernel_size=3, padding=1) # 64x512x512\n",
    "        # Skip Path 2 (Interpolate features from 256x256 stage after conv_after_skip1b)\n",
    "        self.bskip2 = nn.Upsample(scale_factor=2, mode='bicubic', align_corners=False) # Interpolates 128 channels to 512x512\n",
    "        # Convolution after combining skip connection 2 (Input channels = 64 from main path + 128 from skip = 192)\n",
    "        self.conv_after_skip2 = nn.Conv2d(in_channels=192, out_channels=64, kernel_size=3, padding=1) # 192x512x512 -> 64x512x512\n",
    "        self.conv_after_skip2b = nn.Conv2d(in_channels=64, out_channels=64, kernel_size=3, padding=1) # 64x512x512\n",
    "\n",
    "        # --- Downscaling Path (512 -> 256) ---\n",
    "        self.downscale = nn.Conv2d(in_channels=64, out_channels=128, kernel_size=2, stride=2) # 64x512x512 -> 128x256x256 (Increased width)\n",
    "        self.conv4 = nn.Conv2d(in_channels=128, out_channels=128, kernel_size=3, padding=1) # 128x256x256\n",
    "        self.conv4b = nn.Conv2d(in_channels=128, out_channels=128, kernel_size=3, padding=1) # 128x256x256\n",
    "\n",
    "        # --- Final Output Layer ---\n",
    "        # Takes the output of the downscaling path (128 channels) and maps to 1 channel\n",
    "        self.conv_out_final = nn.Conv2d(in_channels=128, out_channels=1, kernel_size=3, padding=1) # 128x256x256 -> 1x256x256\n",
    "\n",
    "    def forward(self, x):\n",
    "        # x: Input 1 x 128 x 128\n",
    "\n",
    "        # --- Initial Feature Extraction ---\n",
    "        features128 = self.relu(self.conv1(x))\n",
    "        features128 = self.relu(self.conv1b(features128)) # 128 x 128 x 128\n",
    "\n",
    "        # --- Upscaling Path 1 + Skip Connection 1 ---\n",
    "        # Calculate interpolated skip features first\n",
    "        skip1_interpolated = self.bskip1(features128) # 128 x 256 x 256\n",
    "        # Main path upscale\n",
    "        up1_out = self.upscale1(features128) # 128 x 256 x 256\n",
    "        features256_main = self.relu(self.conv2(up1_out))\n",
    "        features256_main = self.relu(self.conv2b(features256_main)) # 128 x 256 x 256\n",
    "        # Concatenate main path features and interpolated skip features\n",
    "        concat1 = torch.cat((features256_main, skip1_interpolated), dim=1) # 256 x 256 x 256\n",
    "        # Process combined features\n",
    "        features256_processed = self.relu(self.conv_after_skip1(concat1))\n",
    "        features256_processed = self.relu(self.conv_after_skip1b(features256_processed)) # 128 x 256 x 256\n",
    "\n",
    "        # --- Upscaling Path 2 + Skip Connection 2 ---\n",
    "        # Calculate interpolated skip features (using output from conv_after_skip1b)\n",
    "        skip2_interpolated = self.bskip2(features256_processed) # 128 x 512 x 512\n",
    "        # Main path upscale\n",
    "        up2_out = self.upscale2(features256_processed) # 64 x 512 x 512\n",
    "        features512_main = self.relu(self.conv3(up2_out))\n",
    "        features512_main = self.relu(self.conv3b(features512_main)) # 64 x 512 x 512\n",
    "        # Concatenate main path features and interpolated skip features\n",
    "        concat2 = torch.cat((features512_main, skip2_interpolated), dim=1) # 192 x 512 x 512\n",
    "        # Process combined features\n",
    "        features512_processed = self.relu(self.conv_after_skip2(concat2))\n",
    "        features512_processed = self.relu(self.conv_after_skip2b(features512_processed)) # 64 x 512 x 512\n",
    "\n",
    "        # --- Downscaling Path ---\n",
    "        down_out = self.downscale(features512_processed) # 128 x 256 x 256\n",
    "        features256_final = self.relu(self.conv4(down_out))\n",
    "        features256_final = self.relu(self.conv4b(features256_final)) # 128 x 256 x 256\n",
    "\n",
    "        # --- Final Output Layer ---\n",
    "        output = self.conv_out_final(features256_final) # 1 x 256 x 256\n",
    "\n",
    "        return output"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "___\n",
    "### Create a function to execute training. Note, we will call this function later."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Helper function for one training epoch\n",
    "def train_one_epoch(model, train_loader, criterion, optimizer, device, scaler):\n",
    "    model.train() # Set model to training mode\n",
    "    total_train_loss = 0.0\n",
    "    for x_tr_batch, y_tr_batch in train_loader:\n",
    "        x_tr_batch, y_tr_batch = x_tr_batch.to(device), y_tr_batch.to(device)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        # --- Mixed Precision: Forward pass with autocast ---\n",
    "        with torch.amp.autocast(device_type=device.type, dtype=torch.float16, enabled=scaler.is_enabled()):\n",
    "            y_hat_tr_batch = model(x_tr_batch)\n",
    "            loss = criterion(y_hat_tr_batch, y_tr_batch)\n",
    "\n",
    "        # --- Mixed Precision: Scale loss and backward pass ---\n",
    "        scaler.scale(loss).backward()\n",
    "\n",
    "        # --- Mixed Precision: Scaler step and update ---\n",
    "        scaler.step(optimizer)\n",
    "        scaler.update()\n",
    "\n",
    "        total_train_loss += loss.item()\n",
    "\n",
    "    avg_train_loss = total_train_loss / len(train_loader)\n",
    "    return avg_train_loss\n",
    "\n",
    "# Helper function for one validation epoch\n",
    "def validate_one_epoch(model, val_loader, criterion, psnr_metric, ssim_metric, device):\n",
    "    model.eval() # Set model to evaluation mode\n",
    "    total_val_loss = 0.0\n",
    "    total_val_psnr = 0.0\n",
    "    total_val_ssim = 0.0\n",
    "    num_samples = 0\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for x_val_batch, y_val_batch in val_loader:\n",
    "            x_val_batch, y_val_batch = x_val_batch.to(device), y_val_batch.to(device)\n",
    "            batch_size = x_val_batch.size(0)\n",
    "            num_samples += batch_size\n",
    "\n",
    "            # Upsample low-resolution input for baseline comparison (outside autocast if not needed)\n",
    "            x_val_batch_interpolated = torch.nn.functional.interpolate(x_val_batch, scale_factor=2, mode='bicubic', align_corners=False)\n",
    "\n",
    "            # --- Mixed Precision: Validation forward pass with autocast ---\n",
    "            # Enable autocast for potential efficiency, disable if it causes issues with metrics/loss\n",
    "            with torch.amp.autocast(device_type=device.type, dtype=torch.float16, enabled=True):\n",
    "                y_hat_val_batch = model(x_val_batch)\n",
    "                # Ensure loss calculation happens with autocast output type or cast explicitly if needed\n",
    "                val_loss = criterion(y_hat_val_batch, y_val_batch)\n",
    "\n",
    "            total_val_loss += val_loss.item() * batch_size # Accumulate total loss\n",
    "\n",
    "            # Calculate metrics (ensure they handle potential float16 inputs from autocast)\n",
    "            # TorchMetrics usually handles this, but double-check if using custom metrics.\n",
    "            # Cast inputs to float32 if metrics require it:\n",
    "            # psnr = psnr_metric(y_hat_val_batch.float(), y_val_batch.float())\n",
    "            # ssim = ssim_metric(y_hat_val_batch.float(), y_val_batch.float())\n",
    "            psnr = psnr_metric(y_hat_val_batch, y_val_batch)\n",
    "            ssim = ssim_metric(y_hat_val_batch, y_val_batch)\n",
    "\n",
    "            total_val_psnr += psnr.item() * batch_size\n",
    "            total_val_ssim += ssim.item() * batch_size\n",
    "\n",
    "    avg_val_loss = total_val_loss / num_samples # Average loss per image\n",
    "    avg_val_psnr = total_val_psnr / num_samples\n",
    "    avg_val_ssim = total_val_ssim / num_samples\n",
    "    avg_val_score = avg_val_psnr + (40 * avg_val_ssim)\n",
    "\n",
    "    return avg_val_loss, avg_val_score, avg_val_psnr, avg_val_ssim\n",
    "\n",
    "def train_model(model, opt, criterion, scheduler, train_loader, val_loader, num_epoch, patience, device, save_dir='saved_models', scaler=None):\n",
    "    os.makedirs(save_dir, exist_ok=True)\n",
    "    epoch_nums, avg_train_losses, avg_val_losses, avg_val_scores = [], [], [], []\n",
    "    best_val_score = -float('inf')\n",
    "    epochs_no_improve = 0\n",
    "    best_model_state = None\n",
    "\n",
    "    # Initialize metrics outside the loop - ensure they handle accumulation correctly or reset them if needed\n",
    "    # For torchmetrics, calling them per batch and averaging manually is often safer than relying on internal state across epochs.\n",
    "    # We will re-initialize inside validate_one_epoch for clarity here.\n",
    "    psnr_metric = PeakSignalNoiseRatio().to(device)\n",
    "    ssim_metric = StructuralSimilarityIndexMeasure(data_range=1.0).to(device) # data_range=1.0 assumes normalization\n",
    "\n",
    "    overall_pbar = tqdm(range(num_epoch), desc=\"Overall Training Progress\")\n",
    "\n",
    "    for epoch in overall_pbar:\n",
    "        # --- Training ---\n",
    "        avg_train_loss = train_one_epoch(model, train_loader, criterion, opt, device, scaler)\n",
    "\n",
    "        # --- Validation ---\n",
    "        avg_val_loss, avg_val_score, avg_val_psnr, avg_val_ssim = validate_one_epoch(\n",
    "            model, val_loader, criterion, psnr_metric, ssim_metric, device\n",
    "        )\n",
    "\n",
    "        # Store metrics for plot\n",
    "        epoch_nums.append(epoch + 1)\n",
    "        avg_train_losses.append(avg_train_loss)\n",
    "        avg_val_losses.append(avg_val_loss)\n",
    "        avg_val_scores.append(avg_val_score)\n",
    "\n",
    "        # --- Update Progress Bar ---\n",
    "        current_lr = opt.param_groups[0]['lr']\n",
    "        overall_pbar.set_description(\n",
    "            f\"LR: {current_lr:.1e} | Train Loss: {avg_train_loss:.4f} | Current Val Score: {avg_val_score:.2f} | Best Val Score: {best_val_score:.2f} | Epochs Since Best: {epochs_no_improve:.0f}\"\n",
    "        )\n",
    "\n",
    "        # --- Learning Rate Scheduler Step ---\n",
    "        scheduler.step(avg_val_score)\n",
    "\n",
    "        # --- Checkpointing and Early Stopping ---\n",
    "        if avg_val_score > best_val_score:\n",
    "            best_val_score = avg_val_score\n",
    "            epochs_no_improve = 0\n",
    "            best_model_state = copy.deepcopy(model.state_dict())\n",
    "            # Optionally add a print statement here if desired\n",
    "        else:\n",
    "            epochs_no_improve += 1\n",
    "\n",
    "        if epochs_no_improve >= patience:\n",
    "            print(f\"\\nEarly stopping triggered after {patience} epochs with no improvement.\")\n",
    "            break\n",
    "\n",
    "    # --- Save Best Model ---\n",
    "    if best_model_state:\n",
    "        timestamp = datetime.datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
    "        score_str = f\"{best_val_score:.2f}\".replace('.', 'p')\n",
    "        filename = f\"best_model_score_{score_str}_time_{timestamp}.pth\"\n",
    "        save_path = os.path.join(save_dir, filename)\n",
    "        torch.save(best_model_state, save_path)\n",
    "        print(f\"\\nTraining finished. Best model saved to '{save_path}' with score: {best_val_score:.2f}\")\n",
    "        model.load_state_dict(best_model_state) # Load best state into model\n",
    "    else:\n",
    "        print(\"\\nTraining finished. No model state was saved.\")\n",
    "\n",
    "    # --- Final Plot ---\n",
    "    fig_final, ax1_final = plt.subplots(figsize=(10, 6))\n",
    "    ax2_final = ax1_final.twinx()\n",
    "    line1_final, = ax1_final.plot(epoch_nums, avg_train_losses, 'r-', label='Training Loss')\n",
    "    line2_final, = ax1_final.plot(epoch_nums, avg_val_losses, 'r--', label='Validation Loss')\n",
    "    line3_final, = ax2_final.plot(epoch_nums, avg_val_scores, 'b-', label='Validation Score')\n",
    "    ax1_final.set_xlabel('Epochs')\n",
    "    ax1_final.set_ylabel('Loss', color='tab:red')\n",
    "    ax1_final.tick_params(axis='y', labelcolor='tab:red')\n",
    "    ax2_final.set_ylabel('Validation Score (PSNR + 40*SSIM)', color='tab:blue')\n",
    "    ax2_final.tick_params(axis='y', labelcolor='tab:blue')\n",
    "    ax1_final.legend(handles=[line1_final, line2_final], loc='upper left')\n",
    "    ax2_final.legend(handles=[line3_final], loc='upper right')\n",
    "    ax1_final.grid(True)\n",
    "    plt.title('Final Training Progress')\n",
    "    fig_final.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "    return best_val_score"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "___\n",
    "### Now, let us define hyperparameters and train the network. \n",
    "\n",
    "#### Note, in addition to the parameters that controls the network architecture or the training process, you need to select/initialize (i) a data loader, (ii) a model, (iii) an optimizer, and (iv) a loss function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Hyperparameters ---\n",
    "batch_size = 5\n",
    "lr = 1e-4\n",
    "num_epoch = 200\n",
    "patience_early_stopping = 15\n",
    "scheduler_patience = 5\n",
    "scheduler_factor = 0.1\n",
    "loss_alpha = 0.5\n",
    "loss_beta = 0.5\n",
    "save_dir = 'saved_models'\n",
    "\n",
    "# --- Setup ---\n",
    "# Create DataLoaders\n",
    "train_loader = create_loader(train_dataset, batch_size, shuffle_data=True)\n",
    "val_loader = create_loader(val_dataset, batch_size, shuffle_data=False) # Ensure val_dataset is defined\n",
    "\n",
    "# Print dataset sizes (assuming datasets are defined)\n",
    "if 'train_dataset' in locals() and 'val_dataset' in locals():\n",
    "    print(f'Number of original training images: {getattr(train_dataset, \"num_original_images\", \"N/A\")}')\n",
    "    print(f'Number of training images after augmentation: {len(train_dataset)}')\n",
    "    print(f'Number of original validation images: {getattr(val_dataset, \"num_original_images\", \"N/A\")}')\n",
    "    print(f'Number of validation images (no augmentation): {len(val_dataset)}')\n",
    "\n",
    "# Model\n",
    "model = SuperResolutionNet().to(device)\n",
    "scaler = torch.amp.GradScaler('cuda')\n",
    "\n",
    "# Optimizer\n",
    "opt = optim.Adam(model.parameters(), lr=lr)\n",
    "\n",
    "# Loss Function (Criterion)\n",
    "criterion = CombinedLoss(alpha=loss_alpha, beta=loss_beta, device=device).to(device)\n",
    "\n",
    "# Learning Rate Scheduler\n",
    "scheduler = ReduceLROnPlateau(\n",
    "    opt,\n",
    "    mode='max',\n",
    "    factor=scheduler_factor,\n",
    "    patience=scheduler_patience\n",
    ")\n",
    "\n",
    "# --- Train the model ---\n",
    "best_score = train_model(\n",
    "    model=model,\n",
    "    opt=opt,\n",
    "    criterion=criterion,\n",
    "    scheduler=scheduler,\n",
    "    train_loader=train_loader,\n",
    "    val_loader=val_loader,\n",
    "    num_epoch=num_epoch,\n",
    "    patience=patience_early_stopping,\n",
    "    device=device,\n",
    "    save_dir=save_dir,\n",
    "    scaler=scaler\n",
    ")\n",
    "\n",
    "print(f\"\\nBest validation score achieved during training: {best_score:.2f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "___\n",
    "### Apply it one of the validation image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load one (low-res, high-res) image pair from validation dataset and move it to the dedvice\n",
    "val_low_res, val_high_res = val_dataset[1]  # Input (128x128), Ground truth (256x256)\n",
    "val_low_res, val_high_res = val_low_res.to(device), val_high_res.to(device)\n",
    "\n",
    "# Keep the interpolated version ONLY for visualization comparison\n",
    "val_low_res_interpolated = torch.nn.functional.interpolate(val_low_res.unsqueeze(0), scale_factor=2, mode='bicubic', align_corners=False).squeeze(0)\n",
    "\n",
    "# Apply the trained model to the original low-res image\n",
    "# Add batch dimension for model, then remove it and detach\n",
    "val_super_res = model(val_low_res.unsqueeze(0)).detach().squeeze(0)\n",
    "\n",
    "# Convert tensors to numpy for visualization\n",
    "val_low_res_np = val_low_res_interpolated.squeeze().cpu().numpy()  # Use the interpolated version for vis/error maps\n",
    "val_high_res_np = val_high_res.squeeze().cpu().numpy()\n",
    "val_super_res_np = val_super_res.squeeze().cpu().numpy()\n",
    "\n",
    "# Plot an example image and error maps\n",
    "fig, ax = plt.subplots(2, 3, figsize=(10, 7))\n",
    "\n",
    "# Plot images\n",
    "ax[0, 0].imshow(val_high_res_np, cmap='gray', vmin=0, vmax=1)\n",
    "ax[0, 0].set_title('ground truth (high-res)')\n",
    "ax[0, 0].axis('off')\n",
    "\n",
    "ax[0, 1].imshow(val_low_res_np, cmap='gray', vmin=0, vmax=1)\n",
    "ax[0, 1].set_title('interpolated low-res image')\n",
    "ax[0, 1].axis('off')\n",
    "\n",
    "ax[0, 2].imshow(val_super_res_np, cmap='gray', vmin=0, vmax=1)\n",
    "ax[0, 2].set_title('super-resolved image')\n",
    "ax[0, 2].axis('off')\n",
    "\n",
    "# Error maps\n",
    "ax[1, 0].imshow(5 * np.abs(val_high_res_np - val_high_res_np), cmap='gray', vmin=0, vmax=1)\n",
    "ax[1, 0].axis('off')\n",
    "ax[1, 0].text(0.02, 0.98, r'$\\times 5$', transform=ax[1, 0].transAxes, fontsize=14, va='top', ha='left', color='white')\n",
    "\n",
    "ax[1, 1].imshow(5 * np.abs(val_high_res_np - val_low_res_np), cmap='gray', vmin=0, vmax=1)\n",
    "ax[1, 1].axis('off')\n",
    "ax[1, 1].text(0.02, 0.98, r'$\\times 5$', transform=ax[1, 1].transAxes, fontsize=14, va='top', ha='left', color='white')\n",
    "\n",
    "ax[1, 2].imshow(5 * np.abs(val_high_res_np - val_super_res_np), cmap='gray', vmin=0, vmax=1)\n",
    "ax[1, 2].axis('off')\n",
    "ax[1, 2].text(0.02, 0.98, r'$\\times 5$', transform=ax[1, 2].transAxes, fontsize=14, va='top', ha='left', color='white')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Compute PSNR and SSIM over the entire validation dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_model(model, data_loader, device):\n",
    "    \"\"\"Computes PSNR, SSIM, and Score over a dataset using a DataLoader.\"\"\"\n",
    "    model.eval() # Set model to evaluation mode\n",
    "    psnr_metric = PeakSignalNoiseRatio().to(device)\n",
    "    ssim_metric = StructuralSimilarityIndexMeasure(data_range=1.0).to(device)\n",
    "\n",
    "    total_psnr_sr = 0.0\n",
    "    total_ssim_sr = 0.0\n",
    "    total_psnr_interp = 0.0\n",
    "    total_ssim_interp = 0.0\n",
    "    num_samples = 0\n",
    "\n",
    "    with torch.no_grad():\n",
    "        eval_pbar = tqdm(data_loader, desc=\"Evaluating Model\")\n",
    "        for x_batch, y_batch in eval_pbar:\n",
    "            x_batch, y_batch = x_batch.to(device), y_batch.to(device) # x_batch is 128x128\n",
    "            batch_size_actual = x_batch.size(0)\n",
    "            num_samples += batch_size_actual\n",
    "\n",
    "            # Keep this interpolation for baseline calculation (outside autocast)\n",
    "            x_batch_interpolated = torch.nn.functional.interpolate(x_batch, scale_factor=2, mode='bicubic', align_corners=False)\n",
    "\n",
    "            # --- Mixed Precision: Evaluation forward pass with autocast ---\n",
    "            # Enable autocast for potential efficiency\n",
    "            with torch.amp.autocast(device_type=device.type, dtype=torch.float16, enabled=True):\n",
    "                y_hat_batch = model(x_batch) # Model outputs 256x256\n",
    "\n",
    "            # Calculate metrics for super-resolved\n",
    "            # Cast inputs to float32 if metrics require it:\n",
    "            # psnr_sr = psnr_metric(y_hat_batch.float(), y_batch.float())\n",
    "            # ssim_sr = ssim_metric(y_hat_batch.float(), y_batch.float())\n",
    "            psnr_sr = psnr_metric(y_hat_batch, y_batch)\n",
    "            ssim_sr = ssim_metric(y_hat_batch, y_batch)\n",
    "            total_psnr_sr += psnr_sr.item() * batch_size_actual\n",
    "            total_ssim_sr += ssim_sr.item() * batch_size_actual\n",
    "\n",
    "            # Calculate metrics for interpolated (baseline) - ensure y_batch is float32 if needed\n",
    "            psnr_interp = psnr_metric(x_batch_interpolated, y_batch)\n",
    "            ssim_interp = ssim_metric(x_batch_interpolated, y_batch)\n",
    "            total_psnr_interp += psnr_interp.item() * batch_size_actual\n",
    "            total_ssim_interp += ssim_interp.item() * batch_size_actual\n",
    "\n",
    "    # Calculate averages\n",
    "    avg_psnr_sr = total_psnr_sr / num_samples\n",
    "    avg_ssim_sr = total_ssim_sr / num_samples\n",
    "    avg_psnr_interp = total_psnr_interp / num_samples\n",
    "    avg_ssim_interp = total_ssim_interp / num_samples\n",
    "\n",
    "    # Calculate scores\n",
    "    avg_score_sr = avg_psnr_sr + (40 * avg_ssim_sr)\n",
    "    avg_score_interp = avg_psnr_interp + (40 * avg_ssim_interp)\n",
    "\n",
    "    # Print results\n",
    "    print(\"\\n--- Final Evaluation Results ---\")\n",
    "    print(f'Average PSNR (interpolated): {avg_psnr_interp:.2f} dB')\n",
    "    print(f'Average PSNR (super-resolved): {avg_psnr_sr:.2f} dB')\n",
    "    print(f'Average SSIM (interpolated): {avg_ssim_interp:.4f}')\n",
    "    print(f'Average SSIM (super-resolved): {avg_ssim_sr:.4f}')\n",
    "    print(f'Average Score (interpolated): {avg_score_interp:.2f}')\n",
    "    print(f'Average Score (super-resolved): {avg_score_sr:.2f}')\n",
    "\n",
    "    return avg_score_sr, avg_score_interp\n",
    "\n",
    "# --- Call the Evaluation Function ---\n",
    "final_sr_score, final_interp_score = evaluate_model(model, val_loader, device)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
